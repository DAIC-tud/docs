[{"body":"General software Most common general software, like programming languages and libraries, is installed on the DAIC servers.\nTo check if the program that you need is pre-installed, you can simply try to start it:\n$ python Python 2.7.5 (default, Jun 28 2022, 15:30:04) [GCC 4.8.5 20150623 (Red Hat 4.8.5-44)] on linux2 Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. \u003e\u003e\u003e \u003e\u003e\u003e quit() $ $ python4 -bash: python4: command not found Alternatively, you can try to locate the program or library using the whereis command:\n$ whereis python python: /usr/bin/python3.4m-config /usr/bin/python3.6m-x86_64-config /usr/bin/python2.7 /usr/bin/python3.6-config /usr/bin/python3.4m-x86_64-config /usr/bin/python3.6m-config /usr/bin/python3.4 /usr/bin/python3.4m /usr/bin/python2.7-config /usr/bin/python3.6 /usr/bin/python3.4-config /usr/bin/python /usr/bin/python3.6m /usr/lib/python2.7 /usr/lib/python3.4 /usr/lib/python3.6 /usr/lib64/python2.7 /usr/lib64/python3.4 /usr/lib64/python3.6 /etc/python /usr/include/python2.7 /usr/include/python3.4m /usr/include/python3.6m /usr/share/man/man1/python.1.gz $ $ whereis python4 python4:$ Or, you can check if the package is installed using the rpm -qa command as follows:\n$ rpm -qa python python-2.7.5-92.el7_9.x86_64 $ $ rpm -qa python4 $ Environment modules Some often used third-party software (eg, CUDA, cuDNN, Matlab) is installed in the cluster as environment modules   .\nTo see or use the available modules, first, enable the software collection:\n$ module use /opt/insy/modulefiles Now, to see all available packages and versions:\n$ module avail ---------------------------------------------------------------------------------------------- /opt/insy/modulefiles ---------------------------------------------------------------------------------------------- albacore/2.2.7-Python-3.4 cuda/11.8 cudnn/11.5-8.3.0.98 devtoolset/6 devtoolset/10 intel/oneapi (D) matlab/R2021b (D) miniconda/3.9 (D) comsol/5.5 cuda/12.0 cudnn/12-8.9.1.23 (D) devtoolset/7 devtoolset/11 (D) intel/2017u4 miniconda/2.7 nccl/11.5-2.11.4 comsol/5.6 (D) cuda/12.1 (D) cwp-su/43R8 devtoolset/8 diplib/3.2 matlab/R2020a miniconda/3.7 openmpi/4.0.1 cuda/11.5 cudnn/11-8.6.0.163 cwp-su/44R1 (D) devtoolset/9 : : : # Output omitted for brevity And to check the description of a specific module:\n$ $ module whatis cudnn cudnn/12-8.9.1.23 : cuDNN 8.9.1.23 for CUDA 12 cudnn/12-8.9.1.23 : NVIDIA CUDA Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. And to use the module or package, load it as follows:\n$ module load cuda/11.2 cudnn/11.2-8.1.1.33 # load the module $ $ module list # check the loaded modules Currently Loaded Modules: 1) cuda/11.2 2) cudnn/11.2-8.1.1.33  Note For more information about using the module system, run module help.  ","categories":"","description":"How to find and work with pre-installed software?\n","excerpt":"How to find and work with pre-installed software?\n","ref":"/docs/software_environment/available_software/","tags":"","title":"Available software and libraries"},{"body":"Containerization Technology Containerization is a convenient means to deploy libraries and applications to different environments in a reproducible manner. A container image, typically a *.sif file, is a self-contained file with all necessary components to run an application, including code, runtime libraries, and dependencies.\nDAIC supports Apptainer (previously Singularity), an open-source container platform, designed to run complex applications on HPC clusters. Apptainer makes it possible to use docker images natively at a higher level of security and isolation. (see Pulling images)\nGenerally, to launch a container image, your commands look as follows:\n$ apptainer shell \u003ccontainer\u003e # OR $ apptainer exec \u003ccontainer\u003e \u003ccommand\u003e $ apptainer run \u003ccontainer\u003e where:\n \u003ccontainer\u003e is the path to a container image, typically, a *.sif file \u003ccommand\u003e is the command you like to run from inside the container, eg, hostname Both shell and exec can be used to launch container images. The difference is that shell allows you to work inside the container image interactively; while exec executes the \u003ccommand\u003e inside the image and exits. Of course, by using something like /bin/bash as the \u003ccommand\u003e, exec behaves exactly like shell. run also launches a container image, but runs the default action defined in the container image. See an example use case in Building images   The question is now: where to get the \u003ccontainer\u003e file from? You can either:\n use a pre-built image by pulling from a repository (see Pulling images), or, build your own container image and use it accordingly (see Building images).  Note If you intend to extensively work/test your image interactively, it is best to first submit an interactive SLURM job with the needed resources, eg, memory, gpus, … etc:\n$ hostname # To check this is DAIC. login[1-3] are the login nodes login1.hpc.tudelft.nl $ sinteractive # Default resources: --time=01:00:00 --cpus-per-task = 2 --mem=1024  Note: interactive sessions are automatically terminated when they reach their time limit (1 hour)! srun: job 8543393 queued and waiting for resources srun: job 8543393 has been allocated resources 13:35:30 up 5 days, 3:41, 0 users, load average: 8,79, 7,60, 7,11 $ hostname # To check we are on a compute node grs3.hpc.tudelft.nl   Pulling images Many repositories exist where container images are hosted. Apptainer allows pulling and using images from repositories like DockerHub, BioContainers and NVIDIA GPU Cloud (NGC).\nPulling from DockerHub For example, to obtain the latest Ubuntu image from DockerHub:\n$ hostname # check this is DAIC login1.hpc.tudelft.nl $ cd \u0026\u0026 mkdir containers \u0026\u0026 cd containers # as convenience, use this directory $ apptainer pull docker://ubuntu:latest # actually pull the image INFO: Converting OCI blobs to SIF format INFO: Starting build... Getting image source signatures Copying blob 837dd4791cdc done Copying config 1f6ddc1b25 done Writing manifest to image destination Storing signatures ... INFO: Creating SIF file... Now, to check the obtained image file:\n$ ls ubuntu_latest.sif $ apptainer exec ubuntu_latest.sif cat /etc/os-release # execute cat command and exit PRETTY_NAME=\"Ubuntu 22.04.2 LTS\" NAME=\"Ubuntu\" VERSION_ID=\"22.04\" VERSION=\"22.04.2 LTS (Jammy Jellyfish)\" VERSION_CODENAME=jammy ID=ubuntu ID_LIKE=debian HOME_URL=\"https://www.ubuntu.com/\" SUPPORT_URL=\"https://help.ubuntu.com/\" BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\" PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\" UBUNTU_CODENAME=jammy $ ls /.singularity.d/ # container-specific directory should not be found on host ls: cannot access /.singularity.d/: No such file or directory $ apptainer shell ubuntu_latest.sif # launch container interactively Apptainer\u003e Apptainer\u003e hostname login1.hpc.tudelft.nl Apptainer\u003e ls ubuntu_latest.sif Apptainer\u003e ls /.singularity.d/ Singularity actions env labels.json libs runscript startscript Apptainer\u003e exit In the above snippet, note:\n The command prompt changes within the container to Apptainer\u003e The container seamlessly interacts with the host system. For example, it inherits its hostname (the DAIC login node in this case). The container also inherits the $HOME variable, and is able to edit/delete files from there. The container has its own file system, which is distinct from the host. The presence of a directory like /.singularity.d is another feature of the specific to the container.  Warning To isolate files in your system (ie, your local machine or DAIC) from the files inside the container (and thus, avoid possible erroneous deletes/edits), it is recommended to add a -c or -C flags to your apptainer commands\n$ apptainer shell -C ubuntu_latest.sif   Pulling from NVIDIA GPU cloud (NGC) This is a specialized registry provided by NVIDIA for GPU accelerated applications or GPU software development tools. These images are large, and one is recommended to download them locally in your machine, and only send the downloaded image to DAIC. For this, you need to have Apptainer locally installed first.\nTip To install Apptainer in your machine, follow the official Installing Apptainer instructions. Apptainer needs a Linux kernel to run. Thus, if you are a Mac user, you can use Lima to install both a Linux virtual machine and Apptainer.  Warning By default, Apptainer images are saved to ~/.singularity. Ideally, to avoid quota issues, you’d set the environment variable SINGULARITY_CACHEDIR to a different location. At present, both the bulk and umbrella filesystems do not support pulling images, so you are advised to pull these to your local machine and then copy over the image file to DAIC.  $ hostname #check this is your own PC/laptop $ apptainer pull docker://nvcr.io/nvidia/pytorch:23.05-py3 $ scp pytorch_23.05-py3.sif hpc-login:/tudelft.net/staff-umbrella/...\u003cYourDirectory\u003e/apptainer Now, to check this particular image on DAIC:\n$ hostname # check this is DAIC not your own PC/laptop login1.hpc.tudelft.nl $ cd /tudelft.net/staff-umbrella/...\u003cYourDirectory\u003e/apptainer # path where you put images $ apptainer shell -C --nv pytorch_23.05-py3.sif #--nv to use NVIDIA GPU and have CUDA support Apptainer\u003e Apptainer\u003e hostname login1.hpc.tudelft.nl # hostname inherited Apptainer\u003e ls /.singularity.d/ # verify this is the image Singularity actions env labels.json libs runscript startscript Building images If you prefer (or need) to have a custom container image, then you can build your own container image from a recipe file, typically *.def file, that sets up the image with your custom dependencies. The only requirement for building is to be in a machine (eg, your local laptop/pc) where you have sudo/root privileges. In other words, you can not build images on DAIC directly: First, you must build the image locally, and then send it to DAIC to run there.\nReminder Always build the image first in your local machine. To send the built image, YourImage.sif to DAIC, do the following:\n$ hostname # check this is your machine $ scp YourImage.sif \u003cYourNetID\u003e@login.hpc.tudelft.nl:/tudelft.net/staff-umbrella/../\u003cYourDirectory\u003e/apptainer # send the image to DAIC   Tip  To install Apptainer in your machine, follow the official Installing Apptainer instructions. If you are a Mac user, you can use Lima to install both a Linux virtual machine and Apptainer.   An example recipe file, cuda_based_recipe.def, for a cuda-enabled container may look as follows:\n$ cat cuda_based_recipe.def # Header Bootstrap: docker From: nvidia/cuda:12.1.1-devel-ubuntu22.04 # (Optional) Sections/ data blobs %post apt-get update # update system apt-get install -y git # install git git clone https://github.com/NVIDIA/cuda-samples.git # clone target repository cd cuda-samples git fetch origin --tags \u0026\u0026 git checkout v12.1 # fetch certain repository version cd Samples/1_Utilities/deviceQuery \u0026\u0026 make # install certain tool %runscript /cuda-samples/Samples/1_Utilities/deviceQuery/deviceQuery where:\n The header, the first 2 lines of this example, specify the source of a base image, (eg, Bootstrap: docker), and the base image (From: nvidia/cuda:12.1.1-devel-ubuntu22.04) to be pulled from this source. The container image will be built on top of this base image. In this example, the base image will be built from Ubuntu 22.04 OS with the CUDA toolkit 12.1 pre-installed. The rest of the file are optional data blobs or sections. In this example, the following blobs are used:  %post blob: the steps to download, configure and install needed custom software and libraries on the base image. In this example, the steps install git, clone a repo, and install a package via make %runscript blob: the scripts or commands to execute when the container image is run. That is, this code is the entry point to the container with the run command. In this example, the deviceQuery is executed once the container is run. Other blobs may be present in the def file. See Definition files documentation for more details and examples.    And now, build this image and send it over to DAIC:\n$ hostname #check this is your machine $ sudo apptainer build cuda_based_image.sif cuda_based_recipe.def # building may take ~ 2-5 min, depending on your internet INFO: Starting build... Getting image source signatures Copying blob d5d706ce7b29 [=\u003e------------------------------------] 29.2MiB / 702.5MiB ... INFO: Adding runscript INFO: Creating SIF file... INFO: Build complete: cuda_based_image.sif $ $ scp cuda_based_image.sif hpc-login:/tudelft.net/staff-umbrella/...\u003cYourDirectory\u003e/apptainer # send to DAIC On DAIC, check the image:\n$ hostname # check you are on DAIC login1.hpc.tudelft.nl $ sinteractive --cpus-per-task=2 --mem=1024 --gres=gpu --time=00:05:00 # request a gpu node $ hostname # check you are on a compute node insy13.hpc.tudelft.nl $ apptainer run --nv -C cuda_based_image.sif # --nv to use NVIDIA GPU and have CUDA support /cuda-samples/Samples/1_Utilities/deviceQuery/deviceQuery Starting... CUDA Device Query (Runtime API) version (CUDART static linking) Detected 1 CUDA Capable device(s) Device 0: \"NVIDIA GeForce GTX 1080 Ti\" CUDA Driver Version / Runtime Version 12.1 / 12.1 CUDA Capability Major/Minor version number: 6.1 Total amount of global memory: 11172 MBytes (11714887680 bytes) (028) Multiprocessors, (128) CUDA Cores/MP: 3584 CUDA Cores GPU Max Clock rate: 1582 MHz (1.58 GHz) Memory Clock rate: 5505 Mhz Memory Bus Width: 352-bit L2 Cache Size: 2883584 bytes Maximum Texture Dimension Size (x,y,z) 1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384) Maximum Layered 1D Texture Size, (num) layers 1D=(32768), 2048 layers Maximum Layered 2D Texture Size, (num) layers 2D=(32768, 32768), 2048 layers Total amount of constant memory: 65536 bytes Total amount of shared memory per block: 49152 bytes Total shared memory per multiprocessor: 98304 bytes Total number of registers available per block: 65536 Warp size: 32 Maximum number of threads per multiprocessor: 2048 Maximum number of threads per block: 1024 Max dimension size of a thread block (x,y,z): (1024, 1024, 64) Max dimension size of a grid size (x,y,z): (2147483647, 65535, 65535) Maximum memory pitch: 2147483647 bytes Texture alignment: 512 bytes Concurrent copy and kernel execution: Yes with 2 copy engine(s) Run time limit on kernels: No Integrated GPU sharing Host Memory: No Support host page-locked memory mapping: Yes Alignment requirement for Surfaces: Yes Device has ECC support: Disabled Device supports Unified Addressing (UVA): Yes Device supports Managed Memory: Yes Device supports Compute Preemption: Yes Supports Cooperative Kernel Launch: Yes Supports MultiDevice Co-op Kernel Launch: Yes Device PCI Domain ID / Bus ID / location ID: 0 / 141 / 0 Compute Mode: \u003c Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) \u003e deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 12.1, CUDA Runtime Version = 12.1, NumDevs = 1 Result = PASS  Warning Always pass --nv to apptainer to run GPU-accelerated applications or libraries inside the container. Note that you also need 1) your host system must have NVIDIA GPU drivers installed and compatible with the version of Singularity you are using, and 2) the container you are running should have the necessary dependencies and configurations to support GPU acceleration.\n$ apptainer shell --nv -C cuda_based_image.sif   Note Building container images from a recipe file is recommended to ensure the reproducibility of the resulting container image. However, there can be cases of complex dependencies where it is not clear upfront how the software installations and dependencies should be set up. In such cases, it is possible to interactively develop the image by building it in writable sandbox mode first. In such cases, take note of all installation commands used in the sandbox, so you can include them in a recipe file. See Apptainer Sandbox Directories for more details.  Building from local image file During software development, it is common to incrementally build code and go through many iterations of debugging and testing. A development container may be used in this process. In such scenarios, re-building the container from the base image with each debugging or testing iteration becomes taxing very quickly, due to dependencies and installations involved. Instead, the Bootstrap: localimage and From:\u003cpath/to/local/image\u003e header can be used to base the development container on some local image.\nAs an example, assume it is desirable to develop some code on the basis of the cuda_based_recipe.sif image created in the Building images section. Building from the original cuda_based_recipe.def file can take ~ 4 minutes. However, if the *.sif file is already available, building on top of it, via a dev_on_cuda_based_recipe.def file as below, takes ~ 2 minutes. This is already a time saving factor of 2 in this case.\n$ hostname # check this is your machine $ cat dev_on_cuda_based_recipe.def # def file for an image based on localimage # Header Bootstrap: localimage From: cuda_based_recipe.sif # (Optional) Sections/ data blobs %runscript echo \"Arguments received: $*\" exec echo \"$@\" $ $ sudo apptainer build dev_image.sif # build the image INFO: Starting build... INFO: Verifying bootstrap image cuda_based_recipe.sif WARNING: integrity: signature not found for object group 1 WARNING: Bootstrap image could not be verified, but build will continue. INFO: Adding runscript INFO: Creating SIF file... INFO: Build complete: dev_image.sif $ $ apptainer run dev_image.sif \"hello world\" # check runscript of the new def file is executed INFO: gocryptfs not found, will not be able to use gocryptfs Arguments received: hello world hello world $ $ apptainer shell dev_image.sif # further look inside the image Apptainer\u003e Apptainer\u003e ls /cuda-samples/Samples/1_Utilities/deviceQuery/deviceQuery # commands installed in the original image are available /cuda-samples/Samples/1_Utilities/deviceQuery/deviceQuery Apptainer\u003e Apptainer\u003e cat /.singularity.d/bootstrap_history/Apptainer0 # The original def file is also preserved  bootstrap: docker from: nvidia/cuda:12.1.1-devel-ubuntu22.04 %runscript /cuda-samples/Samples/1_Utilities/deviceQuery/deviceQuery %post apt-get update # update system apt-get install -y git # install git git clone https://github.com/NVIDIA/cuda-samples.git # clone target repository cd cuda-samples git fetch origin --tags \u0026\u0026 git checkout v12.1 # fetch certain repository version cd Samples/1_Utilities/deviceQuery \u0026\u0026 make # install certain tool As can be seen in this example, the new def file not only preserves the dependencies of the original image, but it also preserves a complete history of all build processes while giving flexible environment that can be customized as need arises.\nDeploying conda in a container There might be situations where you have a certain conda environment in your local machine that you need to set up in DAIC to commence your analysis. In such cases, deploying your conda environment in a container and sending this container to DAIC does the job for you.\nAs an example, let’s create a simple demo environment, demo-env in our local machine,\n$ hostname # check this is your machine $ conda create -n demo-env python=3.10 # create an environment containing python3.8 $ conda activate demo-env $ conda install numpy # install some conda libraries $ pip install matplotlib # install some pip libraries $ conda env export --from-history \u003e demo-env.yml # flag added to avoid unnecessary dependencies $ # You may manually remove the Prefix line from this file $ cat demo-env.yml # check the environment file name: demo-env channels: - defaults dependencies: - python=3.10 - numpy $ # Notice pip-installed libraries are not included in this file Now, it is time to create the container image. One option is to base the image on condaforge/mambaforge, which is a minimal Ubuntu installation with conda preinstalled at /opt/conda:\n$ cat demo-env-recipe.def Bootstrap: docker From: condaforge/mambaforge %files demo-env.yml /opt %post conda env create -f /opt/demo-env.yml # create environment from yml file conda clean -afy # remove unused packages and caches exec /opt/conda/envs/demo-env/bin/pip install matplotlib # install PyPI libraries This file is similar to the file in the Building images, with the addition of %files area. %files specifies the files in the host system (ie, your machine) that need to be copied to the container image, and optionally, where should they be available. In the previous example, the demo-env.yml file will be available in /opt/ in the container.\nNow, time to build and check the image:\n$ sudo apptainer build demo-env-image.sif demo-env-recipe.def INFO: Starting build... Getting image source signatures ... INFO: Creating SIF file... INFO: Build complete: demo-env-image.sif $ $ apptainer shell -C demo-env-image.sif Apptainer\u003e Apptainer\u003e source activate /opt/conda/envs/demo-env/ # activate the environment (demo-env) Apptainer\u003e python # run python and check imports Python 3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. \u003e\u003e\u003e import numpy \u003e\u003e\u003e import matplotlib \u003e\u003e\u003e exit() Apptainer\u003e exit $ scp demo-env-image.sif hpc-login:/tudelft.net/staff-umbrella/...\u003cYourDirectory\u003e/apptainer Now, to use the environment in this image to run code in a file, analysis.py, which uses some data to generate a plot:\n$ cat analysis.py # check python code #!/usr/bin/env python3 import matplotlib.pyplot as plt import numpy as np x = np.linspace(0, 2 * np.pi, 100) y = np.sin(x) plt.plot(x, y) plt.title('Sine Wave') plt.savefig('sine_wave.png') $ $ apptainer exec demo-env-image.sif /bin/bash -c \"source activate /opt/conda/envs/demo-env \u0026\u0026 python analysis.py\" $ ls # check the image file was created sine_wave.png  Warning In the last example, the container read and wrote a file to the host system directly. This behavior is risky. You are strongly recommended to expose only the desired host directories to the container. See Exposing host directories  Exposing host directories Depending on use case, it may be necessary for the container to read or write data from or to the host system. For example, to expose only files in a host directory called ProjectDataDir to the container image’s /mnt directory, add the --bind directive with appropriate \u003chostDir\u003e:\u003ccontainerDir\u003e mapping to the commands you use to launch the container, in conjunction with the -C flag eg, shell or exec as below:\n$ ls # check ProjectDataDir exists $ ls ProjectDataDir # check contents of ProjectDataDir raw_data.txt $ apptainer shell -C --bind ProjectDataDir:/mnt ubuntu_latest.sif # Launch the container with ProjectDataDir bound Apptainer\u003e ls Apptainer\u003e ls /mnt # check the files are accessible inside the container raw_data.txt Apptainer\u003e echo \"Date: $(date)\" \u003e\u003e raw_data.txt # edit the file Apptainer\u003e tail -n1 raw_data.txt # check the date was written to the file Apptainer\u003e exit # exit the container $ tail -n1 ProjectDataDir/raw_data.txt # check the change persisted If the desire is to expose this directory as read-only inside the container, the --mount directive should be used instead of --bind, with rodesignation as follows:\napptainer shell -C --mount type=bind,source=ProjectDataDir,destination=/mnt,ro ubuntu_latest.sif # Launch the container with ProjectDataDir bound Apptainer\u003e ls /mnt # check the files are accessible inside the container raw_data.txt Apptainer\u003e echo \"Date: $(date)\" \u003e\u003e /mnt/raw_data.txt # attempt to edit fails bash: tst: Read-only file system Advanced: containers and (fake) native installation It’s possible to use Apptainer to install and then use software as if it were installed natively in the host system. For example, if you are a bioinformatician, you may be using software like samtools or bcftools for many of your analyses, and it may be advantageous to call it directly. Let’s take this as an illustrative example:\n For hygiene, create the following file hierarchy: below a software directory an exec directory to put the container images and other executables, and a bin directory to contain softlinks:  $ mkdir -p software/bin/ software/exec Create the image definition file (or pull from a repository, as appropriate) and build:  $ cd software/exec $ $ cat bio-recipe.def Bootstrap: docker From: ubuntu:latest %post apt-get update # update system apt-get install -y samtools bcftools # install software apt-get clean # clean up $ sudo apptainer build bio-container.sif bio-recipe.def Now, create the following wrapper script:  $ cat wrapper_bio-container.sh #!/bin/bash containerdir=\"$(dirname $(readlink -f ${BASH_SOURCE[0]}))\" cmd=\"$(basename $0)\" apptainer exec \"${containerdir}/bio-container.sif\" \"$cmd\" \"$@\" $ $ chmod +x wrapper_bio-container.sh # make it executable Create the softlinks:  $ cd ../bin $ ln -s ../exec/wrapper_bio-container.sh samtools $ ln -s ../exec/wrapper_bio-container.sh bcftools Add the installation directory to your $PATH variable, and you will be able to call these tools  $ export PATH=$PATH:$PWD $ $ bcftools -v INFO: gocryptfs not found, will not be able to use gocryptfs bcftools 1.13 Using htslib 1.13+ds Copyright (C) 2021 Genome Research Ltd. License Expat: The MIT/Expat license This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. $ $ samtools version INFO: gocryptfs not found, will not be able to use gocryptfs samtools 1.13 Using htslib 1.13+ds Copyright (C) 2021 Genome Research Ltd.  Note  At the end of the previous steps, you will get the following tree structure. Please be mindful of when and where commands were executed.  $ tree software/ software/ ├── bin │ ├── bcftools -\u003e ../exec/wrapper.sh │ └── samtools -\u003e ../exec/wrapper.sh └── exec ├── bio-container.sif └── wrapper.sh  To permanently reflect changes to your $PATH variable, you may wish to add the step:  echo export PATH=$PATH:$PWD \u003e\u003e ~/.bash_profile   ","categories":"","description":"","excerpt":"Containerization Technology Containerization is a convenient means to …","ref":"/tutorials/container_images/","tags":"","title":"Container images"},{"body":"A compilation of FAQs is available in this word doc\n -- ","categories":"","description":"","excerpt":"A compilation of FAQs is available in this word doc\n -- ","ref":"/support/faqs/","tags":"","title":"Frequently asked questions"},{"body":"SSH: The authenticity of host ‘loginX’ can’t be established. When connecting to a login node for the first time, you must not continue when the key fingerprint reported by your ssh connection does not match one of the fingerprints shown here:\n For login1.hpc.tudelft.nl and login2.hpc.tudelft.nl:  2iPjH/j/Tf5JZU4OJyLpASA/GZ40eCqvcQnSSa++3nQ (ECDSA) MURg8IQL8oG5o2KsUwx1nXXgCJmDwHbttCJ9ljC9bFM (ED25519) mKgxUQvmOVM74XvFNhWt0ODsRvfnmwIgZWcw8uPJ68o (RSA) 05:24:a0:b4:83:27:05:32:4b:83:78:2a:20:99:f8:5c (ECDSA) C5:21:46:cb:73:cd:72:e6:18:04:d6:67:2a:67:90:75 (ED25519) 05:17:84:7f:9f:18:e3:71:b4:df:5e:c0:12:db:e8:fc (RSA)   For login3.hpc.tudelft.nl:  IaBwyYiZi1Etj7yBDtdv7sByHzH+hedW69QA8UxGUqk (ECDSA) O3AjQQjCfcrwJQ4Ix4dyGaUoYiIv/U+isMT5+sfeA5Q (ED25519) fslv0RnC9zkVBf34i3g1BPKaYBcsTgKqu8+PMKLTEvw (RSA) 5e:9a:69:30:75:d3:b5:75:29:b3:32:fc:48:ab:b2:f9 (ECDSA) 31:eb:cd:95:8f:d1:78:29:e1:70:f9:8b:b0:cd:56:5c (ED25519) ba:b9:92:4b:1a:00:8c:f1:aa:49:09:53:fa:b6:79:5f (RSA) When the key fingerprint matches, you can safely continue. When in the future your ssh connection tells you that the key has changed, and it doesn’t match one of the fingerprints above, contact the DAIC support team.    SSH: Permission denied, please try again.  The DAIC cluster is not freely accessible. It is facilitated by several departments and groups within the university for their research/education. If you have access, this message indicates that either your access expired (in case an end date was set), or your account was (temporarily) disabled due to problems with your use of the login nodes, or there is a problem with your NetID account/password.  -bash: cd: /home/nfs/\u003cNetID\u003e: Key has expired  This means your Kerberos ticket has expired. Your need to renew it, either by running kinit, or by logging out then logging in again (using your password!). Also see Kerberos authentication. Please log out when you’re not using the cluster (so you don’t hit this problem, and so you don’t block resources on the login node).  Disk quota exceeded  The size of the data in this storage has reached the maximum allowed size (also known as quota limit). For $HOME folders (see Personal storage) the maximum allowed amount of data is 8 GB, for project storage (see Project storage) the quota limit can be up to 5TB of data. To see how much space your $HOME files are using, run du -h ~ | sort -h. (When you have many files or folders, this can take a long time!) To make space available, you’ll either need to clean up some files (like installation archives and caches), or move some of your files somewhere else.  Note Your home folder is for storing settings and installing small software packages, not for storing data or large software installations. You need to store those in project storage. Your project leader/supervisor can request project storage for you via the Self Service Portal (TOPdesk).  The system load on login_X_ is too high! Please use another node if you can.  This message is mainly a warning for the person that is causing the high load. If that is you, you should either do the work as a cluster job, or limit the number of threads or memory that you use. If you’re not the one running heavy tasks, you can choose to ignore it.  staff-umbrella: Operation not permitted  The network filesystem for the bulk, groups and project storage (staff-bulk, staff-groups, staff-umbrella) does not support chmod (changing permissions) or chown (changing owner or group) operations:  When you run these operations, you will receive an Operation not permitted error. This has nothing to do with your personal rights, it’s just not supported. It’s also not necessary to change these, since the default permissions are correct for normal use. So, you can safely skip these operations or ignore these errors in many situations.   For rsync operations , use rsync -a --no-perms. If all else fails, a workaround is to (temporarily!) use the /tmp folder: move your folder that gives the error to /tmp, create a symbolic link from the folder in /tmp to the original location, rerun the commands that gave the error as before, then move your folder back from /tmp to the original location. For example, when you get an error in folder \u003cfoldername\u003e, do: mkdir /tmp/${USER} mv \u003cfoldername\u003e /tmp/${USER}/\u003cfoldername\u003e ln -s /tmp/${USER}/\u003cfoldername\u003e \u003cfoldername\u003e : : #\u003crerun command(s) that gave the error\u003e : rm \u003cfoldername\u003e mv /tmp/${USER}/\u003cfoldername\u003e \u003cfoldername\u003e rmdir /tmp/${USER}   ","categories":"","description":"","excerpt":"SSH: The authenticity of host ‘loginX’ can’t be established. When …","ref":"/support/faqs/general/","tags":"","title":"General questions"},{"body":"Purpose and Benefits of HPC Systems A High Performance Computing (HPC) cluster, is a collection of (large) computing resources, like Processors (CPUs), Graphics processors (GPUs), Memory and Storage, that are shared among a group of users.\nUsing multiple computers as such makes it possible to perform lengthy and resource-intense computations beyond the capabilities of a single computer, and is especially handy for modern scientific computing applications where datasets are typically large in size, models are big in parameters' size and complexity, and computations need specialized hardware (like GPUs and FPGAs).\nBrief history of DAIC The Delft AI Cluster (DAIC - formerly known as INSY-HPC or just plainly HPC) is an HPC cluster that was initiated within the INSY department in 2015. Later, resources were joined with ST, collectively called CS@Delft, and with other departments across faculties in subsequent expansion cycles. Today, DAIC servers are organized as partitions (see Batch Queuing System Overview ) that corresponds to the groups contributing these resources, as can be seen in Table 1.\nDAIC has been designed based on the needs of CS@Delft from the beginning. It has grown in time to serve researchers in other TU Delft Departments but maintained the needs of CS and AI in each expansion phase (See TU Delft clusters comparison).\n  Table 1: Current partitions within DAIC and contributing TU Delft departments/faculties.    I DAIC partition Contributor Faculty Faculty abbreviation (English/Dutch)     1 3dgi 3D Geoinformation Research Group Faculty of Architecture and the Built Environment ABE/BK   2 imphys Imaging Physics Faculty of Applied Sciences AS/TNW   3 cor Cognitive Robotics Faculty of Mechanical, Maritime and Materials Engineering 3mE   4 tbm Faculty of Technology, Policy and Management  Faculty of Technology, Policy and Management TPM/TBM   5 grs Geoscience \u0026 Remote Sensing Faculty Of Civil Engineering and Geosciences CEG/CiTG   6 influence Intelligent Systems Faculty of Electrical Engineering, Mathematics \u0026 Computer Science EEMCS/EWI   7 visionlab   8 insy   9 wis Software Technology   10 st    DAIC advisory board Thomas Abeel  Department of Intelligent Systems  Pattern Recognition and Bioinformatics group    Frans Oliehoek  Department of Intelligent Systems  Interactive Intelligence group    Asterios Katsifodimos  Software Technology Department  Web Informatics group     ","categories":"","description":"Why and What is DAIC?\n","excerpt":"Why and What is DAIC?\n","ref":"/docs/intro_daic/","tags":"","title":"Introducing DAIC"},{"body":" These are issues sys admins are aware of and are currently working to address\n ","categories":"","description":"","excerpt":" These are issues sys admins are aware of and are currently working to …","ref":"/support/known-issues/","tags":"","title":"Known issues"},{"body":"DAIC is a cluster dedicated for TU Delft researchers (eg, PhD students, postdocs, .. etc) from participating groups (see Brief history of DAIC).\nTo access DAIC resources, eligible candidates from these groups can request an account via the DAIC access request form. Additionally, requests for resources reservations can also be accommodated (see General cluster usage). To make such a request, please fill this form\nNote Please remember to post any scientific output based-off work performed on DAIC to the ScientificOutput MatterMost channel. A list of such outputs is found in Scientific outputs  Note To learn more about reservations, please refer to the Resources' reservation sections.  ","categories":"","description":"How can users access DAIC?\n","excerpt":"How can users access DAIC?\n","ref":"/docs/intro_daic/access_accounts/","tags":"","title":"Access and Accounts"},{"body":" Topics may be merged into others…\n  Specifications and availability of GPU and CPU resources Leveraging GPU resources for accelerated computing (when not to use CPUs) GPU programming frameworks (e.g., CUDA) and their integration with AI frameworks Guidelines for requesting and utilizing GPU and CPU resources efficiently. Optimizing AI code for GPU acceleration  References:\n  https://enccs.github.io/gpu-programming/\n  https://enccs.github.io/cuda/\n  https://enccs.github.io/OpenACC-CUDA-beginners/\n  https://www.nhr.kit.edu/userdocs/horeka/programming_offload/\n  ","categories":"","description":"When to use each?\n","excerpt":"When to use each?\n","ref":"/tutorials/cpu_gpu_resources/","tags":"","title":"CPU and GPU resources"},{"body":"Basic principles   On a cluster, it’s important that software is available and identical on all nodes, both login and compute nodes (see Batch queuing system). For self-installed software, it’s easier to install the software in one shared location than installing and maintaining the same software separately on every single node. You should therefore install your software on one of the network shares (eg, your $HOME folder or an umbrella or bulk folder) that are accessible from all nodes (see File system overview).\n  As a regular Linux user you don’t have administrator rights. Yet, you can do your normal work, including installing software in a personal folder, without needing administrator rights. Consequently, you don’t need (nor are you allowed) to use the sudo or su commands that are often shown in manuals.\n  Stop! Although both Linux flavors Red Hat Enterprise Linux (RHEL, CentOS, Scientific Linux, Fedora) and Debian (Ubuntu) can run the same Linux software, they use completely different package systems for installing software. The available software, packages' names and package versions might differ, and the package formats and package management tools are incompatible. This means:\n It is not possible to install Ubuntu or Debian .deb packages in CentOS or use apt-get to install software in DAIC. So when installing software, use a manual for CentOS, Red Hat or Fedora. If you can only find a manual for Ubuntu, you have to substitute the CentOS versions for any Ubuntu-specific packages or commands.   Using binaries when possible Some programs come as precompiled binaries or are written in a scripting language such as Perl, PHP, Python or shell script. Most of these programs don’t actually need to be “installed” since you can simply run these programs directly. In certain scenarios, you may need to make the program executable first:\n$ ./program # attempting to run the binary `program` -bash: ./program: Permission denied $ $ chmod +x program # making `program` executable, since it fails due to permissions $ $ ./program # checking `program` works! Hello world! $ Installing from source When a pre-made binary of your software is not available, you’ll have to install the software yourself from the source. You may need to set up your Installation environment before following this Installation recipe.\nInstallation environment When you are installing software for the very first time, you need to set up your environment. If you have already done this before , you can skip this section and go directly to the Installation recipe section.\nTo set up your environment, first, add the following lines to your ~/.bash_profile or, alternatively, download this (bash_profile.txt) as shown in the subsequent commands:\nbash_profile.txt  # Get the aliases and functions if [ -f ~/.bashrc ]; then . ~/.bashrc fi # User specific environment and startup settings export PREFIX=\"$HOME/.local\" export ACLOCAL_PATH=\"$PREFIX/share/aclocal${ACLOCAL_PATH:+:$ACLOCAL_PATH}\" export CPATH=\"$PREFIX/include${CPATH:+:$CPATH}\" export LD_LIBRARY_PATH=\"$PREFIX/lib64:$PREFIX/lib${LD_LIBRARY_PATH:+:$LD_LIBRARY_PATH}\" export LIBRARY_PATH=\"$PREFIX/lib64:$PREFIX/lib${LIBRARY_PATH:+:$LIBRARY_PATH}\" export MANPATH=\"$PREFIX/share/man${MANPATH:+:$MANPATH}\" export PATH=\"$HOME/bin:$PREFIX/bin:$PATH\" export PERL5LIB=\"$PREFIX/lib64/perl5:$PREFIX/share/perl5${PERL5LIB:+:$PERL5LIB}\" export PKG_CONFIG_PATH=\"$PREFIX/lib64/pkgconfig:$PREFIX/share/pkgconfig${PKG_CONFIG_PATH:+:$PKG_CONFIG_PATH}\" export PYTHONPATH=\"$PREFIX/lib/python2.7/site-packages${PYTHONPATH:+:$PYTHONPATH}\"    Note!  if you already have some of these settings in your ~/.bash_profile (or elsewhere), you should combine them so they don’t duplicate the paths. if you want to use python3.6 instead of python2.7, you need to set the PYTHONPATH to python3.6.   $ cp ~/.bash_profile ~/.bash_profile.bak # back up your file $ curl -s https://wiki.tudelft.nl/pub/Research/InsyCluster/InstallingSoftware/bash_profile.txt \u003e\u003e ~/.bash_profile # download and append the lines above $ $ # clean up any duplicate settings $ $ source ~/.bash_profile #  $ mkdir -p \"$PREFIX\" The line export PREFIX=\"$HOME/.local\" sets your software installation directory to /home/nfs/\u003cYourNetID\u003e/.local (which is the default and accessible on all nodes). This is in your personal home directory where you have a space quota of 8GB. However, for software for your research project, you should instead use a project share, for example:\nexport PREFIX=\"/tudelft.net/staff-umbrella/project/software\" The other variables will let you use your self-installed programs. You are now ready to install your software!\nInstallation recipe Software installation usually just requires you to follow the general installation recipe described below, but you always need to consult the documentation for your software.\n Place the source of the software in a folder under /tmp:  $ mkdir /tmp/$USER $ cd /tmp/$USER $ wget http://host/path/software.tar.gz $ tar -xzf software.tar.gz Or from github: git clone https://github.com/software $ cd software  Note Note: .tgz is the same as .tar.gz, for .tar.bz2 files use tar -xjf software.tar.bz2.   If the software provides a configure script, run it:  $ ./configure --prefix=\"$PREFIX\" If configure complains about missing software, you’ll either have to install that software, tell configure where it is (--with-feature _path_=) or disable the feature (--disable-feature).\nIf your software provides a CMakeLists.txt file, run cmake (note: the trailing two dots on the last line are needed exactly as shown):\n$ mkdir -p build $ cd build $ cmake -DCMAKE_INSTALL_PREFIX=\"$PREFIX\" .. Again, if cmake complains about missing software, you’ll either have to install that software or tell cmake where it is (-DCMAKE_SYSTEM_PREFIX_PATH=\"/usr/local;/usr;$PREFIX;path\").\nIf neither is provided, consult the documentation for dependencies and configuration (specifically for the installation directory).\nThere is no point in continuing until all reported problems have been fixed.\nCompile the software:  $ make If compilation is aborted due to an error, Google   the error for possible solutions. Again, there is no point in continuing until all reported problems have been fixed.\nInstall the software. When you used configure or cmake, you can simply run:  $ make install When you used neither, you need to use:\n$ make prefix=\"$PREFIX\" install Your software should now be ready to use, so check it:  $ cd $ _program_ When the program works, clean up /tmp/netid:  $ rm -r /tmp/$USER Python modules After setting up the installation environment (above) you are also able to install Python modules by yourself, by using the --user option. The easiest way is when the module is available through pip2 (for Python 2) or pip3 (for Python 3):\n$ pip2 search module $ pip2 install --user module When you only have the source code for the module, follow the installation instructions for the module, but make sure to use –user in the installation step:\n$ python setup.py install --user Containerization See the Containerization Technology tutorial\n","categories":"","description":"How to install unavailable software?\n","excerpt":"How to install unavailable software?\n","ref":"/docs/software_environment/installing_software/","tags":"","title":"Installing software"},{"body":"My program requires a newer version of CMake Use cmake3.\nHow can I run a Docker container? Using singularity. See the Container images tutorial for more information.\nMy program requires a newer version of GCC Newer versions of GCC are available through the devtoolset   modules. See the Environment modules for information on using modules.\nI want to use R There are a few options:\n You can use the pre-installed R You can install R using Conda   . Conda is available via the miniconda module. You can use R from a container   . Containers can be run using Apptainer on DAIC, as explained in this Container images tutorial.  How to use TensorBoard on the DAIC cluster?  TensorBoard is very insecure: anybody can connect to it, without authentication (i.e. when you run TensorBoard on the DAIC cluster, any TU Delft user can connect to it). And this is actually on purpose   , because making it secure and being able to guarantee that would require too much effort. So you can’t run TensorBoard directly on the DAIC cluster! The most secure way to run TensorBoard is to run it on your personal computer (with a proper firewall). When you put your TensorFlow log files on a network folder, you can access them directly on your personal computer so you can use TensorBoard in the same way as you do in the DAIC cluster. (You can also download the log files if you find that easier.)  ","categories":"","description":"","excerpt":"My program requires a newer version of CMake Use cmake3.\nHow can I run …","ref":"/support/faqs/software/","tags":"","title":"Software questions"},{"body":"Compute Nodes: CPUs and GPUs DAIC compute nodes are all multi CPU servers, with large memories, and some with GPUs. The nodes in the cluster are heterogeneous, i.e. they have different types of hardware (processors, memory, GPUs), different functionality (some more advanced than others) and different performance characteristics. If a program requires specific features, you need to specifically request those for that job (see Job scripts). The following table gives an overview of current nodes and their characteristics:\n  Table 1: Overview of DAIC compute nodes    All nodes are also parts of the general partition.   Abbreviations: 10gbe: 10 Gigabit Ethernet network connection (upgrade from the default 1 Gigabit Ethernet connection), ib InfiniBand connection, ssdSolid-State Disk for /tmp storage (instead of the default spinning disk), bigmem: For jobs that needs a lot of memory, so remaining resources are available to others, imphysexclusive: imphys nodes use infiband for jobs that run across a large number of nodes, and it is handy that those nodes are somewhat reserved, avx512: extra instructions on cpu, like avx1 and avx2 only on the newer CPUs, if you have code that need them, gpumem32: gpu memory for newer GPUs: do not use gpu types, but use this feature instead if you have a need for gpu with more memory, nvme: Non-Volatile Memory Express for tmp storage       PARTITION NODELIST CPUS MEMORY (MB) GRES ACTIVE_FEATURES     100plus 100plus 64 768000 - avx,avx2,ht,10gbe,bigmem   3dgi 3dgi[1-2] 64 256000 - avx,avx2,ht,10gbe,ssd   gpu11 96 512000 gpu:a40:3 avx,avx2,ht,10gbe,bigmem,gpumem32,ssd   cor cor1 64 1536000 gpu:v100:8 avx,avx2,ht,10gbe,avx512,gpumem32,ssd   grs grs[1-4] 32 256000 - avx,avx2,ht,ib,ssd   imphys awi01 72 384000 gpu:v100:1 avx,avx2,ht,10gbe,avx512,gpumem32,nvme,ssd   awi02 56 512000 gpu:v100:2 avx,avx2,ht,10gbe,bigmem,ssd   awi[03-14] 56 512000 - avx,avx2,ht,ib,imphysexclusive   awi[15-26] 56+ 256000+ - avx,avx2,ht,ib,ssd   gpu[09-10] 96 512000 gpu:a40:3 avx,avx2,ht,10gbe,bigmem,gpumem32,ssd   influence influ1 64 384000 gpu:turing:8 avx,avx2,ht,10gbe,avx512,nvme,ssd   influ[2-3] 64 190000 gpu:turing:4 avx,avx2,ht,10gbe,avx512,ssd   influ4 128 256000 - avx,avx2,ht,10gbe,ssd   influ[5-6] 128 512000 - avx,avx2,ht,10gbe,bigmem,ssd   insy 100plus 64 768000 - avx,avx2,ht,10gbe,bigmem   gpu[01-04] 96 512000 gpu:a40:3 avx,avx2,ht,10gbe,bigmem,gpumem32,ssd   influ1 64 384000 gpu:turing:8 avx,avx2,ht,10gbe,avx512,nvme,ssd   influ[2-3] 64 190000 gpu:turing:4 avx,avx2,ht,10gbe,avx512,ssd   influ4 128 256000 - avx,avx2,ht,10gbe,ssd   influ[5-6] 128 512000 - avx,avx2,ht,10gbe,bigmem,ssd   insy11 64 256000 gpu:pascal:5 avx,avx2,ht,10gbe   insy12 64 256000 gpu:pascal:7 avx,avx2,ht,10gbe   insy[13-14] 64 256000 gpu:pascal:8 avx,avx2,ht,10gbe   insy[15-16] 64 768000 gpu:turing:4 avx,avx2,ht,10gbe,avx512,bigmem,ssd   st gpu[05-08] 96 512000 gpu:a40:3 avx,avx2,ht,10gbe,bigmem,gpumem32,ssd   wis1 64 768000 gpu:p100:2 avx,avx2,ht,10gbe,avx512,bigmem   tbm tbm5 64 768000 - avx,avx2,ht,10gbe,avx512,bigmem,ssd   visionlab insy11 64 256000 gpu:pascal:5 avx,avx2,ht,10gbe   insy12 64 256000 gpu:pascal:7 avx,avx2,ht,10gbe   insy[13-14] 64 256000 gpu:pascal:8 avx,avx2,ht,10gbe   wis wis1 64 768000 gpu:p100:2 avx,avx2,ht,10gbe,avx512,bigmem    Total  59 compute nodes  4064 cores 25620 GB 99 GPUs   Note All servers have Advanced Vector Extensions 1 and 2 (AVX, AVX2) support, and hyper-threading (ht) processors (two CPUs per core, always allocated in pairs).  Note You can use Slurm’s sinfo command to get various information about cluster nodes. For example, to get an overview of compute nodes on DAIC, you can use the command:\n$ sinfo --all --format=\"%P %N %c %m %G %b\" --hide -S P,N -a | grep -v \"general\" | awk 'NR==1 {print; next} {match($5, /gpu:[^,]+:[0-9]+/); if (RSTART) print $1, $2, $3, $4, substr($5, RSTART, RLENGTH), $6; else print $1, $2, $3, $4, \"-\", $6 }' Check out the Slurm’s sinfo page and wikipedia’s awk page for more info on these commands.\n CPUs All nodes have multiple Central Processing Units (CPUs) that perform the operations. Each CPU can process one thread (i.e. a separate string of computer code) at a time. A computer program consists of one or multiple threads, and thus needs one or multiple CPUs simultaneously to do its computations (see wikipedia's CPU page   ).\nNote Most programs use a fixed number of threads. Requesting more CPUs for a program than its number of threads will not make it any faster because it won’t know how to use the extra CPUs. When a program has less CPUs available than its number of threads, the threads will have to time-share the available CPUs (i.e. each thread only gets part-time use of a CPU), and, as a result, the program will run slower (And even slower because of the added overhead of the switching of the threads). So it’s always necessary to match the number of CPUs to the number of threads, or the other way around. See Job scripts for setting resources for batch jobs.  The number of threads running simultaneously determines the load of a server. If the number of running threads is equal to the number of available CPUs, the server is loaded 100% (or 1.00). When the number of threads that want to run exceed the number of available CPUs, the load rises above 100%.\nThe CPU functionality is provided by the hardware cores in the processor chips in the machines. Traditionally, one physical core contained one logical CPU, thus the CPUs operated completely independent. Most current chips feature hyper-threading: one core contains two (or more) logical CPUs. These CPUs share parts of the core and the cache, so one CPU may have to wait when a shared resource is in use by the other CPU. Therefore these CPUs are always allocated in pairs by the job scheduler.\nMemory All machines have large main memories for performing computations on big data sets. A job cannot use more than it’s allocated amount of memory. If it needs to use more memory, it will fail or be killed. It’s not possible to combine the memory from multiple nodes for a single task. 32-bit programs can only address (use) up to 3Gb (gigabytes) of memory. See Job scripts for setting resources for batch jobs.\nGPUs A few types of GPUs are available in some of DAIC nodes, as shown in table 1. The total numbers of these GPUs/type and their technical specifications are shown in table 2. See Jobs on GPU resources for requesting a certain GPU for a computational job.\nStop! It’s forbidden to use DAIC GPUs for anything other than research!     Table 2: Counts and specifications of DAIC GPUs    GPU type\n Count Model  Architecture Compute Capability  CUDA cores  Memory     a40 33 NVIDIA A40 Ampere 8.6 10752 46068 MiB   p100 2 Tesla P100-PCIE-16GB Pascal 6.0 3584 16384 MiB   pascal 29 GeForce GTX 1080 Ti Pascal 6.1 3584 11264 MiB   turing 24 NVIDIA GeForce RTX 2080 Ti Turing 7.5 4352 11264 MiB   v100 11 Tesla V100-SXM2-32GB Volta 7.0 5120 32768 MiB    In table 2: the headers denote:\n Model: The official product name of the GPU Architecture: The hardware design used, and thus the hardware specifications and performance characteristics of the GPU. Each new architecture brings forward a new generation of GPUs.  Compute capability: determines the general functionality, available features and CUDA support of the GPU. A GPU with a higher capability supports more advanced functionality.  CUDA cores: The number of cores perform the computations: The more cores, the more work can be done in parallel (provided that the algorithm can make use of higher parallelization).  Memory: Total installed GPU memory. The GPUs provide their own internal (fixed-size) memory for storing data for GPU computations. All required data needs to fit in the internal memory or your computations will suffer a big performance penalty.   Note To inspect a given GPU and obtain the data of table 2, you can run the following commands on an interactive session or an sbatch script (see Jobs on GPU resources). The apptainer image used in this code snippet was built as demonstrated in the Building images tutorial.\n$ sinteractive --cpus-per-task=2 --mem=500 --time=00:02:00 --gres=gpu Note: interactive sessions are automatically terminated when they reach their time limit (1 hour)! srun: job 8607783 queued and waiting for resources srun: job 8607783 has been allocated resources 15:50:29 up 51 days, 3:26, 0 users, load average: 60,33, 59,72, 54,65 SomeNetID@influ1:~$ SomeNetID@influ1:~$ SomeNetID@influ1:~$ nvidia-smi --format=csv,noheader --query-gpu=name\tNVIDIA GeForce RTX 2080 Ti SomeNetID@influ1:~$ SomeNetID@influ1:~$ SomeNetID@influ1:~$ nvidia-smi -q | grep Architecture\tProduct Architecture : Turing SomeNetID@influ1:~$ SomeNetID@influ1:~$ SomeNetID@influ1:~$ nvidia-smi --query-gpu=compute_cap --format=csv,noheader 7.5\tSomeNetID@influ1:~$ SomeNetID@influ1:~$ SomeNetID@influ1:~$ apptainer run --nv cuda_based_image.sif | grep \"CUDA Cores\"\t# using the apptainer image of the tutorial (068) Multiprocessors, (064) CUDA Cores/MP: 4352 CUDA Cores SomeNetID@influ1:~$ SomeNetID@influ1:~$ SomeNetID@influ1:~$ nvidia-smi --format=csv,noheader --query-gpu=memory.total 11264 MiB SomeNetID@influ1:~$ SomeNetID@influ1:~$ exit   Storage Systems Networking DAIC compared with other TU Delft clusters For an overview of other compute environments accessible to TU Delft affiliates and their collaborators, see TU Delft clusters comparison\n","categories":"","description":"What are the components of the DAIC cluster?\n","excerpt":"What are the components of the DAIC cluster?\n","ref":"/docs/intro_daic/hardware_infra/","tags":"","title":"Hardware infrastructure"},{"body":" Techniques for scaling deep learning models on HPC systems Distributed training strategies (e.g., data parallelism, model parallelism) Parallelization and optimization for large-scale deep learning Data Management/ storage startegies for AI Workloads/projects Benchmarking and performance evaluation of AI applications AI-Specific case studies and examples/ AI workflows and best practices  ","categories":"","description":"What does your user need to know to try your project?\n","excerpt":"What does your user need to know to try your project?\n","ref":"/tutorials/deep_learning/","tags":"","title":"High-Performance Deep Learning"},{"body":"What are the limits?  The hard limits are in place only to protect the cluster from extreme overloads. The guiding principles of the cluster are fair-share and fair-use: all users should be able to use the cluster at the same time, and nobody should cause problems for the cluster or other users (see Slurm’s job scheduling and waiting times). So you need to make sure that your jobs do not unfairly hinder other jobs.  What is the minimum runtime for a job?  Submitting, scheduling and starting jobs bring along a certain overhead. To reduce the effects of the overhead, jobs need to run for at least 1 minute, but preferably 5 minutes to 1 hour. So limit the number of CPUs to at most 2 to 4 (to make the jobs easier to schedule and not finish too quickly) and where possible combine multiple jobs into one job (to reduce the number of jobs to at most 50 and add together the runtime of short jobs).  How to determine the CPU load, memory use and number of active threads of your program?  Log in on a login node (login1 or login3) and run your program (python …). Log in a second time on the same login node and run top -H -o TIME -u $USER to see all your threads and their %CPU and %MEM use.  A %CPU of 100 corresponds to 1 CPU, 200 to 2 CPUs, and less than 75 is an indication that your program is not able to fully (optimally) use a CPU. %MEM is a percentage of 16 GB (login1) or 512 GB (login3); you’ll need to convert the %MEM to MB or GB for your job script (round up to the next GB for a little extra headroom). To determine the active threads of the program, count the threads (belonging to the program) with %CPU \u003e 5 and steadily increasing TIME+. (Quit top by pressing q.)   For running jobs, the CPU Load statistics on the website is one indicator of problems (when the CPU load exceeds 100%). Background: the system restricts a job so that it can only use the allocated CPU cores, and a CPU core can only run one thread at a time. So the actual CPU use cannot exceed the allocated number of CPU cores. When your program uses more active threads than the number of allocated CPU cores, only some threads can run, and the rest of the threads have to wait. So the CPU load, i.e. the number of active (running and waiting) threads per core, goes above 100%. Having threads waiting reduces the performance (and adds overhead).  Example\nLet’s assume you want to know the sum of 50 numbers. The simplest way to calculate this is to add the first two numbers, then add the third number to that, and so on. You will need to perform 49 additions, one at a time (one after the other), in one sequence, thus 1 thread. To run this single thread, a single CPU is needed (to perform one addition at a time). The “time” needed to compute the sum is 49 CPU cycles (1 addition per cycle). The CPU load is 100% (1 active thread per CPU) and the CPU efficiency is 100% (the one CPU will be active the whole time).\nA simple way to speed this up would be to divide the 50 numbers in two groups of 25 numbers, and use 2 separate threads to calculate the sums of the two groups at the same time (in parallel). Then add the sums of the two groups to get the total sum. Each thread needs its own CPU to perform the addition, so you need 2 CPUs to be able to run the 2 threads in parallel. Afterwards you will need to add the two sums together to get the total sum. So the time needed when using 2 threads is 25 cycles (24 additions to sum each group, and 1 addition for the total). The average CPU load will be 98% (1 active thread per CPU for 24 cycles, plus one final addition on only 1 CPU; the other CPU will be idle). The average CPU efficiency is also 98% (49 additions in 50 available CPU cycles).\nThe 2 threads could also have shared 1 CPU (by first doing an addition in the first thread, temporarily storing that result, then doing an addition in the second thread, temporarily storing that result, and so on). However, because of the additional overhead, it would take longer than by using one thread. The average CPU load would be 196% (24 additions with 2 active threads on 1 CPU, plus the final addition). The CPU efficiency would be 100% (the one CPU will be active the whole time), but for a longer time. In general, more than 1 active thread per CPU is less efficient.\nYou might have jumped to the conclusion that more threads would be even better, right? What happens when you divide the numbers in 8 groups (6 groups of 6 numbers and 2 groups of 7 numbers), and use 8 threads (on 8 CPUs) in parallel to sum those groups? The 6 threads that sum 6 numbers will be finished in 5 cycles, but the 2 threads that sum 7 numbers need 6 cycles. So those 6 CPUs will have to wait idle for one CPU cycle for the other 2 threads to finish. Then you still have to add the sums of the 8 groups to get the total sum. If you use 1 thread for this, you will need 7 CPU cycles. During this time, the other 7 CPUs will all be waiting idle!\nThe time needed would be 13 cycles (excluding any additional overhead): 6 cycles to sum the groups and 7 cycles to calculate the total. The first 5 cycles 8 active threads run (on the 8 CPUs), the next cycle only 2 active threads remain and the final 7 cycles only 1 active thread runs. So the average CPU load is only 47% (= (5 * (8 / 8) + (2 / 8) + 7 * (1 / 8)) / 13). In total 104 CPU cycles (13 cycles * 8 CPUs) were available to perform the 49 additions, so the CPU efficiency is only 47% (= 49 / 104). In general, the more threads you use, the lower the average CPU efficiency!\nConclusion: in this example, you can run 4 jobs with 2 threads each in less time than it takes to run 2 jobs with 8 threads each (especially considering that larger jobs often have longer waiting times). So choose your number of threads optimally.\nHow to determine the GPU use of your program?  Log in on a login node with a GPU (login1 or login3). Run nvidia-smi -l. Make sure no processes are using the GPU. Log in a second time on the same login node and run your program (python …). The current GPU utilization of your program is reported by nvidia-smi under GPU-Util. Quit your program and nvidia-smi by pressing Ctrl+c, and log out from the login node.  How do I request CPUs for a multithreaded program?   If you can specify the number of threads your program will use:\n determine a “smart“ number of threads ( for example the number of currently available CPUs, or the number of threads needed to finish within 4 hours), always a multiple of 2, but preferably no more than 8, request a cpu for each thread (--cpus-per-task=\u003c#threads\u003e), and tell your program to use the $SLURM_CPUS_PER_TASK variable for threads. #/bin/sh #SBATCH --ntasks=1 --cpus-per-task=2 srun ggsearch36 -t \"$SLURM_CPUS_PER_TASK\"     If your program uses a fixed number (2, 4,..) of threads:\n request a cpu for each thread (#SBATCH --cpus-per-task=\u003c#threads\u003e). #/bin/sh #SBATCH --ntasks=1 --cpus-per-task=2 srun my_program     If your program automatically uses as many threads as there are CPUs on a node (for example java programs):\n  do not use functions that detect the total number of CPUs of a computer (for example, os.cpu_count() for Python), instead, use functions that detect the CPU affinity (for example, len(os.sched_getaffinity(0)) for Python).\n  for some code you can explicitly specify the correct number of threads using an environment variable (export \u003cVARIABLE\u003e=\u003cvalue\u003e):\n#/bin/sh #SBATCH --ntasks=1 --cpus-per-task=2 export NUMBA_NUM_THREADS=\"$SLURM_CPUS_PER_TASK\" srun python script.py   If that is not possible, request a complete node for one task (#SBATCH --ntasks=1 --exclusive), and tell srun to use \"$SLURM_CPUS_ON_NODE\" threads.\n#/bin/sh #SBATCH --ntasks=1 --exclusive srun --cpus-per-task=\"$SLURM_CPUS_ON_NODE\" java_program     How can I use a GPU?  See Jobs on GPU resources  How can I let multiple programs use the same GPU?   If you want to let multiple instances of your program share a GPU, you’ll have to start them in parallel from the same job using srun.\n#!/bin/sh #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 srun program   If you want to use different programs, use the --multi-prog option:\njob.conf:\n0 \u003cprogram 1\u003e 1 \u003cprogram 2\u003e job.sh:\n#!/bin/sh #SBATCH --gres=gpu #SBATCH --ntasks=2 srun --multi-prog job.conf   How much memory can I use?  A task can run on a single node only. Since most jobs consist of a single task, those jobs are therefore limited to the total amount of memory in a node. However, since that would leave no memory for other jobs on the node, do not request more memory than your jobs need! (Also see How can I see the CPU or RAM usage of a job?). There is also a per-user and per-QoS memory limit for the combined requested memory of all running jobs (of a user) in a certain QoS (see Partitions and Quality of service). So, to run the most jobs at the same time, don’t request more memory than your jobs need. The average amount of memory that you can request when you want to run a lot of jobs is less than 8Gb per job (less than 4Gb per CPU). This will give you the most running jobs on the cluster.  How can I see the CPU or RAM usage of a job?  sstat shows specific usage information for (running) job steps (i.e. when you start your program using srun program), using something like this (all on one line): sstat --allsteps --format=JobID,NTasks,MinCPU,MaxRSS,MaxDiskRead,MaxDiskWrite \u003cjobid\u003e The MaxRSS field, for example, shows the maximum amount of memory used until now. seff \u003cjobid\u003e shows basic CPU and memory usage statistics of the whole job, including easy to understand percentages, but only for finished jobs.  How can I see the GPU usage of a job?  See Jobs on GPU resources  How can I limit the number of jobs per node?  When a job risks overloading a node (for example because it creates a large load on the network storage, or because it uses a lot of space in /tmp), you need to explicitly limit the number of jobs that can be run simultaneously on a node. One way to do this is to use the GRES (Generic consumable RESource) jobspernode. This allows you to limit the number of jobs per node to one, two, three or four. You do this by requesting one of the following GRES in your jobs (pick the one you need): #SBATCH --gres=jobspernode:one:1 #SBATCH --gres=jobspernode:two:1 #SBATCH --gres=jobspernode:three:1 #SBATCH --gres=jobspernode:four:1 When you specify one of these in your job, for example the one for four jobs per node, the scheduler will start (up to) four jobs on one node, and will then wait until a job finishes before starting another job on that node.  Should I feel guilty about running a lot of jobs with GPU/CPU usage?  Actually, the more jobs running and waiting in the queue, the more efficient and fair the scheduler can plan and divide the resources over the waiting jobs, so the higher the throughput and the sooner all work is finished. The scheduler will make sure that the available resources are divided fairly between the jobs of all users. This is done based on previous usage: the amount of allocated CPUs, GPUs and memory multiplied by a job’s real runtime. The higher a user’s previous usage, the lower the priority of that user’s jobs waiting in the queue. So when one user has a high previous usage, the waiting jobs of other users will be started before the jobs of that user. (The previous usage continuously decays, so when the usage stops, that user’s priority automatically goes back up again in a few days.)  How do I clean up /tmp (when a job fails)   When your job stores temporary data locally on a node, your job needs to clean up this data before it exits. So include an rm command at the end of your job script. (The system does not clean up this data automatically.)\n  When the job fails (or is canceled or hits a timeout), the clean up requires some special code in the job script:\n#!/bin/sh #SBATCH ... # Your usual resources' specifications tmpdir=\"/tmp/${USER}/${SLURM_JOBID}\" # Create local temporary folder mkdir --parents \"$tmpdir\" # You may want to uncomment this to know what to clean up when the clean up fails: #echo \"Temporary folder: $(hostname --short):${tmpdir}\" function clean_up { # Cleanup temporary folder rm --recursive --force \"$tmpdir\" \u0026\u0026 echo \"Clean up of $tmpdircompleted successfully.\" exit } trap 'clean_up' EXIT # Setup clean_up to run on exit # Your code- Make sure your program uses this \"$tmpdir\" location srun …   If all else fails, login interactively to the node and manually clean up the files:\nsinteractive --nodelist=\u003cnode\u003e   Request a node with enough space in /tmp (at least twice what your script needs, because other jobs need to use /tmp too):\n#SBATCH --tmp=\u003csize\u003eG   ","categories":"","description":"","excerpt":"What are the limits?  The hard limits are in place only to protect the …","ref":"/support/faqs/job_resources/","tags":"","title":"Job resources questions"},{"body":"     -- Overall impact Since 2015, DAIC has facilitated more than 2000 scientific outputs from the various DAIC-participating departments:\n    Article Conference/Meeting contribution Book/Book chapter/Book editing Dissertation (TU Delft) Abstract Other Editorial Patent Grand Total     Grand Total 1067 854 123 99 69 32 29 8 2281    These outputs span a wide range of application areas, with titles reflecting an emphasis on data analysis and machine learning:\n  Wordcloud of the most common words in titles of Scientific outputs produced via DAIC\n  Reference The table and wordcloud provided here are based on retrospective retrieval of all DAIC users' scientific outputs between 2015-2023 from TU Delft’s Pure database. The data has been generated by the Strategic Development – Data Insights team.  Scientific outputs Note The compilation of the following list is done retrospectively by the Data Insights team and/or is based on self-reporting by individual researchers. As a result, it may not be exhaustive nor complete.\n If your paper is not in this list, then please post its details to the ScientificOutput MatterMost channel; and make sure to acknowledge and cite us as shown in the Acknowledgement section.   Acknowledgement   Research reported in this work was partially or completely facilitated by computational resources and support of the the Delft AI Cluster (DAIC) at TU Delft (URL:https://doc.daic.tudelft.nl/), but remains the sole responsibility of the authors, not the DAIC team.\n   ","categories":"","description":"How has DAIC contributed to TU Delft research?\n","excerpt":"How has DAIC contributed to TU Delft research?\n","ref":"/docs/intro_daic/scientific_outputs/","tags":"","title":"Scientific outputs"},{"body":"SSH access If you have a valid DAIC account (see Access and Accounts page), you can access DAIC resources using an SSH client. SSH (Secure SHell) is a protocol that allows you to connect to a remote computer via a secure network connection. SSH supports remote command-line login and remote command execution. SCP (Secure CoPy) and SFTP (Secure File Transfer Protocol) are file transfer protocols based on SSH (see wikipedia's ssh page   ).\nCommand line access Most modern operating systems, including Linux, macOS and Windows 10, come by default with SSH, SCP and SFTP clients from OpenSSH pre-installed (but you can also use a third-party SSH or SFTP program). To connect to DAIC within TU Delft network (ie, via eduram or wired connection), open a command-line interface (prompt, or terminal, see Wikipedia's CLI page   ), and run the following command:\n$ ssh [\u003cYourNetID\u003e@]login.daic.tudelft.nl This command logs you in into one of the three login nodes of DAIC (login[1-3]), where \u003cYourNetID\u003e is your TU Delft’s NetID. You can optionally omit the square brackets and their contents if the username on the machine you are connecting from matches your NetID.\nNote The first time that you connect to an SSH server, you will be asked to confirm the server’s identity. This identity will be used in future sessions to detect (evil) server changes. If the SSH client later on detects a change, do not connect, unless you have been informed that the identity was changed legitimately.\nThe authenticity of host 'login.daic.tudelft.nl (131.180.183.244)' can't be established. ED25519 key fingerprint is SHA256:MURg8IQL8oG5o2KsUwx1nXXgCJmDwHbttCJ9ljC9bFM. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added 'login.daic.tudelft.nl' (ED25519) to the list of known hosts.   The server then asks you for your password, so enter it to proceed to the welcome screen. Note that the password you print will not appear:\nThe HPC cluster is restricted to authorized users only. YourNetID@login.daic.tudelft.nl's password: Last login: Mon Jul 24 18:36:23 2023 from tud262823.ws.tudelft.net ######################################################################### # # # Welcome to login1, login server of the HPC cluster. # # # # By using this cluster you agree to the terms and conditions. # # # # For information about using the HPC cluster, see: # # https://login.hpc.tudelft.nl/ # # # # The bulk, group and project shares are available under /tudelft.net/, # # your windows home share is available under /winhome/$USER/. # # # ######################################################################### 18:40:16 up 51 days, 6:53, 9 users, load average: 0,82, 0,36, 0,53 YourNetID@login1:~$ hostname login1.hpc.tudelft.nl YourNetID@login1:~$ YourNetID@login1:~$ YourNetID@login1:~$ echo $HOME /home/nfs/YourNetID YourNetID@login1:~$ YourNetID@login1:~$ pwd /home/nfs/YourNetID YourNetID@login1:~$ YourNetID@login1:~$ YourNetID@login1:~$ exit logout Connection to login.daic.tudelft.nl closed. In this example, the user, YourNetID, is logged in via the login node login1.hpc.tudelft.nl as can be seen from the hostname output. The user has landed in the $HOME directory, as can be seen by printing its value, and checked by the pwd command. Finally, the exit command is used to exit the cluster.\nAlternatively, if you like to run graphical applications on DAIC, you need to enable X11 forwarding via the -X option as follows:\n$ ssh -X [\u003cYourNetID\u003e@]linux-bastion.tudelft.nl Configuration files For convenience, you can place certain information about your SSH connections to a configuration file in your local machine. The SSH configuration file can be found in ~/.ssh/config on Linux systems, and in C:\\Users\\\u003cYourUserName\u003e\\.ssh on Windows.\nFor example, on a Linux system, you can place the following lines in the configuration file:\n$ cat ~/.ssh/config Host daic HostName login.daic.tudelft.nl User \u003cYourNetID\u003e Port 22 where:\n The Host keyword starts the SSH configuration block and specifies the name (or pattern of names, like daic in this example) to which the configuration entries will apply. The HostName is the actual hostname to log into. Numeric IP addresses are also permitted (both on the command line and in HostName specifications). The User is the login username. This is especially important when the username differs between your machine and the remote server/cluster. The Port is the port number on the remote host. Port 22 is the default port for ssh.  You can then connect to DAIC by just typing the following command:\n$ ssh daic The HPC cluster is restricted to authorized users only. YourNetID@login.daic.tudelft.nl's password: Last login: Tue Jul 25 02:24:49 2023 from srv228.tudelft.net ######################################################################### # # # Welcome to login1, login server of the HPC cluster. # # # # By using this cluster you agree to the terms and conditions. # # # # For information about using the HPC cluster, see: # # https://login.hpc.tudelft.nl/ # # # # The bulk, group and project shares are available under /tudelft.net/, # # your windows home share is available under /winhome/$USER/. # # # ######################################################################### 02:24:59 up 51 days, 14:38, 1 user, load average: 0,08, 0,10, 0,13 YourNetID@login1:~$ YourNetID@login1:~$ hostname # check you are in DAIC login1.hpc.tudelft.nl  Warning This method of access applies only when connecting from within TU Delft’s network. If connecting from outside the network, for example, from home, follow the instructions in Access from outside university network  Graphical clients For Windows, the (free) graphical clients PuTTY (SSH) and FileZilla (SFTP) are available (see official PuTTY page   and FileZilla page   ). On machines with a TUD-configured Windows installation, you can find PuTTY under Start -\u003e All Programs -\u003e Tools -\u003e Putty Suite -\u003e PuTTY and FileZilla under Start -\u003e All Programs -\u003e Internet -\u003e Filezilla FTP Client-\u003e FileZilla.\nIn Linux, you can use your default file manager (Konqueror or Nautilus) for SFTP, and just run SSH from a terminal. PuTTY (SSH) and FileZilla (SFTP) are available, but have to be installed by hand.\nMachines with a TUD-configured Mac OS X installation come with Fetch (SFTP) installed in the Application folder. FileZilla (SFTP) is available, but has to be installed. For SSH, it’s probably easiest to just run SSH from a terminal.\nAccess from outside university network Direct access to DAIC from outside the university network is blocked by a firewall. Thus, to access DAIC, there are two options. You can either:\n Use TU Delft’s EduVPN or other recommended VPN. Once connected to a VPN, you can ssh to DAIC directly, as in SSH access. See TU Delft Access via VPN   page, or Connect via Linux bastion server. In this case, first, you connect (using an SSH or SCP/SFTP client) to the bastion server, and then ssh into DAIC, as depicted in Fig 1.    Connecting to DAIC from outside TU Delft network\n  For the linux bastion, if you are an employee or guest, use linux-bastion.tudelft.nl. If you are a student (BSc or MSc), then use student-linux.tudelft.nl as per the following examples:\n$ hostname # check this is your local machine $ ssh YourNetID@linux-bastion.tudelft.nl The authenticity of host 'linux-bastion.tudelft.nl (131.180.123.195)' can't be established. ED25519 key fingerprint is SHA256:VJUFsQkIebODETsXwczkInnRrpdYYqAZDbsoKP1we+A. This key is not known by any other names Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added 'linux-bastion.tudelft.nl' (ED25519) to the list of known hosts. YourNetID@linux-bastion.tudelft.nl's password: ____ ____ _____ ___ _ ____ _|___ \\|___ \\___ | / __| '__\\ \\ / / __) | __) | / / \\__ \\ | \\ V / / __/ / __/ / / |___/_| \\_/ |_____|_____/_/ YourNetID@srv227:~$ YourNetID@srv227:~$ hostname # check you are on the bastion server srv227.tudelft.net YourNetID@srv227:~$ YourNetID@srv227:~$ ssh login.daic The authenticity of host 'login.daic (131.180.183.244)' can't be established. ECDSA key fingerprint is SHA256:2iPjH/j/Tf5JZU4OJyLpASA/GZ40eCqvcQnSSa++3nQ. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added 'login.daic' (ECDSA) to the list of known hosts. The HPC cluster is restricted to authorized users only. YourNetID@login.daic's password: Last login: Tue Jul 25 01:32:08 2023 from srv227.tudelft.net ######################################################################### # # # Welcome to login1, login server of the HPC cluster. # # # # By using this cluster you agree to the terms and conditions. # # # # For information about using the HPC cluster, see: # # https://login.hpc.tudelft.nl/ # # # # The bulk, group and project shares are available under /tudelft.net/, # # your windows home share is available under /winhome/$USER/. # # # ######################################################################### 01:28:15 up 51 days, 13:41, 1 user, load average: 0,10, 0,13, 0,14 YourNetID@login1:~$ YourNetID@login1:~$ YourNetID@login1:~$ hostname login1.hpc.tudelft.nl YourNetID@login1:~$ YourNetID@login1:~$ As seen in the Configuration files section, you can simplify access by using an ssh configuration file. Assuming you are an employee or guest, add the following to your ssh config file:\nHost bastion Hostname linux-bastion.tudelft.nl # If you are student, use: student-linux.tudelft.nl instead User \u003cYourNetID\u003e PreferredAuthentications password You can then connect simply by:\n$ hostname # check you are on your local machine $ ssh bastion YourNetID@linux-bastion-ex.tudelft.nl's password: ____ ____ ___ ___ _ ____ _|___ \\|___ \\( _ ) / __| '__\\ \\ / / __) | __) / _ \\ \\__ \\ | \\ V / / __/ / __/ (_) | |___/_| \\_/ |_____|_____\\___/ !! Attention dear users !! ===== This server is not meant for storing large files. It's mainly for hopping to another server and storing your ssh keys. Your homedirectory is therefore limited in space. If you store large files on this server anyway we reserve the right to remove them. ===== Last login: Wed Jul 19 12:32:01 2023 from 145.90.39.240 [YourNetID@srv228 ~]$ [YourNetID@srv228 ~]$ hostname # check you are on the bastion server srv228.tudelft.net Single Sign-On with bastion server By default you have to enter your password for every connection (first to the bastion and then to DAIC, for all SSH and SCP/SFTP connections). It’s much more convenient to only have to enter your password once. This is possible with a combination of SSH and Kerberos authentication. SSH multiplexing can be configured to reduce these logins by adding the following to the end of the configuration file:\n$ cat ~/.ssh/config Host * ControlMaster auto ControlPath /tmp/ssh-%r@%h:%p where:\n The ControlPath specifies where to store the “control socket” for the multiplexed connections. In this case, %r refers to the remote login name, %h refers to the target host name, and %p refers to the destination port. Including this information in the control socket name helps SSH separate control sockets for connections to different hosts. The ControlMaster is what activates multiplexing. With the auto setting, SSH will try to use a master connection if one exists, but if one doesn’t exist it will create a new one (this is probably the most flexible approach, but you can refer to ssh-config(5) for more details on the other settings).  Note Windows users may need to adapt the ControlPath location to match Windows.  ssh proxy support To connect directly from your machine to a DAIC login node (without connecting to the bastion server first), create a connection via a proxy by adding the following lines to the configuration file ~/.ssh/config on your local computer:\nHost daic-login Hostname login.daic.tudelft.nl ProxyCommand ssh -W %h:%p bastion User \u003cYourNetID\u003e You can then simply use: ssh daic-login to login. You will be prompted for your password twice: once for the bastion server, and then for DAIC:\n$ ssh hpc-login YourNetID@linux-bastion-ex.tudelft.nl's password: The HPC cluster is restricted to authorized users only. YourNetID@login.hpc.tudelft.nl's password: Last login: Tue Jul 25 02:13:33 2023 from srv228.tudelft.net ######################################################################### # # # Welcome to login1, login server of the HPC cluster. # # # # By using this cluster you agree to the terms and conditions. # # # # For information about using the HPC cluster, see: # # https://login.hpc.tudelft.nl/ # # # # The bulk, group and project shares are available under /tudelft.net/, # # your windows home share is available under /winhome/$USER/. # # # ######################################################################### 02:13:56 up 51 days, 14:27, 1 user, load average: 0,02, 0,10, 0,12 YourNetID@login1:~$ YourNetID@login1:~$ hostname login1.hpc.tudelft.nl ","categories":"","description":"How to connect to DAIC?\n","excerpt":"How to connect to DAIC?\n","ref":"/docs/connecting/","tags":"","title":"Connecting to DAIC"},{"body":"The available processing power and memory in DAIC is large, but still limited. You should use the available resources efficiently and fairly. This page lays out a few general principles and guidelines for considerate use of DAIC.\nUsing shared resources The computing servers within DAIC are primarily meant to run large, long (non-interactive) jobs. You share these resources with other users across departments. Thus, you need to be cautious of your usage so you do not hinder other users.\nTo help protect the active jobs and resources, when a login server becomes overloaded, new logins to this server are automatically disabled. This means that you will sometimes have to wait for other jobs to finish and at other times ICT may have to kill a job to create space for other users.\nOne rule: Respect your fellow users.\nImplication: we reserve the right to terminate any job or process that we feel is clearly interfering with the ability of others to complete work, regardless of technical measures or its resource usage.\n Best practices   Connect only directly from the bastion server to the login servers (See Connecting to DAIC)\n  Always choose the login server with the lowest use (most importantly system load and memory usage), by checking the Current resource usage page   or the servers command for information.\n Each server displays a message at login. Make sure you understand it before proceeding. This message includes the current load of the server, so look at it at every login    Only use the storage best suited to your files (See Filesystem and storage).\n    Do interactive code development, debugging and testing in your local machine, as much as possible. In the cluster, try to organize your code as scripts, instead of working interactively in the command line.\n  If you need to test and debug in the cluster, for example, in a GPU node, request an interactive session and do not work in the login node itself (See Interactive jobs on compute nodes).\n  Save results frequently: your job can crash, the server can become overloaded, or the network shares can become unavailable.\n  Write your code in a modular way, so that you can continue the job from the point where it last crashed.\n   Actively monitor the status of your jobs and the loads of the servers.  Make sure your job runs normally and is not hindering other jobs. Check the following at the start of a job and thereafter at least twice a day:  If your job is not working correctly (or halted) because of a programming error, terminate it immediately; debug and fix the problem instead of just trying again (the result will almost certainly be exactly the same). If your screen’s Kerberos ticket has expired, renew it so your job can successfully save it’s results. Use the top program to monitor the cpu (%CPU) and memory (%MEM) usage of your code. If either is too high, kill your code so it doesn’t cause problems for other users. Don’t leave top running unless your are continuously watching it; press q to quit. Watch the current resource usage (see Current resource usage page   or use the servers command), and if the server is running close to it’s limits (higher than 90% server load or memory, swap or disk usage), consider moving your job to a less busy server.       Computing on login nodes   You can use login nodes for basic tasks like compiling software, preparing submission scripts for the batch queue, submitting and monitoring jobs in the batch queue, analyzing results, and moving data or managing files.\n  Small-scale interactive work may be acceptable on login nodes if your resource requirements are minimal.\n  Note ~~Login nodes have per-user CPU and memory quotas. If you run processes on a login node that push the total usage beyond a certain amount, the limiter will begin killing the largest processes until the total satisfies the limit. ~~  --  Please do not run production research computations on the login nodes. If necessary, request an interactive session in these cases (See Interactive jobs on compute nodes)  Note Most multi-threaded applications (such as Java and Matlab) will automatically use all cpu cores of a server, and thus take away processing power from other jobs. If you can specify the number of threads, set it to at most 25% (¼) of the cores in that server (for a server with 16 cores, use at most 4; this leaves enough processing capacity for other users). Also see How do I request CPUs for a multithreaded program?   Resource limits and quotas Data management: backup and retention Sensitive data ","categories":"","description":"What is acceptable usage of DAIC?\n","excerpt":"What is acceptable usage of DAIC?\n","ref":"/docs/intro_daic/guidelines/","tags":"","title":"Guidelines \u0026 best practices"},{"body":" This is a placeholder page that shows you how to use this template site.\n What is Julia? Multi-threading \u0026 Distributed computing in Julia Message passing Moving to the GPU Scaling up: working in a cluster A few resources on using Julia on HPC:\n Churavy, Valentin, et al. “Bridging HPC Communities through the Julia Programming Language.\" arXiv preprint arXiv:2211.02740 (2022). https://enccs.github.io/julia-for-hpda/ (generic) https://enccs.github.io/julia-for-hpc/ (for HPC really) https://migarstka.github.io/juliahpc/ (a bit oldish)   ","categories":"","description":"How to work with Julia on High Performance Computing environments\n","excerpt":"How to work with Julia on High Performance Computing environments\n","ref":"/tutorials/julia/","tags":"","title":"Julia in Compute clusters"},{"body":"Interactive sessions hang when left for some time without input  This seems to be a bug. For now, use sattach or one of the login nodes. Of course, if a session is really idle (i.e. nothing running), just close it so another job can run.  Job pending with reason QOSGrpCpuLimit  Each QoS (Quality of Service) sets limits on the total number of CPUs in use for that QoS. If the total number of CPUs in use by running jobs (of the whole group of users combined) in a QoS hits the set limit, no new jobs in that QoS can be started, and jobs will stay pending with reason QOSGrpCpuLimit. This is not an error; when running jobs finish, the highest priority pending job will be started automatically. See QoS priority.  Job pending with reason ReqNodeNotAvail  Sometimes nodes are not available to start new jobs (for example when they are reserved for maintenance). When jobs can only run on those nodes, they will remain pending with the reason ReqNodeNotAvail until the node become available again. The requested runtime of a job is an important factor here. When a reservation starts in 1 day, a job with a requested runtime of 7 days will not be able to start (since it would not be finished before the start of the reservation), but a job with a requested runtime of 4 hours can still be run normally. So, when possible, reduce the requested runtime. The requested resources (number of CPUs, amount of memory, number or type of GPUs or specific constraints) limit the number of nodes that are suitable to run a job. So, when possible, reduce the requested resources and constraints, and do not request specific GPU types.  Why does my job run on some nodes but fail on other nodes?  The nodes have different configurations (processor types, number of cores, memory size, GPU support, and so on- see Hardware infrastructure). If you need a specific feature, make sure to request it in your sbatch script. Examples: #SBATCH --constraint=avx #SBATCH --constraint=avx2 #SBATCH --gres=gpu  If your program uses specific processor instruction extensions (AVX/SSE/AES/…), it will crash (with an Illegal instruction error) on processors that do not support those extensions.  Either compile your program to use only standard instructions (be generic), or request nodes that have support. The login node login3 has the least advanced CPUs so if you compile your programs there they should run on all other nodes.    Why does my job fail and is there no slurm-XXXXX.out output (or error)?  When your job doesn’t have a valid Kerberos ticket (see Kerberos authentication) it can’t read or write files (such as the slurm-XXXXX.out output). It’s best to do a fresh login to a login node when you want to submit a new job. This way you’re sure your job’s Kerberos ticket is valid and will remain valid for the next 7 days. If needed, you can update the Kerberos ticket for a running job by executing auks -a (also from a fresh login).  sbatch: error: Batch job submission failed: Access/permission denied  You can only submit jobs (using sbatch) from the login nodes (login1, login2, login3). When you try to submit a job from within another job (including an interactive job) you will receive this error. To submit multiple jobs from a script, create a file with the submit commands: sbatch job1.sh sbatch job2.sh sbatch job3.sh … Then login to one of the login nodes and source the file: source script   sbatch: error: Batch job submission failed: Invalid account or account/partition combination specified Either:\n You are trying to use a (special) partition that you don’t have access to, Or Your account has been (temporarily) disabled because of problems with your jobs. The usual problems are (a combination of) of these:  Your jobs are not using all of the requested resources (CPUs, GPUs, memory, runtime). Your jobs try to use more resources than requested (eg, more active threads than the requested number of CPUs, out of memory failures, timeout failures … etc). Too many jobs are failing or being cancelled. Generally, failing to follow the Cluster workflow.    You need to figure out the problem(s), fix them, and then send an email to the cluster administrators to explain the problem(s) and the way you fixed them.\nsrun: error: Unable to allocate resources: Invalid qos specification  You can’t directly execute a jobscript, you need to submit the jobscript using sbatch: sbatch jobscript.sh   slurmstepd: error: Exceeded step memory limit at some point.  Your program wants to use more memory than you requested. You’ll either need to limit the memory use of your program or request more memory. Also see How much memory can I use? and How can I see the CPU or RAM usage of a job?.  Auks API request failed : auks cred : credential lifetime is too short / Auks API request failed : krb5 cred : unable to renew credential  Your authentication (Kerberos ticket) expired (see Kerberos authentication). You get a Kerberos ticket (using your password) when you log in to the bastion server or a login node. Jobs that run on a compute node also require authentication. Therefore, when you submit a job, your Kerberos ticket is stored in the Auks system so that your job can use it. However, the maximum lifetime of a Kerberos ticket is 7 days. So 7 days after you last logged in and submitted a job, the Kerberos ticket in the system expires and the job fails. Therefore, for infinite jobs or long jobs that have had to wait in the queue for a couple of days, the Kerberos ticket needs to be renewed before it expires. The simple way to do this is to log in to a cluster login node and run auks -a. When you frequently need to do this, you can (on a cluster login node) run install_keytab to install an encrypted version of your password (called Kerberos keytab) for automatic Kerberos ticket renewal.  Important When you change your NetID password, your Kerberos keytab becomes invalid, so you will need to rerun the install_keytab command.  What can be done about some jobs using all CPUs or memory (making it impossible for me to use other unused resources like GPUs)?  Resources can be used by one job at a time only, so when some resources are completely used, other jobs will have to wait until their required resources become available. The waiting is unavoidable. See the answer to Should I feel guilty about running a lot of jobs with GPU/CPU usage? (The scheduler is already dividing the available resources fairly over all jobs based on the policies, using priorities based on previous usage. As soon as a running job finishes and it’s resources become available again, the highest priority waiting job is automatically started.) The policies for the scheduler are set by the cluster users, in the cluster board meeting (see General cluster usage). To change the policies, all users must agree that the changes are necessary and fair to all users. (So the cluster administrators aren’t able to change the policies on request!) You can always contact a user (nicely!😉) about the possibility to (for example) exclude a certain node. That user can decide for him-/herself if he/she wants to cooperate or not (for example in case of deadlines). It’s usually possible to determine a person’s initial (or first name) and last name from his/her username; if not, run: finger \u003cusername\u003e If you experience a real problem because of this (not being able to efficiently develop code because of the waiting, or not going to make an upcoming deadline), you should contact cluster support to see if they can provide support for that specific problem. It’s not desirable to reserve resources for certain types of jobs only. Since other types of jobs wouldn’t be able to use those resources even when they would be idle, this would reduce the overall cluster throughput (and recreate the exact same problem that you experience for those other jobs). No type of research can make special claims regarding resources. All research that uses the cluster is equally dependent on the cluster resources: 50+ CPU jobs or jobs requiring 50GB memory can’t be run on a laptop any more than a GPU job requiring 5GB of GPU memory. When one type of resource is completely used, that is because it is required to do the same kind of research that you need the cluster for.  ","categories":"","description":"","excerpt":"Interactive sessions hang when left for some time without input  This …","ref":"/support/faqs/scheduler/","tags":"","title":"Scheduler questions"},{"body":"This user agreement is intended to establish the expectations between all users and administrators of the cluster with respect to fair-use and fair-share of cluster resources. By using the DAIC cluster you agree to these terms and conditions.\nGeneral information about the DAIC cluster   Cluster structure: The DAIC cluster is made up of shared resources contributed by different labs and groups. The pooling of resources from different groups is beneficial for everyone: it enables larger, parallelized computations and more efficient use of resources with less idle time.\n  Basic principles: Regardless of the specific details, cluster use is always based on basic principles of fair-use and fair-share (through priority) of resources, and all users are expected to take care at all times that their cluster use is not hindering other users.\n  Policies: Cluster policies are decided by the user board and enforced by various automated and non-automated actions, for example by the job scheduler based on QoS limits and the administrators for ensuring the stability and performance of the cluster.\n  Support:\n Cluster administrators offer, during office hours, different levels of support, which include (in order of priority): ensuring the stability and performance of the cluster, providing generic software, helping with cluster-specific questions and problems, and providing information (via e-mails and during the board meeting) about cluster updates. Contact persons from participating groups add and manage users at the level of their respective groups, communicate needs and updates between their groups and system administrators, and may help with cluster-specific questions and problems. HPC Engineers, in CS@Delft, provide support to (CS) students, researchers and staff members to efficiently use DAIC resources. This includes: maintaining updated documentation resources, running onboarding and advanced training courses on cluster usage, organizing workshops to assess compute needs, plan infrastructure upgrades, and may collaborate with researchers on individual projects as fits.    More information: Please see the General cluster usage and What to do in case of problems sections on where to find more information about cluster use.\n  Cluster workflow:\n The typical steps for running a job on the cluster are: Test → Determine resources → Submit → Monitor job → Repeat until results are obtained. See Quick start You can use the logins nodes for testing your code, determining the required resources and submitting jobs. See Computing on login nodes For testing jobs which require larger resources (more than 4 CPUs and/or more than 4 GB of memory and/or one or more powerful GPUs), start an interactive job. See Interactive jobs on compute nodes. For determining resources of larger jobs, you can submit a single (short) test job. See Job submission    QoS:\n A Quality of Service (QoS) is a set of limits that controls what resources a job can use and determines the priority level of a job. DAIC adopts multiple QoSs to optimize the throughput of job scheduling and to reduce the waiting times in the cluster (see QoS priority). The DAIC QoS limits are set by the DAIC user board, and the scheduler strictly enforces these limits. Thus, no user can use more resources than the amount that was set by the user board. Any (perceived) imbalance in the use of resources by a certain QoS or user should not be hold against a user or the scheduler, but should be discussed in the user board.    General cluster usage   You may use cluster resources for your research within the QoS restrictions of your domain user and user group.\n Depending on your user group, you might be eligible to use specific partitions, giving higher priorities on certain nodes. See Priority tiers, and please check this with your lab.    Depending on your user group, you might be eligible to get priorities on certain nodes. For example, you might have access to a specialized partition or limited-time node reservation for your group or department (for example before a conference deadline). Please check this with your lab and try to use these in your *.sbatch file, your jobs should then start faster! See Resources Reservations for more information.\n  In general, you will be informed about standard administrative actions on the cluster. All official DAIC cluster e-mails are sent to your official TU Delft mailbox, so it is advised to check it regularly.\n You will receive e-mails about downtimes relating to scheduled maintenance. You, or your supervisor, will receive e-mails about scheduled cluster user board meetings where any updates and changes to the cluster structure, software, or hardware will be announced. Please check with your lab or feel free to join the cluster board meetings if you want to be up-to-date about any changes. You will receive automated e-mails regarding the efficiency of your jobs. The cluster monitors the use of resources of all jobs. When certain specific inefficiencies are detected for a significant number of jobs in the same day, an automated efficiency mail is sent to inform you about these problems with your resource use, to help you optimize your jobs. These mails will not lead to automatic cancellations or bans. To avoid spamming, limited inefficient use will not trigger a mail. You will receive an e-mail when your jobs are canceled or you receive a cluster ban (see the Expectations from cluster users and Regulations sections). You will be informed about why your jobs were canceled or why you were banned from the cluster (often before the bans take place). If the problem is still not clear to you from the e-mails you already received, please follow the steps detailed in the What to do in case of problems section. You are not entitled to receive personalized help on how to debug your code via e-mail. It is your responsibility to solve technical problems stemming from your code. Please first consult with your lab for a solution to a technical problem (see What to do in case of problems). However, admins might offer help, advice and solutions along with information regarding a job cancellation or ban. Please listen to such advice, it might help you solve your problem and improve fair use of the cluster.    You may join cluster user board meetings. In the meetings you will be informed of any new developments, hardware and software updates and can suggest changes and improvements. These meetings take place roughly every 3 months and will be announced by e-mail and on the MatterMost channel.\n  Expectations from cluster users   You are responsible for your jobs not interfering with other users' cluster usage. Please try to always keep in mind that cluster resources are limited and shared between all users, and that fair use benefits everyone.\n  You are not allowed to use the cluster for reasons unrelated to your studies and research.\n  If your jobs are destructive to other users' jobs or are threatening cluster integrity, your jobs might be canceled. You have the responsibility at all times to avoid behavior which interferes negatively with other users' cluster usage. See DAIC Regulations\n  If the destructive behavior of your jobs does not change over time or you are unresponsive to e-mails from system admins requesting information or requiring immediate action regarding your cluster use, you might receive a ban from the cluster. See DAIC Regulations\n  Regulations  Your jobs might be canceled if:  The node your jobs are running on becomes unresponsive and the node is automatically restarted. The job is overloading the node (for example overloading the network communication of the node). The job is adversely affecting the execution of other jobs (jobs that are not using all requested resources (effectively) and thus unfairly block waiting jobs from running may also be canceled). The jobs ignore the directions from the administrators (for example if a job is (still) affected by the same problem that the administrators informed you about before, and asked you to fix and test before resubmitting). The job is showing clear signs of a problem (like hanging, or being idle, or using only 1 CPU of the multiple CPUs requested, or not using a GPU that was requested).   You might receive a cluster access ban for:  Disallowed use of the cluster, including disallowed use of computing time, purposefully ignoring directions, guidelines, fair-use principles and/or (trying to gain) unauthorized access and/or causing disruptions to the cluster or parts thereof (even if unintentional). Unresponsiveness to e-mails from system admins requesting information or requiring immediate action regarding your cluster use. Repeated problems caused by your cluster use which go unsolved even after attempts to resolve the issue. Your cluster use privileges will be returned when all parties are confident that you understand the problem and it won’t reoccur.   Your jobs won’t be canceled for:  Scheduled maintenance. This is planned in advance and jobs that would run during scheduled maintenance times won’t start until the end of maintenance.    What to do in case of problems When you encounter problems, please follow the subsequent steps, in the indicated order:\n First, please contact your colleagues and fellow cluster users in your lab, concerning problems with your code, job performance and efficiency. They may be running similar jobs and potentially have solutions for your problem. You can also ask questions to fellow users on the MatterMost channel. For prolonged problems, your initial contact point is your supervisor/PI. As a final step, you can contact the cluster administrators for technical sysadmin problems or persistent efficiency problems, or for more information if you are not sure why you are banned from the cluster. You can do this by reporting your question, through the Self Service Portal   , to the Service Desk. In your question, refer to the ‘DAIC cluster’. For severe recurring problems, complaints and suggestions for policy changes, or issues affecting multiple users, you can contact the DAIC advisory board to bring it up as an agenda point in the next user board meeting.  Responsible cluster usage  You are responsible that your jobs run efficiently.  Please keep an eye on your jobs and the automated efficiency e-mails to check for unexpected behavior. Sometimes many jobs from the same user, or from student groups, will be running on many nodes at the same time. While this may seem like one user, or user group, is blocking the cluster for everyone else, please keep in mind that the scheduler operates on a set of predetermined rules based on the QoS and priority settings. We do not want idle resources. Therefore, at the time that those jobs were started, the resources were idle, no higher priority jobs were in the queue and the jobs did not exceed the QoS limits. If you repeatedly observe pending jobs, please bring it up in the user board meeting. Short job efficiency: If you are running many (hundreds or thousands of) very short jobs (duration of a few minutes), you may want to consider that starting and individually loading the same modules for each job may create overheads. When reasonably possible, it might save computation time to instead group some jobs together. The jobs can still be submitted to the short queue if the runtime is less than 4 hours. GPU job efficiency: If you are running multi-GPU jobs (for example due to GPU memory limitations), you may want to consider that the communications between the GPUs and other CPU processes (for example data loaders) may create overheads. It might be useful to consider running jobs on less GPUs with more GPU memory each, or taking advantage of specialized libraries optimized for multi-GPU computing in your code.   If you published any work benefiting from DAIC resources, then:  Please post its details to the ScientificOutput MatterMost channel   . Please acknowledge DAIC in your scientific outputs, using this sentence:   Research reported in this work was partially or completely facilitated by computational resources and support of the the Delft AI Cluster (DAIC) at TU Delft (URL: https://doc.daic.tudelft.nl/), but remains the sole responsibility of the authors, not the DAIC team.\n   Feedback and Suggestions  If you have suggestions for policy changes (changes to QoS, scheduler priorities, and similar), please join the cluster user board meetings as stated in General cluster usage and follow the steps of What to do in case of problems.  ","categories":"","description":"What are the terms of conditions of using DAIC?\n","excerpt":"What are the terms of conditions of using DAIC?\n","ref":"/docs/intro_daic/user_agreement/","tags":"","title":"User agreement"},{"body":" DAIC servers have direct access to the TU Delft home, group and bulk storage. You can use your TU Delft installed machine or an SCP or SFTP client to transfer files to and from these storage areas and others (see Data transfer methods) , as is demonstrated throughout this page.\n File System Overview Unlike TU Delft’s DelftBlue   , DAIC does not have a dedicated storage filesystem. This means no /scratch space for storing temporary files (see DelftBlue’s Storage description   and Disk quota and scratch space   ). Instead, DAIC relies on direct connection to the TU Delft network storage filesystem (see Overview data storage   ) from all its nodes, and offers the following types of storage areas:\nPersonal storage (aka home folder) The Personal Storage is private and is meant to store personal files (program settings, bookmarks). A backup service protects your home files from both hardware failures and user error (you can restore previous versions of files from up to two weeks ago). The available space is limited by a quota limit (since this space is not meant to be used for research data).\nYou have two (separate) home folders: one for Linux and one for Windows (because Linux and Windows store program settings differently). You can access these home folders from a machine (running Linux or Windows OS) using a command line interface or a browser via TU Delft's webdata   . For example, Windows home has a My Documents folder. My documents can be found on a Linux machine under /winhome/\u003cYourNetID\u003e/My Documents\n   Home directory Access from Storage location     Linux  home folder    Linux    /home/nfs/\u003cYourNetID\u003e      Windows  only accessible using an scp/sftp client (see SSH access)    webdata not available    Windows home folder    Linux  /winhome/\u003cYourNetID\u003e    Windows    H:  or  \\\\tudelft.net\\staff-homes\\[a-z]\\\u003cYourNetID\u003e      webdata   https://webdata.tudelft.nl/staff-homes/[a-z]/\u003cYourNetID\u003e      It’s possible to access the backups yourself. In Linux the backups are located under the (hidden, read-only) ~/.snapshot/ folder. In Windows you can right-click the H: drive and choose Restore previous versions.\nNote To see your disk usage, run something like:\ndu -h '\u003c/path/to/folder\u003e' | sort -h | tail   Group storage The Group Storage is meant to share files (documents, educational and research data) with department/group members. The whole department or group has access to this storage, so this is not for confidential or project data. There is a backup service to protect the files, with previous versions up to two weeks ago. There is a Fair-Use policy for the used space.\n   Destination Access from Storage location     Group Storage    Linux    /tudelft.net/staff-groups/\u003cfaculty\u003e/\u003cdepartment\u003e/\u003cgroup\u003e  or     /tudelft.net/staff-bulk/\u003cfaculty\u003e/\u003cdepartment\u003e/\u003cgroup\u003e/\u003cNetID\u003e      Windows    M:  or  \\\\tudelft.net\\staff-groups\\\u003cfaculty\u003e\\\u003cdepartment\u003e\\\u003cgroup\u003e  or     L:  or  \\\\tudelft.net\\staff-bulk\\ewi\\insy\\\u003cgroup\u003e\\\u003cNetID\u003e      webdata   https://webdata.tudelft.nl/staff-groups/\u003cfaculty\u003e/\u003cdepartment\u003e/\u003cgroup\u003e/      Project Storage The Project Storage is meant for storing (research) data (datasets, generated results, download files and programs, …) for projects. Only the project members (including external persons) can access the data, so this is suitable for confidential data (but you may want to use encryption for highly sensitive confidential data). There is a backup service and a Fair-Use policy for the used space.\nProject leaders (or supervisors) can request a Project Storage location via the Self-Service Portal or the Service Desk   .\n   Destination Access from Storage location     Project Storage    Linux    /tudelft.net/staff-umbrella/\u003cproject\u003e      Windows    U:  or  \\\\tudelft.net\\staff-umbrella\\\u003cproject\u003e      webdata    https://webdata.tudelft.nl/staff-umbrella/\u003cproject\u003e  or  https://webdata.tudelft.nl/staff-bulk/\u003cfaculty\u003e/\u003cdepartment\u003e/\u003cgroup\u003e/\u003cNetID\u003e      Local Storage Local storage is meant for temporary storage of (large amounts of) data with fast access on a single computer. You can create your own personal folder inside the local storage. Unlike the network storage above, local storage is only accessible on that computer, not on other computers or through network file servers or webdata. There is no backup service nor quota. The available space is large but fixed, so leave enough space for other users. Files under /tmp that have not been accessed for 10 days are automatically removed.\n   Destination Access from Storage location     Local storage    Linux   /tmp/\u003cNetID\u003e      Windows  not available     webdata  not available     Memory Storage Memory storage is meant for short-term storage of limited amounts of data with very fast access on a single computer. You can create your own personal folder inside the memory storage location. Memory storage is only accessible on that computer, and there is no backup service nor quota. The available space is limited and shared with programs, so leave enough space (the computer will likely crash when you don’t!). Files that have not been accessed for 1 day are automatically removed.\n   Destination Access from Storage location     Memory storage    Linux    /dev/shm/\u003cNetID\u003e      Windows  not available      webdata  not available     Warning Use this only when using other storage makes your job or the whole computer slow.  File Management Guidelines There are different use cases and quota limits for the different TU Delft network drives. For example, Umbrella (project storage), is for everybody and everything, while bulk needs to be cleaned up, migrated and phased out. Always check TU Delft Overview data storage   for guidelines on using network drives and quota limits.\nData Transfer Methods Between TU Delft installed machines Your Windows Personal Storage and the Project and Group Storage are available on all TU Delft installed machines including the DAIC compute servers. If possible use one of these for files that you want to access on both your personal computer and the compute servers.\nUsing webdata Your Windows Personal Storage and the Project and Group Storage are also accessible off-campus through the TU Delft webdata service. See the webdata page   for manuals on using the service with your personal computer.\nUsing SCP/SFTP Both your Linux and Windows Personal Storage and the Project and Group Storage are also available world-wide via an SCP/SFTP client. This is the simplest transfer method via the scp command, which has the following basic syntax:\n$ scp -p \u003csource_file\u003e \u003ctarget_destination\u003e # for files $ scp -p -r \u003csource_folder\u003e \u003ctarget_destination\u003e # for folders For example, to transfer a file from your computer to DAIC:\n$ scp -p mylocalfile [\u003cnetid\u003e@]login.daic.tudelft.nl:~/destination_path_on_DAIC/ To transfer a folder (recursively) from your computer to DAIC:\n$ scp -pr mylocalfolder [\u003cnetid\u003e@]login.daic.tudelft.nl:~/destination_path_on_DAIC/ To transfer a file from DAIC to your computer:\n$ scp -p [\u003cnetid\u003e@]login.daic.tudelft.nl:~/origin_path_on_DAIC/remotefile ./ To transfer a folder from DAIC to your computer:\n$ scp -pr [\u003cnetid\u003e@]login.daic.tudelft.nl:~/origin_path_on_DAIC/remotefolder ./ The above commands will work from either the university network, or when using EduVPN. If a “jump” via linux-bastion is needed (see Access from outside university network), modify the above commands by replacing scp with scp -J \u003cnetid\u003e@linux-bastion.tudelft.nl and keep the rest of the command as before:\n$ scp -p \u003clocal_file\u003e [\u003cnetid\u003e@]linux-bastion.tudelft.nl:\u003cremote_destination\u003e $ scp -p -r \u003clocal_folder\u003e [\u003cnetid\u003e@]linux-bastion.tudelft.nl:\u003cremote_destination\u003e $ scp -p [\u003cnetid\u003e@]linux-bastion.tudelft.nl:\u003cremote_file\u003e \u003clocal_destination\u003e $ scp -p -r [\u003cnetid\u003e@]linux-bastion.tudelft.nl:\u003cremote_folder\u003e \u003clocal_destination\u003e $ sftp [\u003cnetid\u003e@]linux-bastion.tudelft.nl Where:\n Case is important. Items between \u003c \u003e brackets are user-supplied values (so replace with your own NetID, file or folder name). Items between brackets are optional: when your username on your local computer is the same as your NetID username, you don’t have to specify it. When you specify your NetID username, don’t forget the @ character between the username and the computer name.  Note for students Please use student-linux.tudelft.nl instead of linux-bastion.tudelft.nl as an intermediate server!  Hint Use quotes when file or folder names contain spaces or special characters.  SCP alternatives More powerful alternatives to scp exist, like rsync for synchronizing source and destination, and sshfs for mounting folders to your linux computer. See DelftBlue file transfer   documentation for use of these tools, and use the proper DAIC addresses instead.\nTo or from local storage For files that you want to transfer to or from local storage, first transfer them to Project or Scratch Storage. Then in your job script, copy the files to the local storage, run your work, and afterwards delete your files from local storage.\n  ","categories":"","description":"How/Where to store data\n","excerpt":"How/Where to store data\n","ref":"/docs/filesystem/","tags":"","title":"File systems and storage"},{"body":"A few solutions exist, eg, compressing, singularity image, …etc\n","categories":"","description":"How to work around a large number of small files?\n","excerpt":"How to work around a large number of small files?\n","ref":"/tutorials/large_small_files/","tags":"","title":"Too many small files"},{"body":"Batch Queuing System Overview DAIC uses Slurm   as a cluster management and job scheduling system to efficiently manage computational workloads across computing capacity.\nA slurm-based cluster is composed of a set of login nodes that are used to access the cluster and submit computational jobs. A central manager orchestrates computational demands across a set of compute nodes. These nodes are organized logically into groups called partitions, that defines job limits or access rights. The central manager provides fault-tolerant hierarchical communications, to ensure optimal and fair use of available compute resources to eligible users, and make it easier to run and schedule complex jobs across compute resources (multiple nodes).\n  DAIC partitions and access/usage best practices\n  Kerberos Authentication Kerberos is an authentication protocol which uses tickets to authenticate users (and computers). You automatically get a ticket when you log in with your password on a TU Delft installed computer. You can use this ticket to authenticate yourself without password when connecting to other computers or accessing your files. To protect you from misuse, the ticket expires after 10 hours or less (even when you’re still logged in).\nFile access Your Linux and Windows home  directories and the Group and Bulk shares are located on network fileservers, which allows you to access your files from all TU Delft installed computers. Kerberos authentication is used to enable access to, or protect, your files. Without a valid Kerberos ticket (e.g. when the ticket has expired) you will not be able to access your files but instead you will receive a Permission denied error.\nLifetime of Kerberos Tickets Kerberos tickets have a limited valid lifetime (of up to 10 hours) to reduce the risk of abuse, even when you stay logged in. If your tickets expire, you will receive a Permission Denied error when you try to access your files and a password prompt when you try to connect to another computer. When you want your program to be able to access your files for longer than the valid ticket lifetime, you’ll have to renew your ticket (repeatedly) until your program is done. Kerberos tickets can be renewed up to a maximum renewable life period of 7 days (again to reduce the risk of abuse).\nThe command klist -5 lists your cached Kerberos tickets together with their expiration time and maximum renewal time:\n$ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_random Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/01/01 00:00:00 01/01/01 10:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/08/01 00:00:00 Where:\n  Ticket cache: The Kerberos tickets that have been issued to you are stored in a ticket cache file. You can have multiple ticket cache files on the same computer (from different connections, for example) with different tickets and ticket expiration times. Some ticket cache files are automatically removed when you logout. Tip Make sure that you renew the tickets in the right ticket cache file (see this screen example).    Default principal: Your identity.\n  Service principal: The identity of services that you have gotten tickets for. You always need a Kerberos ticket-granting ticket (krbtgt) in order to obtain other tickets for specific services like accessing files (nfs) or connecting to computers (host).\n  Valid starting, Expires: Your ticket is only valid between these times (this period is called the valid lifetime). After this time you will not be able to use the service nor automatically renew the ticket (without password).\n  Renew until: Your ticket can only be renewed without password up to this time. After this time you will have to obtain a new ticket using your password.\n  Renewing Kerberos tickets If you have a valid Kerberos krbtgt ticket, you can renew it at any time (until it expires) by running the command kinit -R:\n$ kinit -R $ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_random Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/01/01 01:00:00 01/01/01 11:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/08/01 00:00:00  Note Renewing the ticket will not change the duration of the valid lifetime, i.e. a krbtgt ticket with a valid lifetime of 1 hour will, after renewal, be valid for another hour.  When the krbtgt ticket has expired or reached it’s renew until time, you will have to obtain a new ticket by running kinit -r 7d (note the difference in case for the r) and authenticating with your password:\n$ kinit -r 7d Password for YourNetID@TUDELFT.NET: $ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_random Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/01/01 11:00:00 01/01/01 21:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/08/01 11:00:00 The new ticket will have a valid lifetime of 10 hours and a renewable life of 7 days.\nOn the TU Delft Linux desktops your Kerberos ticket is refreshed (i.e. replaced by a new ticket) automatically every time you enter your password for unlocking the screen saver.\nTip Do not disable the screen saver password lock.  On remote computers you have to manually renew your tickets before they expire.\nSlurm \u0026 Kerberos  Slurm caches your Kerberos ticket, and uses it to execute your job Regularly renew the ticket in Slurm’s cache while your jobs are queued or running:  $ auks -a Auks API request succeed  To automatically renew your ticket in Slurm’s cache until you change your NetID password, run the following on the login1 server:  $ install_keytab Password for somebody@TUDELFT.NET: Installed keytab. You need to rerun this command whenever you change your NetID password (at least every 6 months). Otherwise, the automatic renewal will not work and you will receive a warning e-mail.\nRenewal using screen On the compute servers, the screen program has been modified to allow jobs to run unattended for up to 7 days. It creates a private ticket cache (to prevent the cache from being destroyed at logout) and automatically renews your ticket up to the maximum renewable life. For example, start MATLAB in Screen with screen matlab (the order is important!).\n$ screen matlab Warning: No display specified. You will not be able to display graphics on the screen. \u003c M A T L A B (R) \u003e Copyright 1984-2010 The MathWorks, Inc. Version 7.11.0.584 (R2010b) 64-bit (glnxa64) August 16, 2010 To get started, type one of these: helpwin, helpdesk, or demo. For product information, visit www.mathworks.com. \u003e\u003e For longer jobs you have to manually obtain a new ticket at least every 7 days by running kinit -r 7d from within screen (so you use the specific ticket cache file that screen is using):\n connect to screen (screen -r), create a new window (Ctrl-a c), run kinit -r 7d, exit the window (exit) and detach from screen (Ctrl-a d).  $ kinit -r 7d Password for YourNetID@TUDELFT.NET: $ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_private Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/08/01 09:00:00 01/08/01 19:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/15/01 09:00:00 $ exit  Tip Use a repeating reminder (twice a week) in your agenda so you don’t forget.  Important When the end of the renewable life is reached, your tickets expire and your program(s) will return Permission denied errors when trying to access your files. Your program(s) will not be terminated automatically; you still have to terminate the program(s) yourself.  Extra functionality can be provided by the k5start and krenew programs. On most computers these are not available by default but can be installed (ask Robbert).\n","categories":"","description":"How to submit jobs to slurm?\n","excerpt":"How to submit jobs to slurm?\n","ref":"/docs/job_submissions/_index-1/","tags":"","title":"Job submission and management"},{"body":"Partitions and Quality of Service When you submit a job in a slurm-based system, it enters a queue waiting for resources. The partition and Quality of Service(QoS) are the two job parameters slurm uses to assign resources for a job:\n The partition is a set of compute nodes on which a job can be scheduled. In DAIC, the nodes contributed or funded by a certain group are lumped into a corresponding partition (see Brief history of DAIC). All nodes in DAIC are part of the general partition, but other partitions exist for prioritization purposes on select nodes (see Priority tiers). The Quality of Service is a set of limits that controls what resources a job can use and, therefore, determines the priority level of a job. This includes the run time, CPU, GPU and memory limits on the given partition. Jobs that exceed these limits are automatically terminated (see QoS priority).  For DAIC, Table 1 shows the QoS limits on the general partition.\n  Table 1: The general partition and its operational and per-QoS per-user limits; specific groups use other partitions and QoS  *infinite QoS jobs will be killed when servers go down, eg, during maintenance. It is not recommended to submit jobs with this QoS.    Partition QoS Priority Max run time Jobs per user CPU limits GPU limits Memory limits   Per QoS Per user Per QoS Per user Per QoS Per User     general interactive high 1 hour 1 running - 2 - 2 - 16G   short normal 4 hours 10000 3672 (85%) 2160 (50%) 109 (85%) 64 (50%) 23159G (85%) 13623G (50%)   medium medium 1 ½ day 2000 3456 (80%) 1512 (35%) 103 (80%) 45 (35%) 21796G (80%) 9536G (35%)   long low 7 days 1000 3240 (75%) 864 (20%) 96 (75%) 25 (20%) 20434G (75%) 5449G (20%)   infinite* none infinite 1 running 32 - 2 - 250G -     Note The priority of a job is a function of both QoS and previous usage (less is better). See Job prioritization and waiting times   Kerberos Authentication Kerberos is an authentication protocol which uses tickets to authenticate users (and computers). You automatically get a ticket when you log in with your password on a TU Delft installed computer. You can use this ticket to authenticate yourself without password when connecting to other computers or accessing your files. To protect you from misuse, the ticket expires after 10 hours or less (even when you’re still logged in).\nFile access Your Linux and Windows home  directories and the Group and Bulk shares are located on network fileservers, which allows you to access your files from all TU Delft installed computers. Kerberos authentication is used to enable access to, or protect, your files. Without a valid Kerberos ticket (e.g. when the ticket has expired) you will not be able to access your files but instead you will receive a Permission denied error.\nLifetime of Kerberos Tickets Kerberos tickets have a limited valid lifetime (of up to 10 hours) to reduce the risk of abuse, even when you stay logged in. If your tickets expire, you will receive a Permission Denied error when you try to access your files and a password prompt when you try to connect to another computer. When you want your program to be able to access your files for longer than the valid ticket lifetime, you’ll have to renew your ticket (repeatedly) until your program is done. Kerberos tickets can be renewed up to a maximum renewable life period of 7 days (again to reduce the risk of abuse).\nThe command klist -5 lists your cached Kerberos tickets together with their expiration time and maximum renewal time:\n$ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_random Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/01/01 00:00:00 01/01/01 10:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/08/01 00:00:00 Where:\n  Ticket cache: The Kerberos tickets that have been issued to you are stored in a ticket cache file. You can have multiple ticket cache files on the same computer (from different connections, for example) with different tickets and ticket expiration times. Some ticket cache files are automatically removed when you logout. Tip Make sure that you renew the tickets in the right ticket cache file (see this screen example).    Default principal: Your identity.\n  Service principal: The identity of services that you have gotten tickets for. You always need a Kerberos ticket-granting ticket (krbtgt) in order to obtain other tickets for specific services like accessing files (nfs) or connecting to computers (host).\n  Valid starting, Expires: Your ticket is only valid between these times (this period is called the valid lifetime). After this time you will not be able to use the service nor automatically renew the ticket (without password).\n  Renew until: Your ticket can only be renewed without password up to this time. After this time you will have to obtain a new ticket using your password.\n  Renewing Kerberos tickets If you have a valid Kerberos krbtgt ticket, you can renew it at any time (until it expires) by running the command kinit -R:\n$ kinit -R $ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_random Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/01/01 01:00:00 01/01/01 11:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/08/01 00:00:00  Note Renewing the ticket will not change the duration of the valid lifetime, i.e. a krbtgt ticket with a valid lifetime of 1 hour will, after renewal, be valid for another hour.  When the krbtgt ticket has expired or reached it’s renew until time, you will have to obtain a new ticket by running kinit -r 7d (note the difference in case for the r) and authenticating with your password:\n$ kinit -r 7d Password for YourNetID@TUDELFT.NET: $ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_random Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/01/01 11:00:00 01/01/01 21:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/08/01 11:00:00 The new ticket will have a valid lifetime of 10 hours and a renewable life of 7 days.\nOn the TU Delft Linux desktops your Kerberos ticket is refreshed (i.e. replaced by a new ticket) automatically every time you enter your password for unlocking the screen saver.\nTip Do not disable the screen saver password lock.  On remote computers you have to manually renew your tickets before they expire.\nSlurm \u0026 Kerberos  Slurm caches your Kerberos ticket, and uses it to execute your job Regularly renew the ticket in Slurm’s cache while your jobs are queued or running:  $ auks -a Auks API request succeed  To automatically renew your ticket in Slurm’s cache until you change your NetID password, run the following on the login1 server:  $ install_keytab Password for somebody@TUDELFT.NET: Installed keytab. You need to rerun this command whenever you change your NetID password (at least every 6 months). Otherwise, the automatic renewal will not work and you will receive a warning e-mail.\nRenewal using screen On the compute servers, the screen program has been modified to allow jobs to run unattended for up to 7 days. It creates a private ticket cache (to prevent the cache from being destroyed at logout) and automatically renews your ticket up to the maximum renewable life. For example, start MATLAB in Screen with screen matlab (the order is important!).\n$ screen matlab Warning: No display specified. You will not be able to display graphics on the screen. \u003c M A T L A B (R) \u003e Copyright 1984-2010 The MathWorks, Inc. Version 7.11.0.584 (R2010b) 64-bit (glnxa64) August 16, 2010 To get started, type one of these: helpwin, helpdesk, or demo. For product information, visit www.mathworks.com. \u003e\u003e For longer jobs you have to manually obtain a new ticket at least every 7 days by running kinit -r 7d from within screen (so you use the specific ticket cache file that screen is using):\n connect to screen (screen -r), create a new window (Ctrl-a c), run kinit -r 7d, exit the window (exit) and detach from screen (Ctrl-a d).  $ kinit -r 7d Password for YourNetID@TUDELFT.NET: $ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_private Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/08/01 09:00:00 01/08/01 19:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/15/01 09:00:00 $ exit  Tip Use a repeating reminder (twice a week) in your agenda so you don’t forget.  Important When the end of the renewable life is reached, your tickets expire and your program(s) will return Permission denied errors when trying to access your files. Your program(s) will not be terminated automatically; you still have to terminate the program(s) yourself.  Extra functionality can be provided by the k5start and krenew programs. On most computers these are not available by default but can be installed (ask Robbert).\n","categories":"","description":"How to submit jobs to slurm?\n","excerpt":"How to submit jobs to slurm?\n","ref":"/docs/job_submissions/_index-2/","tags":"","title":"Job submission and management"},{"body":"Slurm job’s terminology: job, job step, task and CPUs A slurm job (submitted via sbatch) can consists of multiple steps in series. Each step (specified via srun) can run multiple tasks (ie programs) in parallel. Each task gets its own set of CPUs. As an example, consider the workflow and corresponding breakdown shown in fig 2.\n  Slurm job’s terminology\n  In this example, note:\n When you explicitly request 1 CPU per task (--cpus-per-task=1), you should also explicitly specify the number of tasks (--ntasks). Otherwise, srun may start the task twice in parallel (because CPUs are allocated in multiples of 2) The default slurm allocation is a single task and single CPU (ie --ntasks=1 --cpus-per-task=1). Thus, it is not necessary to explicitly request these to run a single task on a single CPU. When using multiple tasks, specify --mem-per-cpu.  Note DAIC is dual-threaded. It means that CPUs are automatically allocated in multiples of 2. Thus, in your job use (a multiple of) 2 threads.  Kerberos Authentication Kerberos is an authentication protocol which uses tickets to authenticate users (and computers). You automatically get a ticket when you log in with your password on a TU Delft installed computer. You can use this ticket to authenticate yourself without password when connecting to other computers or accessing your files. To protect you from misuse, the ticket expires after 10 hours or less (even when you’re still logged in).\nFile access Your Linux and Windows home  directories and the Group and Bulk shares are located on network fileservers, which allows you to access your files from all TU Delft installed computers. Kerberos authentication is used to enable access to, or protect, your files. Without a valid Kerberos ticket (e.g. when the ticket has expired) you will not be able to access your files but instead you will receive a Permission denied error.\nLifetime of Kerberos Tickets Kerberos tickets have a limited valid lifetime (of up to 10 hours) to reduce the risk of abuse, even when you stay logged in. If your tickets expire, you will receive a Permission Denied error when you try to access your files and a password prompt when you try to connect to another computer. When you want your program to be able to access your files for longer than the valid ticket lifetime, you’ll have to renew your ticket (repeatedly) until your program is done. Kerberos tickets can be renewed up to a maximum renewable life period of 7 days (again to reduce the risk of abuse).\nThe command klist -5 lists your cached Kerberos tickets together with their expiration time and maximum renewal time:\n$ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_random Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/01/01 00:00:00 01/01/01 10:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/08/01 00:00:00 Where:\n  Ticket cache: The Kerberos tickets that have been issued to you are stored in a ticket cache file. You can have multiple ticket cache files on the same computer (from different connections, for example) with different tickets and ticket expiration times. Some ticket cache files are automatically removed when you logout. Tip Make sure that you renew the tickets in the right ticket cache file (see this screen example).    Default principal: Your identity.\n  Service principal: The identity of services that you have gotten tickets for. You always need a Kerberos ticket-granting ticket (krbtgt) in order to obtain other tickets for specific services like accessing files (nfs) or connecting to computers (host).\n  Valid starting, Expires: Your ticket is only valid between these times (this period is called the valid lifetime). After this time you will not be able to use the service nor automatically renew the ticket (without password).\n  Renew until: Your ticket can only be renewed without password up to this time. After this time you will have to obtain a new ticket using your password.\n  Renewing Kerberos tickets If you have a valid Kerberos krbtgt ticket, you can renew it at any time (until it expires) by running the command kinit -R:\n$ kinit -R $ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_random Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/01/01 01:00:00 01/01/01 11:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/08/01 00:00:00  Note Renewing the ticket will not change the duration of the valid lifetime, i.e. a krbtgt ticket with a valid lifetime of 1 hour will, after renewal, be valid for another hour.  When the krbtgt ticket has expired or reached it’s renew until time, you will have to obtain a new ticket by running kinit -r 7d (note the difference in case for the r) and authenticating with your password:\n$ kinit -r 7d Password for YourNetID@TUDELFT.NET: $ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_random Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/01/01 11:00:00 01/01/01 21:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/08/01 11:00:00 The new ticket will have a valid lifetime of 10 hours and a renewable life of 7 days.\nOn the TU Delft Linux desktops your Kerberos ticket is refreshed (i.e. replaced by a new ticket) automatically every time you enter your password for unlocking the screen saver.\nTip Do not disable the screen saver password lock.  On remote computers you have to manually renew your tickets before they expire.\nSlurm \u0026 Kerberos  Slurm caches your Kerberos ticket, and uses it to execute your job Regularly renew the ticket in Slurm’s cache while your jobs are queued or running:  $ auks -a Auks API request succeed  To automatically renew your ticket in Slurm’s cache until you change your NetID password, run the following on the login1 server:  $ install_keytab Password for somebody@TUDELFT.NET: Installed keytab. You need to rerun this command whenever you change your NetID password (at least every 6 months). Otherwise, the automatic renewal will not work and you will receive a warning e-mail.\nRenewal using screen On the compute servers, the screen program has been modified to allow jobs to run unattended for up to 7 days. It creates a private ticket cache (to prevent the cache from being destroyed at logout) and automatically renews your ticket up to the maximum renewable life. For example, start MATLAB in Screen with screen matlab (the order is important!).\n$ screen matlab Warning: No display specified. You will not be able to display graphics on the screen. \u003c M A T L A B (R) \u003e Copyright 1984-2010 The MathWorks, Inc. Version 7.11.0.584 (R2010b) 64-bit (glnxa64) August 16, 2010 To get started, type one of these: helpwin, helpdesk, or demo. For product information, visit www.mathworks.com. \u003e\u003e For longer jobs you have to manually obtain a new ticket at least every 7 days by running kinit -r 7d from within screen (so you use the specific ticket cache file that screen is using):\n connect to screen (screen -r), create a new window (Ctrl-a c), run kinit -r 7d, exit the window (exit) and detach from screen (Ctrl-a d).  $ kinit -r 7d Password for YourNetID@TUDELFT.NET: $ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_private Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/08/01 09:00:00 01/08/01 19:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/15/01 09:00:00 $ exit  Tip Use a repeating reminder (twice a week) in your agenda so you don’t forget.  Important When the end of the renewable life is reached, your tickets expire and your program(s) will return Permission denied errors when trying to access your files. Your program(s) will not be terminated automatically; you still have to terminate the program(s) yourself.  Extra functionality can be provided by the k5start and krenew programs. On most computers these are not available by default but can be installed (ask Robbert).\n","categories":"","description":"How to submit jobs to slurm?\n","excerpt":"How to submit jobs to slurm?\n","ref":"/docs/job_submissions/_index-3/","tags":"","title":"Job submission and management"},{"body":"Job Scripts Job scripts are text files, where the header set of directives that specify compute resources, and the remainder is the code that needs to run. All resources and scheduling are specified in the header as #SBATCH directives (see man sbatch for more information). Code could be a set of steps to run in series, or parallel tasks within these steps (see Slurm job’s terminology).\nThe code snippet below is a template script that can be customized to run jobs on DAIC. A useful tool that can be used to streamline the debugging of such scripts is ShellCheck   .\n#!/bin/sh #SBATCH --partition=general # Request partition. Default is 'general'  #SBATCH --qos=short # Request Quality of Service. Default is 'short' (maximum run time: 4 hours) #SBATCH --time=0:01:00 # Request run time (wall-clock). Default is 1 minute #SBATCH --ntasks=1 # Request number of parallel tasks per job. Default is 1 #SBATCH --cpus-per-task=2 # Request number of CPUs (threads) per task. Default is 1 (note: CPUs are always allocated to jobs per 2). #SBATCH --mem=1024 # Request memory (MB) per node. Default is 1024MB (1GB). For multiple tasks, specify --mem-per-cpu instead #SBATCH --mail-type=END # Set mail type to 'END' to receive a mail when the job finishes.  #SBATCH --output=slurm_%j.out # Set name of output log. %j is the Slurm jobId #SBATCH --error=slurm_%j.err # Set name of error log. %j is the Slurm jobId /usr/bin/scontrol show job -d \"$SLURM_JOB_ID\" # check sbatch directives are working # Remaining job commands go below here. For example, to run a Matlab script named \"matlab_script.m\", uncomment: #module use /opt/insy/modulefiles # Use DAIC INSY software collection #module load matlab/R2020b # Load Matlab 2020b version #srun matlab \u003c matlab_script.m # Computations should be started with 'srun'.  Note  DAIC is dual-threaded. It means that CPUs are automatically allocated in multiples of 2. Thus, in your job use (a multiple of) 2 threads. Do not enable mails when submitting large numbers (\u003e20) of jobs at once   Job submission and monitoring Once a job script (see Job script) is ready, it is time to send it to the cluster and start computing (see Data transfer methods).\nHint To check whether you are working on your machine or the cluster, observe the output of the hostname command. The following examples show various outputs depending on where the command was ran from:\n$ hostname login1.hpc.tudelft.nl # You are in a DAIC login1 node $ $ hostname grs1.hpc.tudelft.nl # You are in the DAIC compute node named grs1 $ $ hostname \u003cYourNetID\u003e # You are in your TU Delft laptop/PC   To submit a job script jobscript.sbatch, login to DAIC, and:\n To only test:  $ sbatch --test-only jobscript.sbatch Job 1 to start at 2015-06-30T14:00:00 using 2 processors on nodes insy15 in partition general  To actually submit the job and do the computations:  $ sbatch jobscript.sbatch Submitted batch job 2  To check your job has actually been submitted:  $ squeue -u SomeNetID # Replace SomeNetId with your NetID  JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 2 general jobscip SomeNetI R 0:01 1 insy15  And to check the log of your job, use an editor or viewer of choice (eg, vi, nano or simply cat) to view the log:  $ cat slurm-2.out JobId=2 JobName=jobscript.sbatch UserId=SomeNetId(123) GroupId=domain users(100513) MCS_label=N/A Priority=23909774 Nice=0 Account=ewi-insy QOS=short JobState=RUNNING Reason=None Dependency=(null) Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0 DerivedExitCode=0:0 RunTime=00:00:00 TimeLimit=00:01:00 TimeMin=N/A SubmitTime=2015-06-30T14:00:00 EligibleTime=2015-06-30T14:00:00 AccrueTime=2015-06-30T14:00:00 StartTime=2015-06-30T14:00:01 EndTime=2015-06-30T14:01:01 Deadline=N/A SuspendTime=None SecsPreSuspend=0 LastSchedEval=2015-06-30T14:01:01 Scheduler=Main Partition=general AllocNode:Sid=login1:2220 ReqNodeList=(null) ExcNodeList=(null) NodeList=insy15 BatchHost=insy15 NumNodes=1 NumCPUs=2 NumTasks=1 CPUs/Task=2 ReqB:S:C:T=0:0:*:* TRES=cpu=2,mem=1G,node=1,billing=1 Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=* JOB_GRES=(null) Nodes=insy15 CPU_IDs=26-27 Mem=1024 GRES= MinCPUsNode=2 MinMemoryNode=1G MinTmpDiskNode=50M Features=(null) DelayBoot=00:00:00 OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null) Command=/home/nfs/SomeNetId/jobscript.sbatch WorkDir=/home/nfs/SomeNetId StdErr=/home/nfs/SomeNetId/slurm_2.err StdIn=/dev/null StdOut=/home/nfs/SomeNetId/slurm_2.out Power= MailUser=SomeNetId@tudelft.nl MailType=END  And finally, to cancel a given job:  $ scancel \u003cjobID\u003e  Note It is possible to specify the sbatch directives, like --mem, --ntasks, … etc in the command line as in:\n$ sbatch --time=00:02:00 jobscript.sbatch This specification is generally not recommended for production, as it is less reproducible than specifying within the job script itself.\n Interactive jobs on compute nodes To work interactively on a node, eg, to debug a running code, or test on a GPU, start an interactive session using sinteractve \u003ccompute requirements\u003e. If no parameters were provided, the default are applied. \u003ccompute requirement\u003e can be specified the same way as sbatch directives within an sbatch script (see Job scripts), as in the examples below:\n$ hostname # check you are in one of the login nodes login1.hpc.tudelft.nl $ sinteractive 16:07:20 up 12 days, 4:09, 2 users, load average: 7.06, 7.04, 7.12 $ hostname # check you are in a compute node insy15 $ squeue -u SomeNetID # Replace SomeNetId with your NetID  JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 2 general bash SomeNetI R 1:23 1 insy15 $ logout # exit the interactive job To request a node with certain compute requirements:\n$ sinteractive --ntasks=1 --cpus-per-task=2 --mem=4096 16:07:20 up 12 days, 4:09, 2 users, load average: 7.06, 7.04, 7.12  Warning When you logout from an interactive session, all running processes will be terminated  Note Requesting interactive sessions is subject to the same resource availability constraints as submitting an sbatch script. It means you may need to wait until resources are available as you would when you submit an sbatch script  Jobs on GPU resources Some DAIC nodes have GPUs of different types, that can be used for various compute purposes (see DAIC GPUs).\nTo request a gpu for a job, use the sbatch directive --gres=gpu[:type][:number], where the optional [:type] and [:number] specify the type and number of the GPUs requested, as in the examples below:   Slurm directives to request gpus for a job\n  Note For CUDA programs, first, load the needed modules (CUDA, cuDNN) before running your code. See Environment modules  An example batch script with GPU resources\n#!/bin/sh #SBATCH --partition=general # Request partition. Default is 'general'  #SBATCH --qos=short # Request Quality of Service. Default is 'short' (maximum run time: 4 hours) #SBATCH --time=0:01:00 # Request run time (wall-clock). Default is 1 minute #SBATCH --ntasks=1 # Request number of parallel tasks per job. Default is 1 #SBATCH --cpus-per-task=2 # Request number of CPUs (threads) per task. Default is 1 (note: CPUs are always allocated to jobs per 2). #SBATCH --mem=1024 # Request memory (MB) per node. Default is 1024MB (1GB). For multiple tasks, specify --mem-per-cpu instead #SBATCH --mail-type=END # Set mail type to 'END' to receive a mail when the job finishes.  #SBATCH --output=slurm_%j.out # Set name of output log. %j is the Slurm jobId #SBATCH --error=slurm_%j.err # Set name of error log. %j is the Slurm jobId #SBATCH --gres=gpu:1 # Request 1 GPU # Measure GPU usage of your job (initialization) previous=$(/usr/bin/nvidia-smi --query-accounted-apps='gpu_utilization,mem_utilization,max_memory_usage,time' --format='csv' | /usr/bin/tail -n '+2') /usr/bin/nvidia-smi # Check sbatch settings are working (it should show the GPU that you requested) # Remaining job commands go below here. For example, to run python code that makes use of GPU resources: # Uncomment these lines and adapt them to load the software that your job requires #module use /opt/insy/modulefiles # Use DAIC INSY software collection #module load cuda/11.2 cudnn/11.2-8.1.1.33 # Load certain versions of cuda and cudnn  #srun python my_program.py # Computations should be started with 'srun'. For example: # Measure GPU usage of your job (result) /usr/bin/nvidia-smi --query-accounted-apps='gpu_utilization,mem_utilization,max_memory_usage,time' --format='csv' | /usr/bin/grep -v -F \"$previous\" Similarly, to interactively work in a GPU node:\n$ hostname # check you are in one of the login nodes login1.hpc.tudelft.nl $ $ sinteractive --cpus-per-task=1 --mem=500 --time=00:01:00 --gres=gpu:v100:1 Note: interactive sessions are automatically terminated when they reach their time limit (1 hour)! srun: job 8607665 queued and waiting for resources srun: job 8607665 has been allocated resources 15:27:18 up 51 days, 3:04, 0 users, load average: 62,09, 59,43, 44,04 SomeNetID@insy11:~$ SomeNetID@insy11:~$ hostname # check you are in one of the compute nodes insy11.hpc.tudelft.nl SomeNetID@insy11:~$ SomeNetID@insy11:~$ nvidia-smi # check characteristics of GPU Mon Jul 24 15:37:01 2023 +---------------------------------------------------------------------------------------+ | NVIDIA-SMI 530.30.02 Driver Version: 530.30.02 CUDA Version: 12.1 | |-----------------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+======================+======================| | 0 Tesla V100-SXM2-32GB On | 00000000:88:00.0 Off | 0 | | N/A 32C P0 40W / 300W| 0MiB / 32768MiB | 0% Default | | | | N/A | +-----------------------------------------+----------------------+----------------------+ +---------------------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=======================================================================================| | No running processes found | +---------------------------------------------------------------------------------------+ SomeNetID@insy11:~$ SomeNetID@insy11:~$ exit # exit the interactive session Deploying dependent jobs (job chains) In certain scenarios, it might be desirable to condition the execution of a certain job on the status of another job. In such cases, the sbatch directive --dependency=\u003ccondition\u003e:\u003cjobID\u003e can be used, where \u003ccondition\u003e specifies the type of dependency (See table 2), and \u003cjobID\u003e is the slurm jobID upon which dependency is based. To specify more than one dependency, the , separator is used to indicate that all dependencies must be specified, and, ? is used denotes that any dependency may be satisfied.\nFor example, assume the slurm job scripts, job_1.sbatch, … job_3.sbatch need to run sequentially one after the other. To start this chain, submit the first job and obtain its jobID:\n$ sbatch job_1.sbatch Submitted batch job 8580135 Next, submit the second job to run only if the first job is successful:\n$ sbatch --dependency=afterok:8580135 job_2.sbatch Submitted batch job 8580136  Note Note that if the first job (with jobID 8580135 in the example) fails, the second job (with jobID 8580136) will not run, but it will remain in the queue. You have to use scancel 8580136 to cancel this job  And, now, to run the third job only after the first two jobs have both run successfully:\n$ sbatch --dependency=afterok:8580135,8580136 job_3.sbatch Submitted batch job 8580140 Alternatively, if the third job is dependent on either job running successfully:\n$ sbatch --dependency=afterok:8580135?8580136 job_3.sbatch Submitted batch job 8580141  Warning  If the jobs within a chain involve copying data files to a local disk (/tmp) on a node, you need to make sure all jobs use the same node (--nodelist=\u003cnode\u003e, for example --nodelist=insy15)     Table 2: Possible sbatch dependency conditions    Argument Description     after This job can begin execution after the specified jobs have begun execution   afterany This job can begin execution after the specified jobs have terminated.   aftercorr A task of this job array can begin execution after the corresponding task ID in the specified job has completed successfully   afternotok This job can begin execution after the specified jobs have terminated in some failed state   afterok This job can begin execution after the specified jobs have successfully executed   singleton This job can begin execution after any previously launched jobs sharing the same job name and user have terminated    Kerberos Authentication Kerberos is an authentication protocol which uses tickets to authenticate users (and computers). You automatically get a ticket when you log in with your password on a TU Delft installed computer. You can use this ticket to authenticate yourself without password when connecting to other computers or accessing your files. To protect you from misuse, the ticket expires after 10 hours or less (even when you’re still logged in).\nFile access Your Linux and Windows home  directories and the Group and Bulk shares are located on network fileservers, which allows you to access your files from all TU Delft installed computers. Kerberos authentication is used to enable access to, or protect, your files. Without a valid Kerberos ticket (e.g. when the ticket has expired) you will not be able to access your files but instead you will receive a Permission denied error.\nLifetime of Kerberos Tickets Kerberos tickets have a limited valid lifetime (of up to 10 hours) to reduce the risk of abuse, even when you stay logged in. If your tickets expire, you will receive a Permission Denied error when you try to access your files and a password prompt when you try to connect to another computer. When you want your program to be able to access your files for longer than the valid ticket lifetime, you’ll have to renew your ticket (repeatedly) until your program is done. Kerberos tickets can be renewed up to a maximum renewable life period of 7 days (again to reduce the risk of abuse).\nThe command klist -5 lists your cached Kerberos tickets together with their expiration time and maximum renewal time:\n$ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_random Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/01/01 00:00:00 01/01/01 10:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/08/01 00:00:00 Where:\n  Ticket cache: The Kerberos tickets that have been issued to you are stored in a ticket cache file. You can have multiple ticket cache files on the same computer (from different connections, for example) with different tickets and ticket expiration times. Some ticket cache files are automatically removed when you logout. Tip Make sure that you renew the tickets in the right ticket cache file (see this screen example).    Default principal: Your identity.\n  Service principal: The identity of services that you have gotten tickets for. You always need a Kerberos ticket-granting ticket (krbtgt) in order to obtain other tickets for specific services like accessing files (nfs) or connecting to computers (host).\n  Valid starting, Expires: Your ticket is only valid between these times (this period is called the valid lifetime). After this time you will not be able to use the service nor automatically renew the ticket (without password).\n  Renew until: Your ticket can only be renewed without password up to this time. After this time you will have to obtain a new ticket using your password.\n  Renewing Kerberos tickets If you have a valid Kerberos krbtgt ticket, you can renew it at any time (until it expires) by running the command kinit -R:\n$ kinit -R $ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_random Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/01/01 01:00:00 01/01/01 11:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/08/01 00:00:00  Note Renewing the ticket will not change the duration of the valid lifetime, i.e. a krbtgt ticket with a valid lifetime of 1 hour will, after renewal, be valid for another hour.  When the krbtgt ticket has expired or reached it’s renew until time, you will have to obtain a new ticket by running kinit -r 7d (note the difference in case for the r) and authenticating with your password:\n$ kinit -r 7d Password for YourNetID@TUDELFT.NET: $ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_random Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/01/01 11:00:00 01/01/01 21:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/08/01 11:00:00 The new ticket will have a valid lifetime of 10 hours and a renewable life of 7 days.\nOn the TU Delft Linux desktops your Kerberos ticket is refreshed (i.e. replaced by a new ticket) automatically every time you enter your password for unlocking the screen saver.\nTip Do not disable the screen saver password lock.  On remote computers you have to manually renew your tickets before they expire.\nSlurm \u0026 Kerberos  Slurm caches your Kerberos ticket, and uses it to execute your job Regularly renew the ticket in Slurm’s cache while your jobs are queued or running:  $ auks -a Auks API request succeed  To automatically renew your ticket in Slurm’s cache until you change your NetID password, run the following on the login1 server:  $ install_keytab Password for somebody@TUDELFT.NET: Installed keytab. You need to rerun this command whenever you change your NetID password (at least every 6 months). Otherwise, the automatic renewal will not work and you will receive a warning e-mail.\nRenewal using screen On the compute servers, the screen program has been modified to allow jobs to run unattended for up to 7 days. It creates a private ticket cache (to prevent the cache from being destroyed at logout) and automatically renews your ticket up to the maximum renewable life. For example, start MATLAB in Screen with screen matlab (the order is important!).\n$ screen matlab Warning: No display specified. You will not be able to display graphics on the screen. \u003c M A T L A B (R) \u003e Copyright 1984-2010 The MathWorks, Inc. Version 7.11.0.584 (R2010b) 64-bit (glnxa64) August 16, 2010 To get started, type one of these: helpwin, helpdesk, or demo. For product information, visit www.mathworks.com. \u003e\u003e For longer jobs you have to manually obtain a new ticket at least every 7 days by running kinit -r 7d from within screen (so you use the specific ticket cache file that screen is using):\n connect to screen (screen -r), create a new window (Ctrl-a c), run kinit -r 7d, exit the window (exit) and detach from screen (Ctrl-a d).  $ kinit -r 7d Password for YourNetID@TUDELFT.NET: $ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_private Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/08/01 09:00:00 01/08/01 19:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/15/01 09:00:00 $ exit  Tip Use a repeating reminder (twice a week) in your agenda so you don’t forget.  Important When the end of the renewable life is reached, your tickets expire and your program(s) will return Permission denied errors when trying to access your files. Your program(s) will not be terminated automatically; you still have to terminate the program(s) yourself.  Extra functionality can be provided by the k5start and krenew programs. On most computers these are not available by default but can be installed (ask Robbert).\n","categories":"","description":"How to submit jobs to slurm?\n","excerpt":"How to submit jobs to slurm?\n","ref":"/docs/job_submissions/_index-4/","tags":"","title":"Job submission and management"},{"body":"Checking slurm jobs Sometimes, it may be desirable to inspect slurm jobs beyond their status in the queue. For example, to check which script was submitted, or how the resources were requested and allocated. Below are a few useful commands for this purpose:\n See job definition  $ scontrol show job 8580148 JobId=8580148 JobName=jobscript.sbatch UserId=SomeNetID(123) GroupId=domain users(100513) MCS_label=N/A Priority=23721804 Nice=0 Account=ewi-insy QOS=short JobState=RUNNING Reason=None Dependency=(null) Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0 RunTime=00:00:12 TimeLimit=00:01:00 TimeMin=N/A SubmitTime=2023-07-10T06:41:57 EligibleTime=2023-07-10T06:41:57 AccrueTime=2023-07-10T06:41:57 StartTime=2023-07-10T06:41:58 EndTime=2023-07-10T06:42:58 Deadline=N/A SuspendTime=None SecsPreSuspend=0 LastSchedEval=2023-07-10T06:41:58 Scheduler=Main Partition=general AllocNode:Sid=login1:19162 ReqNodeList=(null) ExcNodeList=(null) NodeList=awi18 BatchHost=awi18 NumNodes=1 NumCPUs=2 NumTasks=1 CPUs/Task=2 ReqB:S:C:T=0:0:*:* TRES=cpu=2,mem=1G,node=1,billing=1 Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=* MinCPUsNode=2 MinMemoryNode=1G MinTmpDiskNode=50M Features=(null) DelayBoot=00:00:00 OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null) Command=/home/nfs/SomeNetID/jobscript.sbatch WorkDir=/home/nfs/SomeNetID StdErr=/home/nfs/SomeNetID/slurm_8580148.err StdIn=/dev/null StdOut=/home/nfs/SomeNetID/slurm_8580148.out Power= MailUser=SomeNetId@tudelft.nl MailType=END  See statistics of a running job  $ sstat 1 JobID AveRSS AveCPU NTasks AveDiskRead AveDiskWrite ------- ------- ------- ------- ------------ ------------ 1.0 426K 00:00.0 1 0.52M 0.01M  See accounting information of a finished job (also see –long option)  $ sacct -j 8580148 JobID JobName Partition Account AllocCPUS State ExitCode ------------ ---------- ---------- ---------- ---------- ---------- -------- 8580148 jobscript+ general ewi-insy 2 COMPLETED 0:0 8580148.bat+ batch ewi-insy 2 COMPLETED 0:0 See overall job efficiency of a finished job\n$ seff 8580148 Job ID: 8580148 Cluster: insy User/Group: SomeNetID/domain users State: COMPLETED (exit code 0) Nodes: 1 Cores per node: 2 CPU Utilized: 00:00:00 CPU Efficiency: 0.00% of 00:01:00 core-walltime Job Wall-clock time: 00:00:30 Memory Utilized: 340.00 KB Memory Efficiency: 0.03% of 1.00 GB  See partition definitions  $ scontrol show partition PartitionName=general AllowGroups=ALL AllowAccounts=ALL DenyQos=influence AllocNodes=login[1-3],oodtest Default=YES QoS=N/A DefaultTime=00:01:00 DisableRootJobs=NO ExclusiveUser=NO GraceTime=0 Hidden=NO MaxNodes=UNLIMITED MaxTime=UNLIMITED MinNodes=0 LLN=NO MaxCPUsPerNode=UNLIMITED Nodes=3dgi[1-2],100plus,awi[01-26],cor1,gpu[01-11],grs[1-4],influ[1-6],insy[11-16],tbm5,wis1 PriorityJobFactor=1 PriorityTier=1 RootOnly=NO ReqResv=NO OverSubscribe=NO OverTimeLimit=NONE PreemptMode=OFF State=UP TotalCPUs=4064 TotalNodes=59 SelectTypeParameters=NONE JobDefaults=(null) DefMemPerNode=1024 MaxMemPerNode=UNLIMITED TRESBillingWeights=CPU=0.5,Mem=0.083333333G,GRES/gpu=16.0  See Quality of Service definitions  $ sacctmgr list qos Name Priority GraceTime Preempt PreemptExemptTime PreemptMode Flags UsageThres UsageFactor GrpTRES GrpTRESMins GrpTRESRunMin GrpJobs GrpSubmit GrpWall MaxTRES MaxTRESPerNode MaxTRESMins MaxWall MaxTRESPU MaxJobsPU MaxSubmitPU MaxTRESPA MaxJobsPA MaxSubmitPA MinTRES ---------- ---------- ---------- ---------- ------------------- ----------- ---------------------------------------- ---------- ----------- ------------- ------------- ------------- ------- --------- ----------- ------------- -------------- ------------- ----------- ------------- --------- ----------- ------------- --------- ----------- ------------- normal 0 00:00:00 cluster DenyOnLimit 1.000000 cpu=1 short 50 00:00:00 cluster DenyOnLimit 1.000000 cpu=3562,gre+ 65536 04:00:00 cpu=2096,gre+ 10000 cpu=1,mem=1M long 25 00:00:00 cluster DenyOnLimit 1.000000 cpu=3144,gre+ 65536 7-00:00:00 cpu=838,gres+ 1000 cpu=1,mem=1M infinite 0 00:00:00 cluster DenyOnLimit 1.000000 cpu=32,gres/+ 65536 1 100 cpu=1,mem=1M interacti+ 100 00:00:00 cluster DenyOnLimit 2.000000 65536 01:00:00 cpu=2,gres/g+ 1 1 cpu=1,mem=1M student 10 00:00:00 cluster DenyOnLimit 1.000000 cpu=192,gres+ 65536 04:00:00 cpu=2,gres/g+ 1 100 cpu=1,mem=1M reservati+ 100 00:00:00 cluster DenyOnLimit,RequiresReservation 1.000000 65536 10000 cpu=1,mem=1M influence 100 00:00:00 cluster DenyOnLimit 1.000000 65536 10000 cpu=1,mem=1M guest-sho+ 10 00:00:00 cluster DenyOnLimit 1.000000 cpu=200,gres+ 65536 04:00:00 cpu=128,gres+ 100 cpu=1,mem=1M guest-long 0 00:00:00 cluster DenyOnLimit 1.000000 cpu=200,gres+ 65536 7-00:00:00 cpu=128,gres+ 1 10 cpu=1,mem=1M medium 35 00:00:00 cluster DenyOnLimit 1.000000 cpu=3352,gre+ 65536 1-12:00:00 cpu=1466,gre+ 2000 cpu=1,mem=1M Kerberos Authentication Kerberos is an authentication protocol which uses tickets to authenticate users (and computers). You automatically get a ticket when you log in with your password on a TU Delft installed computer. You can use this ticket to authenticate yourself without password when connecting to other computers or accessing your files. To protect you from misuse, the ticket expires after 10 hours or less (even when you’re still logged in).\nFile access Your Linux and Windows home  directories and the Group and Bulk shares are located on network fileservers, which allows you to access your files from all TU Delft installed computers. Kerberos authentication is used to enable access to, or protect, your files. Without a valid Kerberos ticket (e.g. when the ticket has expired) you will not be able to access your files but instead you will receive a Permission denied error.\nLifetime of Kerberos Tickets Kerberos tickets have a limited valid lifetime (of up to 10 hours) to reduce the risk of abuse, even when you stay logged in. If your tickets expire, you will receive a Permission Denied error when you try to access your files and a password prompt when you try to connect to another computer. When you want your program to be able to access your files for longer than the valid ticket lifetime, you’ll have to renew your ticket (repeatedly) until your program is done. Kerberos tickets can be renewed up to a maximum renewable life period of 7 days (again to reduce the risk of abuse).\nThe command klist -5 lists your cached Kerberos tickets together with their expiration time and maximum renewal time:\n$ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_random Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/01/01 00:00:00 01/01/01 10:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/08/01 00:00:00 Where:\n  Ticket cache: The Kerberos tickets that have been issued to you are stored in a ticket cache file. You can have multiple ticket cache files on the same computer (from different connections, for example) with different tickets and ticket expiration times. Some ticket cache files are automatically removed when you logout. Tip Make sure that you renew the tickets in the right ticket cache file (see this screen example).    Default principal: Your identity.\n  Service principal: The identity of services that you have gotten tickets for. You always need a Kerberos ticket-granting ticket (krbtgt) in order to obtain other tickets for specific services like accessing files (nfs) or connecting to computers (host).\n  Valid starting, Expires: Your ticket is only valid between these times (this period is called the valid lifetime). After this time you will not be able to use the service nor automatically renew the ticket (without password).\n  Renew until: Your ticket can only be renewed without password up to this time. After this time you will have to obtain a new ticket using your password.\n  Renewing Kerberos tickets If you have a valid Kerberos krbtgt ticket, you can renew it at any time (until it expires) by running the command kinit -R:\n$ kinit -R $ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_random Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/01/01 01:00:00 01/01/01 11:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/08/01 00:00:00  Note Renewing the ticket will not change the duration of the valid lifetime, i.e. a krbtgt ticket with a valid lifetime of 1 hour will, after renewal, be valid for another hour.  When the krbtgt ticket has expired or reached it’s renew until time, you will have to obtain a new ticket by running kinit -r 7d (note the difference in case for the r) and authenticating with your password:\n$ kinit -r 7d Password for YourNetID@TUDELFT.NET: $ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_random Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/01/01 11:00:00 01/01/01 21:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/08/01 11:00:00 The new ticket will have a valid lifetime of 10 hours and a renewable life of 7 days.\nOn the TU Delft Linux desktops your Kerberos ticket is refreshed (i.e. replaced by a new ticket) automatically every time you enter your password for unlocking the screen saver.\nTip Do not disable the screen saver password lock.  On remote computers you have to manually renew your tickets before they expire.\nSlurm \u0026 Kerberos  Slurm caches your Kerberos ticket, and uses it to execute your job Regularly renew the ticket in Slurm’s cache while your jobs are queued or running:  $ auks -a Auks API request succeed  To automatically renew your ticket in Slurm’s cache until you change your NetID password, run the following on the login1 server:  $ install_keytab Password for somebody@TUDELFT.NET: Installed keytab. You need to rerun this command whenever you change your NetID password (at least every 6 months). Otherwise, the automatic renewal will not work and you will receive a warning e-mail.\nRenewal using screen On the compute servers, the screen program has been modified to allow jobs to run unattended for up to 7 days. It creates a private ticket cache (to prevent the cache from being destroyed at logout) and automatically renews your ticket up to the maximum renewable life. For example, start MATLAB in Screen with screen matlab (the order is important!).\n$ screen matlab Warning: No display specified. You will not be able to display graphics on the screen. \u003c M A T L A B (R) \u003e Copyright 1984-2010 The MathWorks, Inc. Version 7.11.0.584 (R2010b) 64-bit (glnxa64) August 16, 2010 To get started, type one of these: helpwin, helpdesk, or demo. For product information, visit www.mathworks.com. \u003e\u003e For longer jobs you have to manually obtain a new ticket at least every 7 days by running kinit -r 7d from within screen (so you use the specific ticket cache file that screen is using):\n connect to screen (screen -r), create a new window (Ctrl-a c), run kinit -r 7d, exit the window (exit) and detach from screen (Ctrl-a d).  $ kinit -r 7d Password for YourNetID@TUDELFT.NET: $ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_private Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/08/01 09:00:00 01/08/01 19:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/15/01 09:00:00 $ exit  Tip Use a repeating reminder (twice a week) in your agenda so you don’t forget.  Important When the end of the renewable life is reached, your tickets expire and your program(s) will return Permission denied errors when trying to access your files. Your program(s) will not be terminated automatically; you still have to terminate the program(s) yourself.  Extra functionality can be provided by the k5start and krenew programs. On most computers these are not available by default but can be installed (ask Robbert).\n","categories":"","description":"How to submit jobs to slurm?\n","excerpt":"How to submit jobs to slurm?\n","ref":"/docs/job_submissions/_index-5/","tags":"","title":"Job submission and management"},{"body":"Slurm’s job scheduling and waiting times When slurm is not configured for FIFO scheduling, jobs are prioritized in the following order:\n Jobs that can preempt: Not enabled in DAIC Jobs with an advanced reservation: See Slurm's Advanced Resource Reservation Guide    Partition PriorityTier: See Priority tiers Job priority: See Priority calculations and QoS priority Job ID  Priority tiers DAIC partitions are tiered: the general partition is in the lowest priority tier, department partitions (eg, insy, st) are in the middle priority tier, and partitions for specific groups (eg, visionlab, wis) are in the highest priority tier. Those partitions correspond to resources contributed by the respective groups or departments (see Brief history of DAIC).\nWhen resources become available, the scheduler will first look for jobs in the highest priority partition that those resources are in, and start the highest (user) priority jobs that fit within the resources (if any). When resources remain, the scheduler will check the next lower priority tier, and so on. Finally, the scheduler will try to backfill lower (user) priority jobs that fit (if any).\nThe partition priorities have no impact on resources that are in use, so jobs have to wait until the resources become available.\nWhere to submit jobs? The idea behind the tiering is that you submit to all partitions, e.g. --partition=wis,st,general, and let the scheduler figure out where the job can start the soonest. This should give the job the highest possible priority on the different partitions (resources) in the cluster, at no cost for yourself or others.\nResources of all partitions (eg, st) are also part of the general partition (see Fig 1). Thus:\n submitting to the general partition allows jobs to use all nodes submitting to group-specific partitions alone results in longer waiting times, since the general partition has much more resources than any of them (The bigger the resource pool, the more chances a job has to be scheduled or back-filled) The optimal way is to submit to both general and group-specific partitions when accessible. This is to skip over higher-priority jobs that would otherwise get started first on resources that are also in the specific partition.  Priority calculations Slurm continually calculates job priorities and schedules the execution of jobs based on its configurations. A few configuration parameters affect priority computations:\n SchedulerType: The type of scheduling used based on available resources, requested resources, and job priorities. On DAIC, slurm is used with backfill scheduling mechanism. This mechanism allows low priority jobs to backfill idle resources if doing so does not delay the expected start time of any high priority job (based on resource availability).  Tip With sched/backfil, jobs can only be started when the resources that they request fit within the available idle resources. Thus:\n The fewer resources a job request, the higher the chance that it will fit within the available idle resources. The more resources a job request, the long it will have to wait before enough resources become available to start. To check how the cluster is configured, you may run:  $ scontrol show config | grep SchedulerType SchedulerType = sched/backfil More details is available in Slurm’s SchedulerType\n  PriorityType: The way priority is computed. On DAIC, a multifactor computation is applied, where job priority at any given time is a weighted sum of the following factors:  Fairshare: a measure of the amount of resources that a group (ie account in slurm terminology) has contributed, and the historical usage of the group and the user. QOS: the quality of service associated with the job, which is specified with the slurm --qos directive (see QoS priority).    Info The whole idea behind the FairShare scheduling in DAIC is to share all the available resources fairly and efficiently with all users (instead of having strict limitations in the amount of resource use or in which hardware users can compute). The resources in the cluster are contributed in different amounts by different groups (see Brief history of DAIC), and the scheduler makes sure that each group can use a share of the resource relative to what the group contributed. To check how the cluster is configured you may run:\n$ scontrol show config | grep PriorityType PriorityType = priority/multifactor $ sprio --weights JOBID PARTITION PRIORITY SITE FAIRSHARE QOS Weights 1 20000000 40000000   The following commands are useful for checking prioritization of your own jobs:\n   Command Purpose     sprio -j \u003cYourJobID\u003e Determine the priority of your job   squeue -j \u003cYourJobID\u003e --start Request your job’s estimated start time   sshare -u \u003cYourNetID\u003e Determine your current fairshare value    Info To get more complete priority configurations of a cluster, run the command:\n$ scontrol show config | grep ^Priority PriorityParameters = (null) PrioritySiteFactorParameters = (null) PrioritySiteFactorPlugin = (null) PriorityDecayHalfLife = 2-00:00:00 PriorityCalcPeriod = 00:05:00 PriorityFavorSmall = No PriorityFlags = PriorityMaxAge = 7-00:00:00 PriorityUsageResetPeriod = NONE PriorityType = priority/multifactor PriorityWeightAge = 0 PriorityWeightAssoc = 0 PriorityWeightFairShare = 20000000 PriorityWeightJobSize = 0 PriorityWeightPartition = 0 PriorityWeightQOS = 40000000 PriorityWeightTRES = (null)   QoS priority The purpose of the (multiple) QoSs in DAIC is to optimize the throughput of the cluster and to reduce the waiting times for jobs:\n Long jobs block resources for a long time, thus leading to long waiting times and fragmentation of resources. Short jobs block resources only for short times, and can more easily fill in the gaps in the scheduling of resources (thus start sooner), and are therefore better for throughput and waiting times.  Thus, DAIC has the following policy:\n  To stimulate short jobs, the short QoS has a higher priority, and allows you to use a larger part of all resources, than the medium and long QoS.\n  To prevent long jobs from blocking all resources in the cluster for long times (thus causing long waiting times), only a certain part of all cluster resources is available to all running long QoS jobs (of all users) combined.\n  All running medium QoS jobs together can use a somewhat larger part of all resources in the cluster, and all running short QoS jobs combined are allowed to fill the biggest part of the cluster.\n These limits are called the QoS group limits. When this limit is reached, no new jobs with this QoS can be started, until some of the running jobs with this QoS finish and release some resources. The scheduler will indicate this with the reason QoS Group CPU/memory/GRES limit.    To prevent one user from single-handedly using all available resources in a certain QoS, there are also limits for the total resources that all running jobs of one user in a specific QoS can use.\n These are called the QoS per-user limits. When this limit is reached, no new jobs of this user with this QoS can be started, until some of the running jobs of this user and with this QoS finish and release some resources. The scheduler will indicate this with the reason QoS User CPU/memory/GRES limit.    These per-group and per-user limits (see Table 1) are set by the DAIC user board, and the scheduler strictly enforces these limits. Thus, no user can use more resources than the amount that was set by the user board. Any (perceived) imbalance in the use of resources by a certain QoS or user should not be held against a user or the scheduler, but should be discussed in the user board.\nKerberos Authentication Kerberos is an authentication protocol which uses tickets to authenticate users (and computers). You automatically get a ticket when you log in with your password on a TU Delft installed computer. You can use this ticket to authenticate yourself without password when connecting to other computers or accessing your files. To protect you from misuse, the ticket expires after 10 hours or less (even when you’re still logged in).\nFile access Your Linux and Windows home  directories and the Group and Bulk shares are located on network fileservers, which allows you to access your files from all TU Delft installed computers. Kerberos authentication is used to enable access to, or protect, your files. Without a valid Kerberos ticket (e.g. when the ticket has expired) you will not be able to access your files but instead you will receive a Permission denied error.\nLifetime of Kerberos Tickets Kerberos tickets have a limited valid lifetime (of up to 10 hours) to reduce the risk of abuse, even when you stay logged in. If your tickets expire, you will receive a Permission Denied error when you try to access your files and a password prompt when you try to connect to another computer. When you want your program to be able to access your files for longer than the valid ticket lifetime, you’ll have to renew your ticket (repeatedly) until your program is done. Kerberos tickets can be renewed up to a maximum renewable life period of 7 days (again to reduce the risk of abuse).\nThe command klist -5 lists your cached Kerberos tickets together with their expiration time and maximum renewal time:\n$ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_random Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/01/01 00:00:00 01/01/01 10:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/08/01 00:00:00 Where:\n  Ticket cache: The Kerberos tickets that have been issued to you are stored in a ticket cache file. You can have multiple ticket cache files on the same computer (from different connections, for example) with different tickets and ticket expiration times. Some ticket cache files are automatically removed when you logout. Tip Make sure that you renew the tickets in the right ticket cache file (see this screen example).    Default principal: Your identity.\n  Service principal: The identity of services that you have gotten tickets for. You always need a Kerberos ticket-granting ticket (krbtgt) in order to obtain other tickets for specific services like accessing files (nfs) or connecting to computers (host).\n  Valid starting, Expires: Your ticket is only valid between these times (this period is called the valid lifetime). After this time you will not be able to use the service nor automatically renew the ticket (without password).\n  Renew until: Your ticket can only be renewed without password up to this time. After this time you will have to obtain a new ticket using your password.\n  Renewing Kerberos tickets If you have a valid Kerberos krbtgt ticket, you can renew it at any time (until it expires) by running the command kinit -R:\n$ kinit -R $ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_random Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/01/01 01:00:00 01/01/01 11:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/08/01 00:00:00  Note Renewing the ticket will not change the duration of the valid lifetime, i.e. a krbtgt ticket with a valid lifetime of 1 hour will, after renewal, be valid for another hour.  When the krbtgt ticket has expired or reached it’s renew until time, you will have to obtain a new ticket by running kinit -r 7d (note the difference in case for the r) and authenticating with your password:\n$ kinit -r 7d Password for YourNetID@TUDELFT.NET: $ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_random Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/01/01 11:00:00 01/01/01 21:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/08/01 11:00:00 The new ticket will have a valid lifetime of 10 hours and a renewable life of 7 days.\nOn the TU Delft Linux desktops your Kerberos ticket is refreshed (i.e. replaced by a new ticket) automatically every time you enter your password for unlocking the screen saver.\nTip Do not disable the screen saver password lock.  On remote computers you have to manually renew your tickets before they expire.\nSlurm \u0026 Kerberos  Slurm caches your Kerberos ticket, and uses it to execute your job Regularly renew the ticket in Slurm’s cache while your jobs are queued or running:  $ auks -a Auks API request succeed  To automatically renew your ticket in Slurm’s cache until you change your NetID password, run the following on the login1 server:  $ install_keytab Password for somebody@TUDELFT.NET: Installed keytab. You need to rerun this command whenever you change your NetID password (at least every 6 months). Otherwise, the automatic renewal will not work and you will receive a warning e-mail.\nRenewal using screen On the compute servers, the screen program has been modified to allow jobs to run unattended for up to 7 days. It creates a private ticket cache (to prevent the cache from being destroyed at logout) and automatically renews your ticket up to the maximum renewable life. For example, start MATLAB in Screen with screen matlab (the order is important!).\n$ screen matlab Warning: No display specified. You will not be able to display graphics on the screen. \u003c M A T L A B (R) \u003e Copyright 1984-2010 The MathWorks, Inc. Version 7.11.0.584 (R2010b) 64-bit (glnxa64) August 16, 2010 To get started, type one of these: helpwin, helpdesk, or demo. For product information, visit www.mathworks.com. \u003e\u003e For longer jobs you have to manually obtain a new ticket at least every 7 days by running kinit -r 7d from within screen (so you use the specific ticket cache file that screen is using):\n connect to screen (screen -r), create a new window (Ctrl-a c), run kinit -r 7d, exit the window (exit) and detach from screen (Ctrl-a d).  $ kinit -r 7d Password for YourNetID@TUDELFT.NET: $ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_private Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/08/01 09:00:00 01/08/01 19:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/15/01 09:00:00 $ exit  Tip Use a repeating reminder (twice a week) in your agenda so you don’t forget.  Important When the end of the renewable life is reached, your tickets expire and your program(s) will return Permission denied errors when trying to access your files. Your program(s) will not be terminated automatically; you still have to terminate the program(s) yourself.  Extra functionality can be provided by the k5start and krenew programs. On most computers these are not available by default but can be installed (ask Robbert).\n","categories":"","description":"How to submit jobs to slurm?\n","excerpt":"How to submit jobs to slurm?\n","ref":"/docs/job_submissions/_index-6/","tags":"","title":"Job submission and management"},{"body":"Resources reservations Slurm gives the possibility to reserve one or more compute nodes exclusively for a specific user or group of users. A reservation ensures that the designated node (or nodes) are dedicated solely to the reservation holder’s tasks and are not shared with other users during the reserved period. This feature allows users to plan the execution of future workloads, and accommodates cluster users with special needs beyond the batch system (eg latency measurement scenarios).\nNote Using reservations is in line with the General cluster usage clauses of DAIC users' agreement. However, please be mindful that reservations are intended to facilitate special needs that cannot be satisfied by the batch system, and should not be requested to guarantee fast throughput for production runs.  Requesting a Reservation To request a reservation for nodes, please use to the Request Reservation form. You can request a reservation for an entire compute node (or a group of nodes) if you have contributed this (or these) nodes to the cluster and you have special needs that needs to be accommodated.\nGeneral guidelines for reservations' requests:\n You can be granted a reservation only on nodes from a partition that is contributed by your group (See DAIC partitions and contributors page to check the name of the partition contributed by your group, and the DAIC hardware page for a listing of available nodes and their features). Please ask for the least amount of resources you need as to minimize impact on other users. Plan ahead and request your reservation as soon as possible: Reservations usually ignore running jobs, so any running job on the machine(s) you request will continue to run when the reservation starts. While jobs from other users will not start on the reserved node(s), the resources in use by an already running job at the start time of the reservation will not be available in the reservation until this running job ends. The earlier ahead you request resources, the easier it is to allocate the requested resources .  Using a reservation Once your reservation request is approved and a reservation is placed on the system, you can run your jobs in the reservation by specifying --qos=reservation along with the following directives to your slurm commands: --reservation=\u003cname\u003e and --partition=\u003cpartition\u003e. For example, to submit the job job.sbatch to a reservation named icra_iv on the cor1 node on the cor partition use:\n$ sbatch --qos=reservation --reservation=icra_iv --partition=cor job.sbatch Alternatively, it is possible to add the following lines to the job.sbatch file, and submitting this file as usual:\n#SBATCH --qos=reservation #SBATCH --reservation=icra_iv #SBATCH --partition=cor  Note It is possible to submit jobs to a reservation once it is created. Jobs will start immediately when the reservation is available, but already running jobs on resources will not be canceled for the reservation to start.  Note When a reservation is used to run your jobs, remember to also pass the reservation parameters to your srun steps:\n$ srun --qos=reservation --reservation=\u003creservation_name\u003e --partition=\u003cpartition_name\u003e \u003csome_script.sh\u003e   Viewing reservations To view all active and future reservations run the scontrol command as follows:\n$ scontrol show reservations ReservationName=icra_iv StartTime=2023-09-09T00:00:00 EndTime=2023-09-16T00:00:00 Duration=7-00:00:00 Nodes=cor1 NodeCnt=1 CoreCnt=32 Features=(null) PartitionName=cor Flags= TRES=cpu=64 Users=(null) Groups=(null) Accounts=3me-cor Licenses=(null) State=ACTIVE BurstBuffer=(null) Watts=n/a MaxStartDelay=(null) ReservationName=maintenance weekend 2023-10-14 StartTime=2023-10-13T20:00:00 EndTime=2023-10-16T09:00:00 Duration=2-13:00:00 Nodes=3dgi[1-2],100plus,awi[01-26],cor1,gpu[01-11],grs[1-4],influ[1-6],insy[11-12,14-16],tbm5,wis1 NodeCnt=58 CoreCnt=2000 Features=(null) PartitionName=(null) Flags=MAINT,IGNORE_JOBS,SPEC_NODES,ALL_NODES TRES=cpu=4000 Users=root Groups=(null) Accounts=(null) Licenses=(null) State=INACTIVE BurstBuffer=(null) Watts=n/a MaxStartDelay=(null)  Note  Jobs can run on a reservation only if explicitly requested as shown in the Requesting a reservation section. Only jobs from the Users or Accounts associated with the reservation (as shown in the scontrol show reservations output) will be run on the reservation STATE of a reservation will show as ACTIVE (instead of INACTIVE) during the reservation window.   Kerberos Authentication Kerberos is an authentication protocol which uses tickets to authenticate users (and computers). You automatically get a ticket when you log in with your password on a TU Delft installed computer. You can use this ticket to authenticate yourself without password when connecting to other computers or accessing your files. To protect you from misuse, the ticket expires after 10 hours or less (even when you’re still logged in).\nFile access Your Linux and Windows home  directories and the Group and Bulk shares are located on network fileservers, which allows you to access your files from all TU Delft installed computers. Kerberos authentication is used to enable access to, or protect, your files. Without a valid Kerberos ticket (e.g. when the ticket has expired) you will not be able to access your files but instead you will receive a Permission denied error.\nLifetime of Kerberos Tickets Kerberos tickets have a limited valid lifetime (of up to 10 hours) to reduce the risk of abuse, even when you stay logged in. If your tickets expire, you will receive a Permission Denied error when you try to access your files and a password prompt when you try to connect to another computer. When you want your program to be able to access your files for longer than the valid ticket lifetime, you’ll have to renew your ticket (repeatedly) until your program is done. Kerberos tickets can be renewed up to a maximum renewable life period of 7 days (again to reduce the risk of abuse).\nThe command klist -5 lists your cached Kerberos tickets together with their expiration time and maximum renewal time:\n$ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_random Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/01/01 00:00:00 01/01/01 10:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/08/01 00:00:00 Where:\n  Ticket cache: The Kerberos tickets that have been issued to you are stored in a ticket cache file. You can have multiple ticket cache files on the same computer (from different connections, for example) with different tickets and ticket expiration times. Some ticket cache files are automatically removed when you logout. Tip Make sure that you renew the tickets in the right ticket cache file (see this screen example).    Default principal: Your identity.\n  Service principal: The identity of services that you have gotten tickets for. You always need a Kerberos ticket-granting ticket (krbtgt) in order to obtain other tickets for specific services like accessing files (nfs) or connecting to computers (host).\n  Valid starting, Expires: Your ticket is only valid between these times (this period is called the valid lifetime). After this time you will not be able to use the service nor automatically renew the ticket (without password).\n  Renew until: Your ticket can only be renewed without password up to this time. After this time you will have to obtain a new ticket using your password.\n  Renewing Kerberos tickets If you have a valid Kerberos krbtgt ticket, you can renew it at any time (until it expires) by running the command kinit -R:\n$ kinit -R $ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_random Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/01/01 01:00:00 01/01/01 11:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/08/01 00:00:00  Note Renewing the ticket will not change the duration of the valid lifetime, i.e. a krbtgt ticket with a valid lifetime of 1 hour will, after renewal, be valid for another hour.  When the krbtgt ticket has expired or reached it’s renew until time, you will have to obtain a new ticket by running kinit -r 7d (note the difference in case for the r) and authenticating with your password:\n$ kinit -r 7d Password for YourNetID@TUDELFT.NET: $ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_random Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/01/01 11:00:00 01/01/01 21:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/08/01 11:00:00 The new ticket will have a valid lifetime of 10 hours and a renewable life of 7 days.\nOn the TU Delft Linux desktops your Kerberos ticket is refreshed (i.e. replaced by a new ticket) automatically every time you enter your password for unlocking the screen saver.\nTip Do not disable the screen saver password lock.  On remote computers you have to manually renew your tickets before they expire.\nSlurm \u0026 Kerberos  Slurm caches your Kerberos ticket, and uses it to execute your job Regularly renew the ticket in Slurm’s cache while your jobs are queued or running:  $ auks -a Auks API request succeed  To automatically renew your ticket in Slurm’s cache until you change your NetID password, run the following on the login1 server:  $ install_keytab Password for somebody@TUDELFT.NET: Installed keytab. You need to rerun this command whenever you change your NetID password (at least every 6 months). Otherwise, the automatic renewal will not work and you will receive a warning e-mail.\nRenewal using screen On the compute servers, the screen program has been modified to allow jobs to run unattended for up to 7 days. It creates a private ticket cache (to prevent the cache from being destroyed at logout) and automatically renews your ticket up to the maximum renewable life. For example, start MATLAB in Screen with screen matlab (the order is important!).\n$ screen matlab Warning: No display specified. You will not be able to display graphics on the screen. \u003c M A T L A B (R) \u003e Copyright 1984-2010 The MathWorks, Inc. Version 7.11.0.584 (R2010b) 64-bit (glnxa64) August 16, 2010 To get started, type one of these: helpwin, helpdesk, or demo. For product information, visit www.mathworks.com. \u003e\u003e For longer jobs you have to manually obtain a new ticket at least every 7 days by running kinit -r 7d from within screen (so you use the specific ticket cache file that screen is using):\n connect to screen (screen -r), create a new window (Ctrl-a c), run kinit -r 7d, exit the window (exit) and detach from screen (Ctrl-a d).  $ kinit -r 7d Password for YourNetID@TUDELFT.NET: $ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_private Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/08/01 09:00:00 01/08/01 19:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/15/01 09:00:00 $ exit  Tip Use a repeating reminder (twice a week) in your agenda so you don’t forget.  Important When the end of the renewable life is reached, your tickets expire and your program(s) will return Permission denied errors when trying to access your files. Your program(s) will not be terminated automatically; you still have to terminate the program(s) yourself.  Extra functionality can be provided by the k5start and krenew programs. On most computers these are not available by default but can be installed (ask Robbert).\n","categories":"","description":"How to submit jobs to slurm?\n","excerpt":"How to submit jobs to slurm?\n","ref":"/docs/job_submissions/_index-7/","tags":"","title":"Job submission and management"},{"body":"Parallelizing jobs with Job Arrays There can be scenarios, eg in simulations or benchmarking, where a job script needs to run many times with only different parameter set each time. If done manually, keeping track of the parameter values and corresponding jobIds is cumbersome. Job Arrays are a convenient mechanism for submitting and managing such jobs.\nA job array is created by adding the --array=\u003cindexes\u003e directive to an sbatch script (or in the command line), where \u003cindexes\u003e can be either a comma separated list of integers, or a range with optional step size, eg, 1-10:2. The minimum index value is 0, and the maximum is a Slurm configuration parameter (MaxArraySize - 1).\nWithin a job array, all jobs have the same SLURM_ARRAY_JOB_ID, but each job will have its own environment variable SLURM_ARRAY_TASK_ID that corresponds to the array index value. Additionally, all jobs in the array inherit the same compute resources requirements. In the following examples, arrays of size 2 are created, but with different indexes:\n$ sbatch --array=1,4 jobscript.sbatch # Indexes specified as a list, and have values 1 and 4 Submitted batch job 8580151 $ $ squeue -u SomeNetID # Replace SomeNetId with your NetID  JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 8580151_1 general jobscrip SomeNetID R 0:01 1 grs4 8580151_4 general jobscrip SomeNetID R 0:01 1 awi18 $ sbatch --array=1-2 jobscript.sbatch # Range specified with default step size = 1. Index have values 1 and 2 Submitted batch job 8580149 $ $ squeue -u SomeNetID # Replace SomeNetId with your NetID  JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 8580149_1 general jobscrip SomeNetID R 0:21 1 grs4 8580149_2 general jobscrip SomeNetID R 0:21 1 awi18  Note To limit the maximum number of simultaneously running jobs in an array use the % separator, eg--array=1-15%3 to run only 3 tasks at a time.  JobId and environment variables As shown in the previous section, Parallelizing jobs with job arrays, jobs within an array are assigned special slurm variables. These variables can be exploited for various computational objectives. Among these, SLURM_ARRAY_TASK_ID is the index of an individual task within the array, and SLURM_ARRAY_JOB_ID is the slurm jobId of the entire array job.\nIn the simplest case, you can use the ${SLURM_ARRAY_TASK_ID} directly in a script to assign parameter values. For example, to run a workflow across a set of images image_1.png … image_5.png, you can simply create an array using the sbatch directive --array=1-5, and then, within your sbatch script, use image_${SLURM_ARRAY_TASK_ID}.png to indicate the corresponding image.\nIn more complex scenarios, eg, when the parameters of interest are not mappable to indexes (of a job array), you can use a config file to map the parameters to the job array indexes. For example, let’s assume the following parameters:\n$ cat jobarray.config i Flower Color Origin 1 Rose Red Worldwide 2 Jasmine White Asia 3 Tulip Various Persia\u0026Turkey 4 Orchid Various Worldwide 5 Lily Various Worldwide Now, you can use these parameters inside a job script as follows:\n$ cat jobarray.sbatch #!/bin/bash #SBATCH --job-name=JobArrayExample #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --array=1-5 # Arry with 5 tasks #SBATCH --output=slurm-%A_%a.out # Set name of output log. %A is SLURM_ARRAY_JOB_ID and %a is SLURM_ARRAY_TASK_ID #SBATCH --error=slurm-%A_%a.err # Set name of error log. %A is SLURM_ARRAY_JOB_ID and %a is SLURM_ARRAY_TASK_ID config=jobarray.config # Path to config file # Obtain parameters from config file: flower=$(awk -v ArrayTaskID=$SLURM_ARRAY_TASK_ID '$1==ArrayTaskID {print $2}' $config) color=$(awk -v ArrayTaskID=$SLURM_ARRAY_TASK_ID '$1==ArrayTaskID {print $3}' $config) origin=$(awk -v ArrayTaskID=$SLURM_ARRAY_TASK_ID '$1==ArrayTaskID {print $4}' $config) # Use the parameters, eg, print the index and parameter values to a file: echo \"Array task: ${SLURM_ARRAY_TASK_ID}, Flower: ${flower}, color: ${color}, origin: ${origin}\" \u003e\u003e output.txt $ $ sbatch jobArray.sbatch Submitted batch job 8580317 $ squeue -u SomeNetID # Replace SomeNetId with your NetID  JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 8580317_[1-5] general JobArray SomeNetID PD 0:00 1 (Priority) In this example, slurm created 5 jobs in a job array, each using the same settings (the name JobArrayExample, the general partition, short QoS, 00:01:00 time, 1 task with 1 CPU and 1G memory, and an output and error file with both array job Id and task id). Each task looks up certain parameter values from a config file leveraging its index via the awk command.\nNote The command:\nflower=$(awk -v ArrayTaskID=$SLURM_ARRAY_TASK_ID '$1==ArrayTaskID {print $2}' $config) assigns a value to the variable flower by reading a configuration file ($config), and printing the value in the second column ({print $2}) where the first column matches the value of the ArrayTaskID variable ($1==ArrayTaskID). The ArrayTaskID is an awk variable set to the value of the SLURM environment variable SLURM_ARRAY_TASK_ID. For more on the awk utility, see this awk tutorial.\n Jobs within a task array are run in parallel, and hence, there’s no guarantee about their order of execution. This is evident looking at the output file from this example:\n$ cat output.txt Array task: 2, Flower: Jasmine, color: White, origin: Asia Array task: 3, Flower: Tulip, color: Various, origin: Persia\u0026Turkey Array task: 1, Flower: Rose, color: Red, origin: Worldwide Array task: 5, Flower: Lily, color: Various, origin: Worldwide Array task: 4, Flower: Orchid, color: Various, origin: Worldwide Other slurm variables that are set inside a job array are shown in the following table, with values based on the preceding example:\n   Slurm Environment Variable Description Value in example     SLURM_ARRAY_JOB_ID The first job ID of the array. 8580317   SLURM_ARRAY_TASK_ID The job array index value. A value in range 1-5   SLURM_ARRAY_TASK_COUNT The number of tasks in the job array. 5   SLURM_ARRAY_TASK_MAX The highest job array index value. 5   SLURM_ARRAY_TASK_MIN The lowest job array index value 1    Slurm commands and job arrays The squeue command reports all submitted jobs. By default, squeue reports all of the tasks associated with a job array in one line and uses a regular expression to indicate the SLURM_ARRAY_TASK_ID values. To explicitly print one job array element per line, use the --array or -r flag. The following examples highlight the difference, using the same jobarray.sbatch file from the JobId and environment variables section:\n$ sbatch jobarray.sbatch Submitted batch job 8593299 $ $ squeue -u SomeNetID # Replace SomeNetId with your NetID  JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 8593299_[1-5] general JobArray SomeNetID PD 0:00 1 (Priority) $ $ squeue -r -u SomeNetID # Replace SomeNetId with your NetID  JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 8593299_1 general JobArray SomeNetID PD 0:00 1 (Priority) 8593299_2 general JobArray SomeNetID PD 0:00 1 (Priority) 8593299_3 general JobArray SomeNetID PD 0:00 1 (Priority) 8593299_4 general JobArray SomeNetID PD 0:00 1 (Priority) 8593299_5 general JobArray SomeNetID PD 0:00 1 (Priority) scancel, on the other hand, can be used to cancel an entire job array by specifying its SLURM_ARRAY_JOB_ID. Alternatively, to cancel a specific task (or tasks), both its SLURM_ARRAY_JOB_ID and SLURM_ARRAY_TASK_ID must be specified, possibly with a regular expression, as shown in the following examples:\n$ sbatch jobarray.sbatch $ squeue -u SomeNetID # Replace SomeNetId with your NetID  JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 8593321_[1-5] general JobArray SomeNetID PD 0:00 1 (Priority) $ $ scancel 8593321_4 # Cancel task with index 4 in the array $ squeue -u SomeNetID # Replace SomeNetId with your NetID  JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 8593321_[1-3,5] general JobArray SomeNetID PD 0:00 1 (Priority) $ $ scancel 8593321_[1-3] # Cancel tasks in index range 1-3 in the array $ squeue -u SomeNetID # Replace SomeNetId with your NetID  JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 8593321_5 general JobArray SomeNetID PD 0:00 1 (Priority) $ $ scancel 8593321 # Cancel all tasks in the array $ squeue -u SomeNetID # Replace SomeNetId with your NetID  JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) $  Note For more information on job arrays, refer to Slurm Job Array Support  Kerberos Authentication Kerberos is an authentication protocol which uses tickets to authenticate users (and computers). You automatically get a ticket when you log in with your password on a TU Delft installed computer. You can use this ticket to authenticate yourself without password when connecting to other computers or accessing your files. To protect you from misuse, the ticket expires after 10 hours or less (even when you’re still logged in).\nFile access Your Linux and Windows home  directories and the Group and Bulk shares are located on network fileservers, which allows you to access your files from all TU Delft installed computers. Kerberos authentication is used to enable access to, or protect, your files. Without a valid Kerberos ticket (e.g. when the ticket has expired) you will not be able to access your files but instead you will receive a Permission denied error.\nLifetime of Kerberos Tickets Kerberos tickets have a limited valid lifetime (of up to 10 hours) to reduce the risk of abuse, even when you stay logged in. If your tickets expire, you will receive a Permission Denied error when you try to access your files and a password prompt when you try to connect to another computer. When you want your program to be able to access your files for longer than the valid ticket lifetime, you’ll have to renew your ticket (repeatedly) until your program is done. Kerberos tickets can be renewed up to a maximum renewable life period of 7 days (again to reduce the risk of abuse).\nThe command klist -5 lists your cached Kerberos tickets together with their expiration time and maximum renewal time:\n$ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_random Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/01/01 00:00:00 01/01/01 10:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/08/01 00:00:00 Where:\n  Ticket cache: The Kerberos tickets that have been issued to you are stored in a ticket cache file. You can have multiple ticket cache files on the same computer (from different connections, for example) with different tickets and ticket expiration times. Some ticket cache files are automatically removed when you logout. Tip Make sure that you renew the tickets in the right ticket cache file (see this screen example).    Default principal: Your identity.\n  Service principal: The identity of services that you have gotten tickets for. You always need a Kerberos ticket-granting ticket (krbtgt) in order to obtain other tickets for specific services like accessing files (nfs) or connecting to computers (host).\n  Valid starting, Expires: Your ticket is only valid between these times (this period is called the valid lifetime). After this time you will not be able to use the service nor automatically renew the ticket (without password).\n  Renew until: Your ticket can only be renewed without password up to this time. After this time you will have to obtain a new ticket using your password.\n  Renewing Kerberos tickets If you have a valid Kerberos krbtgt ticket, you can renew it at any time (until it expires) by running the command kinit -R:\n$ kinit -R $ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_random Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/01/01 01:00:00 01/01/01 11:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/08/01 00:00:00  Note Renewing the ticket will not change the duration of the valid lifetime, i.e. a krbtgt ticket with a valid lifetime of 1 hour will, after renewal, be valid for another hour.  When the krbtgt ticket has expired or reached it’s renew until time, you will have to obtain a new ticket by running kinit -r 7d (note the difference in case for the r) and authenticating with your password:\n$ kinit -r 7d Password for YourNetID@TUDELFT.NET: $ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_random Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/01/01 11:00:00 01/01/01 21:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/08/01 11:00:00 The new ticket will have a valid lifetime of 10 hours and a renewable life of 7 days.\nOn the TU Delft Linux desktops your Kerberos ticket is refreshed (i.e. replaced by a new ticket) automatically every time you enter your password for unlocking the screen saver.\nTip Do not disable the screen saver password lock.  On remote computers you have to manually renew your tickets before they expire.\nSlurm \u0026 Kerberos  Slurm caches your Kerberos ticket, and uses it to execute your job Regularly renew the ticket in Slurm’s cache while your jobs are queued or running:  $ auks -a Auks API request succeed  To automatically renew your ticket in Slurm’s cache until you change your NetID password, run the following on the login1 server:  $ install_keytab Password for somebody@TUDELFT.NET: Installed keytab. You need to rerun this command whenever you change your NetID password (at least every 6 months). Otherwise, the automatic renewal will not work and you will receive a warning e-mail.\nRenewal using screen On the compute servers, the screen program has been modified to allow jobs to run unattended for up to 7 days. It creates a private ticket cache (to prevent the cache from being destroyed at logout) and automatically renews your ticket up to the maximum renewable life. For example, start MATLAB in Screen with screen matlab (the order is important!).\n$ screen matlab Warning: No display specified. You will not be able to display graphics on the screen. \u003c M A T L A B (R) \u003e Copyright 1984-2010 The MathWorks, Inc. Version 7.11.0.584 (R2010b) 64-bit (glnxa64) August 16, 2010 To get started, type one of these: helpwin, helpdesk, or demo. For product information, visit www.mathworks.com. \u003e\u003e For longer jobs you have to manually obtain a new ticket at least every 7 days by running kinit -r 7d from within screen (so you use the specific ticket cache file that screen is using):\n connect to screen (screen -r), create a new window (Ctrl-a c), run kinit -r 7d, exit the window (exit) and detach from screen (Ctrl-a d).  $ kinit -r 7d Password for YourNetID@TUDELFT.NET: $ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_private Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/08/01 09:00:00 01/08/01 19:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/15/01 09:00:00 $ exit  Tip Use a repeating reminder (twice a week) in your agenda so you don’t forget.  Important When the end of the renewable life is reached, your tickets expire and your program(s) will return Permission denied errors when trying to access your files. Your program(s) will not be terminated automatically; you still have to terminate the program(s) yourself.  Extra functionality can be provided by the k5start and krenew programs. On most computers these are not available by default but can be installed (ask Robbert).\n","categories":"","description":"How to submit jobs to slurm?\n","excerpt":"How to submit jobs to slurm?\n","ref":"/docs/job_submissions/_index-8/","tags":"","title":"Job submission and management"},{"body":"Troubleshooting Common Issues Please see the Frequently asked questions on Scheduler problems  and Job resources\nKerberos Authentication Kerberos is an authentication protocol which uses tickets to authenticate users (and computers). You automatically get a ticket when you log in with your password on a TU Delft installed computer. You can use this ticket to authenticate yourself without password when connecting to other computers or accessing your files. To protect you from misuse, the ticket expires after 10 hours or less (even when you’re still logged in).\nFile access Your Linux and Windows home  directories and the Group and Bulk shares are located on network fileservers, which allows you to access your files from all TU Delft installed computers. Kerberos authentication is used to enable access to, or protect, your files. Without a valid Kerberos ticket (e.g. when the ticket has expired) you will not be able to access your files but instead you will receive a Permission denied error.\nLifetime of Kerberos Tickets Kerberos tickets have a limited valid lifetime (of up to 10 hours) to reduce the risk of abuse, even when you stay logged in. If your tickets expire, you will receive a Permission Denied error when you try to access your files and a password prompt when you try to connect to another computer. When you want your program to be able to access your files for longer than the valid ticket lifetime, you’ll have to renew your ticket (repeatedly) until your program is done. Kerberos tickets can be renewed up to a maximum renewable life period of 7 days (again to reduce the risk of abuse).\nThe command klist -5 lists your cached Kerberos tickets together with their expiration time and maximum renewal time:\n$ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_random Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/01/01 00:00:00 01/01/01 10:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/08/01 00:00:00 Where:\n  Ticket cache: The Kerberos tickets that have been issued to you are stored in a ticket cache file. You can have multiple ticket cache files on the same computer (from different connections, for example) with different tickets and ticket expiration times. Some ticket cache files are automatically removed when you logout. Tip Make sure that you renew the tickets in the right ticket cache file (see this screen example).    Default principal: Your identity.\n  Service principal: The identity of services that you have gotten tickets for. You always need a Kerberos ticket-granting ticket (krbtgt) in order to obtain other tickets for specific services like accessing files (nfs) or connecting to computers (host).\n  Valid starting, Expires: Your ticket is only valid between these times (this period is called the valid lifetime). After this time you will not be able to use the service nor automatically renew the ticket (without password).\n  Renew until: Your ticket can only be renewed without password up to this time. After this time you will have to obtain a new ticket using your password.\n  Renewing Kerberos tickets If you have a valid Kerberos krbtgt ticket, you can renew it at any time (until it expires) by running the command kinit -R:\n$ kinit -R $ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_random Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/01/01 01:00:00 01/01/01 11:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/08/01 00:00:00  Note Renewing the ticket will not change the duration of the valid lifetime, i.e. a krbtgt ticket with a valid lifetime of 1 hour will, after renewal, be valid for another hour.  When the krbtgt ticket has expired or reached it’s renew until time, you will have to obtain a new ticket by running kinit -r 7d (note the difference in case for the r) and authenticating with your password:\n$ kinit -r 7d Password for YourNetID@TUDELFT.NET: $ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_random Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/01/01 11:00:00 01/01/01 21:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/08/01 11:00:00 The new ticket will have a valid lifetime of 10 hours and a renewable life of 7 days.\nOn the TU Delft Linux desktops your Kerberos ticket is refreshed (i.e. replaced by a new ticket) automatically every time you enter your password for unlocking the screen saver.\nTip Do not disable the screen saver password lock.  On remote computers you have to manually renew your tickets before they expire.\nSlurm \u0026 Kerberos  Slurm caches your Kerberos ticket, and uses it to execute your job Regularly renew the ticket in Slurm’s cache while your jobs are queued or running:  $ auks -a Auks API request succeed  To automatically renew your ticket in Slurm’s cache until you change your NetID password, run the following on the login1 server:  $ install_keytab Password for somebody@TUDELFT.NET: Installed keytab. You need to rerun this command whenever you change your NetID password (at least every 6 months). Otherwise, the automatic renewal will not work and you will receive a warning e-mail.\nRenewal using screen On the compute servers, the screen program has been modified to allow jobs to run unattended for up to 7 days. It creates a private ticket cache (to prevent the cache from being destroyed at logout) and automatically renews your ticket up to the maximum renewable life. For example, start MATLAB in Screen with screen matlab (the order is important!).\n$ screen matlab Warning: No display specified. You will not be able to display graphics on the screen. \u003c M A T L A B (R) \u003e Copyright 1984-2010 The MathWorks, Inc. Version 7.11.0.584 (R2010b) 64-bit (glnxa64) August 16, 2010 To get started, type one of these: helpwin, helpdesk, or demo. For product information, visit www.mathworks.com. \u003e\u003e For longer jobs you have to manually obtain a new ticket at least every 7 days by running kinit -r 7d from within screen (so you use the specific ticket cache file that screen is using):\n connect to screen (screen -r), create a new window (Ctrl-a c), run kinit -r 7d, exit the window (exit) and detach from screen (Ctrl-a d).  $ kinit -r 7d Password for YourNetID@TUDELFT.NET: $ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_private Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/08/01 09:00:00 01/08/01 19:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/15/01 09:00:00 $ exit  Tip Use a repeating reminder (twice a week) in your agenda so you don’t forget.  Important When the end of the renewable life is reached, your tickets expire and your program(s) will return Permission denied errors when trying to access your files. Your program(s) will not be terminated automatically; you still have to terminate the program(s) yourself.  Extra functionality can be provided by the k5start and krenew programs. On most computers these are not available by default but can be installed (ask Robbert).\n","categories":"","description":"How to submit jobs to slurm?\n","excerpt":"How to submit jobs to slurm?\n","ref":"/docs/job_submissions/_index-9/","tags":"","title":"Job submission and management"},{"body":"Batch Queuing System Overview DAIC uses Slurm   as a cluster management and job scheduling system to efficiently manage computational workloads across computing capacity.\nA slurm-based cluster is composed of a set of login nodes that are used to access the cluster and submit computational jobs. A central manager orchestrates computational demands across a set of compute nodes. These nodes are organized logically into groups called partitions, that defines job limits or access rights. The central manager provides fault-tolerant hierarchical communications, to ensure optimal and fair use of available compute resources to eligible users, and make it easier to run and schedule complex jobs across compute resources (multiple nodes).\n  DAIC partitions and access/usage best practices\n  Partitions and Quality of Service When you submit a job in a slurm-based system, it enters a queue waiting for resources. The partition and Quality of Service(QoS) are the two job parameters slurm uses to assign resources for a job:\n The partition is a set of compute nodes on which a job can be scheduled. In DAIC, the nodes contributed or funded by a certain group are lumped into a corresponding partition (see Brief history of DAIC). All nodes in DAIC are part of the general partition, but other partitions exist for prioritization purposes on select nodes (see Priority tiers). The Quality of Service is a set of limits that controls what resources a job can use and, therefore, determines the priority level of a job. This includes the run time, CPU, GPU and memory limits on the given partition. Jobs that exceed these limits are automatically terminated (see QoS priority).  For DAIC, Table 1 shows the QoS limits on the general partition.\n  Table 1: The general partition and its operational and per-QoS per-user limits; specific groups use other partitions and QoS  *infinite QoS jobs will be killed when servers go down, eg, during maintenance. It is not recommended to submit jobs with this QoS.    Partition QoS Priority Max run time Jobs per user CPU limits GPU limits Memory limits   Per QoS Per user Per QoS Per user Per QoS Per User     general interactive high 1 hour 1 running - 2 - 2 - 16G   short normal 4 hours 10000 3672 (85%) 2160 (50%) 109 (85%) 64 (50%) 23159G (85%) 13623G (50%)   medium medium 1 ½ day 2000 3456 (80%) 1512 (35%) 103 (80%) 45 (35%) 21796G (80%) 9536G (35%)   long low 7 days 1000 3240 (75%) 864 (20%) 96 (75%) 25 (20%) 20434G (75%) 5449G (20%)   infinite* none infinite 1 running 32 - 2 - 250G -     Note The priority of a job is a function of both QoS and previous usage (less is better). See Job prioritization and waiting times   Slurm job’s terminology: job, job step, task and CPUs A slurm job (submitted via sbatch) can consists of multiple steps in series. Each step (specified via srun) can run multiple tasks (ie programs) in parallel. Each task gets its own set of CPUs. As an example, consider the workflow and corresponding breakdown shown in fig 2.\n  Slurm job’s terminology\n  In this example, note:\n When you explicitly request 1 CPU per task (--cpus-per-task=1), you should also explicitly specify the number of tasks (--ntasks). Otherwise, srun may start the task twice in parallel (because CPUs are allocated in multiples of 2) The default slurm allocation is a single task and single CPU (ie --ntasks=1 --cpus-per-task=1). Thus, it is not necessary to explicitly request these to run a single task on a single CPU. When using multiple tasks, specify --mem-per-cpu.  Note DAIC is dual-threaded. It means that CPUs are automatically allocated in multiples of 2. Thus, in your job use (a multiple of) 2 threads.  Job Scripts Job scripts are text files, where the header set of directives that specify compute resources, and the remainder is the code that needs to run. All resources and scheduling are specified in the header as #SBATCH directives (see man sbatch for more information). Code could be a set of steps to run in series, or parallel tasks within these steps (see Slurm job’s terminology).\nThe code snippet below is a template script that can be customized to run jobs on DAIC. A useful tool that can be used to streamline the debugging of such scripts is ShellCheck   .\n#!/bin/sh #SBATCH --partition=general # Request partition. Default is 'general'  #SBATCH --qos=short # Request Quality of Service. Default is 'short' (maximum run time: 4 hours) #SBATCH --time=0:01:00 # Request run time (wall-clock). Default is 1 minute #SBATCH --ntasks=1 # Request number of parallel tasks per job. Default is 1 #SBATCH --cpus-per-task=2 # Request number of CPUs (threads) per task. Default is 1 (note: CPUs are always allocated to jobs per 2). #SBATCH --mem=1024 # Request memory (MB) per node. Default is 1024MB (1GB). For multiple tasks, specify --mem-per-cpu instead #SBATCH --mail-type=END # Set mail type to 'END' to receive a mail when the job finishes.  #SBATCH --output=slurm_%j.out # Set name of output log. %j is the Slurm jobId #SBATCH --error=slurm_%j.err # Set name of error log. %j is the Slurm jobId /usr/bin/scontrol show job -d \"$SLURM_JOB_ID\" # check sbatch directives are working # Remaining job commands go below here. For example, to run a Matlab script named \"matlab_script.m\", uncomment: #module use /opt/insy/modulefiles # Use DAIC INSY software collection #module load matlab/R2020b # Load Matlab 2020b version #srun matlab \u003c matlab_script.m # Computations should be started with 'srun'.  Note  DAIC is dual-threaded. It means that CPUs are automatically allocated in multiples of 2. Thus, in your job use (a multiple of) 2 threads. Do not enable mails when submitting large numbers (\u003e20) of jobs at once   Job submission and monitoring Once a job script (see Job script) is ready, it is time to send it to the cluster and start computing (see Data transfer methods).\nHint To check whether you are working on your machine or the cluster, observe the output of the hostname command. The following examples show various outputs depending on where the command was ran from:\n$ hostname login1.hpc.tudelft.nl # You are in a DAIC login1 node $ $ hostname grs1.hpc.tudelft.nl # You are in the DAIC compute node named grs1 $ $ hostname \u003cYourNetID\u003e # You are in your TU Delft laptop/PC   To submit a job script jobscript.sbatch, login to DAIC, and:\n To only test:  $ sbatch --test-only jobscript.sbatch Job 1 to start at 2015-06-30T14:00:00 using 2 processors on nodes insy15 in partition general  To actually submit the job and do the computations:  $ sbatch jobscript.sbatch Submitted batch job 2  To check your job has actually been submitted:  $ squeue -u SomeNetID # Replace SomeNetId with your NetID  JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 2 general jobscip SomeNetI R 0:01 1 insy15  And to check the log of your job, use an editor or viewer of choice (eg, vi, nano or simply cat) to view the log:  $ cat slurm-2.out JobId=2 JobName=jobscript.sbatch UserId=SomeNetId(123) GroupId=domain users(100513) MCS_label=N/A Priority=23909774 Nice=0 Account=ewi-insy QOS=short JobState=RUNNING Reason=None Dependency=(null) Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0 DerivedExitCode=0:0 RunTime=00:00:00 TimeLimit=00:01:00 TimeMin=N/A SubmitTime=2015-06-30T14:00:00 EligibleTime=2015-06-30T14:00:00 AccrueTime=2015-06-30T14:00:00 StartTime=2015-06-30T14:00:01 EndTime=2015-06-30T14:01:01 Deadline=N/A SuspendTime=None SecsPreSuspend=0 LastSchedEval=2015-06-30T14:01:01 Scheduler=Main Partition=general AllocNode:Sid=login1:2220 ReqNodeList=(null) ExcNodeList=(null) NodeList=insy15 BatchHost=insy15 NumNodes=1 NumCPUs=2 NumTasks=1 CPUs/Task=2 ReqB:S:C:T=0:0:*:* TRES=cpu=2,mem=1G,node=1,billing=1 Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=* JOB_GRES=(null) Nodes=insy15 CPU_IDs=26-27 Mem=1024 GRES= MinCPUsNode=2 MinMemoryNode=1G MinTmpDiskNode=50M Features=(null) DelayBoot=00:00:00 OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null) Command=/home/nfs/SomeNetId/jobscript.sbatch WorkDir=/home/nfs/SomeNetId StdErr=/home/nfs/SomeNetId/slurm_2.err StdIn=/dev/null StdOut=/home/nfs/SomeNetId/slurm_2.out Power= MailUser=SomeNetId@tudelft.nl MailType=END  And finally, to cancel a given job:  $ scancel \u003cjobID\u003e  Note It is possible to specify the sbatch directives, like --mem, --ntasks, … etc in the command line as in:\n$ sbatch --time=00:02:00 jobscript.sbatch This specification is generally not recommended for production, as it is less reproducible than specifying within the job script itself.\n Interactive jobs on compute nodes To work interactively on a node, eg, to debug a running code, or test on a GPU, start an interactive session using sinteractve \u003ccompute requirements\u003e. If no parameters were provided, the default are applied. \u003ccompute requirement\u003e can be specified the same way as sbatch directives within an sbatch script (see Job scripts), as in the examples below:\n$ hostname # check you are in one of the login nodes login1.hpc.tudelft.nl $ sinteractive 16:07:20 up 12 days, 4:09, 2 users, load average: 7.06, 7.04, 7.12 $ hostname # check you are in a compute node insy15 $ squeue -u SomeNetID # Replace SomeNetId with your NetID  JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 2 general bash SomeNetI R 1:23 1 insy15 $ logout # exit the interactive job To request a node with certain compute requirements:\n$ sinteractive --ntasks=1 --cpus-per-task=2 --mem=4096 16:07:20 up 12 days, 4:09, 2 users, load average: 7.06, 7.04, 7.12  Warning When you logout from an interactive session, all running processes will be terminated  Note Requesting interactive sessions is subject to the same resource availability constraints as submitting an sbatch script. It means you may need to wait until resources are available as you would when you submit an sbatch script  Jobs on GPU resources Some DAIC nodes have GPUs of different types, that can be used for various compute purposes (see DAIC GPUs).\nTo request a gpu for a job, use the sbatch directive --gres=gpu[:type][:number], where the optional [:type] and [:number] specify the type and number of the GPUs requested, as in the examples below:   Slurm directives to request gpus for a job\n  Note For CUDA programs, first, load the needed modules (CUDA, cuDNN) before running your code. See Environment modules  An example batch script with GPU resources\n#!/bin/sh #SBATCH --partition=general # Request partition. Default is 'general'  #SBATCH --qos=short # Request Quality of Service. Default is 'short' (maximum run time: 4 hours) #SBATCH --time=0:01:00 # Request run time (wall-clock). Default is 1 minute #SBATCH --ntasks=1 # Request number of parallel tasks per job. Default is 1 #SBATCH --cpus-per-task=2 # Request number of CPUs (threads) per task. Default is 1 (note: CPUs are always allocated to jobs per 2). #SBATCH --mem=1024 # Request memory (MB) per node. Default is 1024MB (1GB). For multiple tasks, specify --mem-per-cpu instead #SBATCH --mail-type=END # Set mail type to 'END' to receive a mail when the job finishes.  #SBATCH --output=slurm_%j.out # Set name of output log. %j is the Slurm jobId #SBATCH --error=slurm_%j.err # Set name of error log. %j is the Slurm jobId #SBATCH --gres=gpu:1 # Request 1 GPU # Measure GPU usage of your job (initialization) previous=$(/usr/bin/nvidia-smi --query-accounted-apps='gpu_utilization,mem_utilization,max_memory_usage,time' --format='csv' | /usr/bin/tail -n '+2') /usr/bin/nvidia-smi # Check sbatch settings are working (it should show the GPU that you requested) # Remaining job commands go below here. For example, to run python code that makes use of GPU resources: # Uncomment these lines and adapt them to load the software that your job requires #module use /opt/insy/modulefiles # Use DAIC INSY software collection #module load cuda/11.2 cudnn/11.2-8.1.1.33 # Load certain versions of cuda and cudnn  #srun python my_program.py # Computations should be started with 'srun'. For example: # Measure GPU usage of your job (result) /usr/bin/nvidia-smi --query-accounted-apps='gpu_utilization,mem_utilization,max_memory_usage,time' --format='csv' | /usr/bin/grep -v -F \"$previous\" Similarly, to interactively work in a GPU node:\n$ hostname # check you are in one of the login nodes login1.hpc.tudelft.nl $ $ sinteractive --cpus-per-task=1 --mem=500 --time=00:01:00 --gres=gpu:v100:1 Note: interactive sessions are automatically terminated when they reach their time limit (1 hour)! srun: job 8607665 queued and waiting for resources srun: job 8607665 has been allocated resources 15:27:18 up 51 days, 3:04, 0 users, load average: 62,09, 59,43, 44,04 SomeNetID@insy11:~$ SomeNetID@insy11:~$ hostname # check you are in one of the compute nodes insy11.hpc.tudelft.nl SomeNetID@insy11:~$ SomeNetID@insy11:~$ nvidia-smi # check characteristics of GPU Mon Jul 24 15:37:01 2023 +---------------------------------------------------------------------------------------+ | NVIDIA-SMI 530.30.02 Driver Version: 530.30.02 CUDA Version: 12.1 | |-----------------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+======================+======================| | 0 Tesla V100-SXM2-32GB On | 00000000:88:00.0 Off | 0 | | N/A 32C P0 40W / 300W| 0MiB / 32768MiB | 0% Default | | | | N/A | +-----------------------------------------+----------------------+----------------------+ +---------------------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=======================================================================================| | No running processes found | +---------------------------------------------------------------------------------------+ SomeNetID@insy11:~$ SomeNetID@insy11:~$ exit # exit the interactive session Deploying dependent jobs (job chains) In certain scenarios, it might be desirable to condition the execution of a certain job on the status of another job. In such cases, the sbatch directive --dependency=\u003ccondition\u003e:\u003cjobID\u003e can be used, where \u003ccondition\u003e specifies the type of dependency (See table 2), and \u003cjobID\u003e is the slurm jobID upon which dependency is based. To specify more than one dependency, the , separator is used to indicate that all dependencies must be specified, and, ? is used denotes that any dependency may be satisfied.\nFor example, assume the slurm job scripts, job_1.sbatch, … job_3.sbatch need to run sequentially one after the other. To start this chain, submit the first job and obtain its jobID:\n$ sbatch job_1.sbatch Submitted batch job 8580135 Next, submit the second job to run only if the first job is successful:\n$ sbatch --dependency=afterok:8580135 job_2.sbatch Submitted batch job 8580136  Note Note that if the first job (with jobID 8580135 in the example) fails, the second job (with jobID 8580136) will not run, but it will remain in the queue. You have to use scancel 8580136 to cancel this job  And, now, to run the third job only after the first two jobs have both run successfully:\n$ sbatch --dependency=afterok:8580135,8580136 job_3.sbatch Submitted batch job 8580140 Alternatively, if the third job is dependent on either job running successfully:\n$ sbatch --dependency=afterok:8580135?8580136 job_3.sbatch Submitted batch job 8580141  Warning  If the jobs within a chain involve copying data files to a local disk (/tmp) on a node, you need to make sure all jobs use the same node (--nodelist=\u003cnode\u003e, for example --nodelist=insy15)     Table 2: Possible sbatch dependency conditions    Argument Description     after This job can begin execution after the specified jobs have begun execution   afterany This job can begin execution after the specified jobs have terminated.   aftercorr A task of this job array can begin execution after the corresponding task ID in the specified job has completed successfully   afternotok This job can begin execution after the specified jobs have terminated in some failed state   afterok This job can begin execution after the specified jobs have successfully executed   singleton This job can begin execution after any previously launched jobs sharing the same job name and user have terminated    Checking slurm jobs Sometimes, it may be desirable to inspect slurm jobs beyond their status in the queue. For example, to check which script was submitted, or how the resources were requested and allocated. Below are a few useful commands for this purpose:\n See job definition  $ scontrol show job 8580148 JobId=8580148 JobName=jobscript.sbatch UserId=SomeNetID(123) GroupId=domain users(100513) MCS_label=N/A Priority=23721804 Nice=0 Account=ewi-insy QOS=short JobState=RUNNING Reason=None Dependency=(null) Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0 RunTime=00:00:12 TimeLimit=00:01:00 TimeMin=N/A SubmitTime=2023-07-10T06:41:57 EligibleTime=2023-07-10T06:41:57 AccrueTime=2023-07-10T06:41:57 StartTime=2023-07-10T06:41:58 EndTime=2023-07-10T06:42:58 Deadline=N/A SuspendTime=None SecsPreSuspend=0 LastSchedEval=2023-07-10T06:41:58 Scheduler=Main Partition=general AllocNode:Sid=login1:19162 ReqNodeList=(null) ExcNodeList=(null) NodeList=awi18 BatchHost=awi18 NumNodes=1 NumCPUs=2 NumTasks=1 CPUs/Task=2 ReqB:S:C:T=0:0:*:* TRES=cpu=2,mem=1G,node=1,billing=1 Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=* MinCPUsNode=2 MinMemoryNode=1G MinTmpDiskNode=50M Features=(null) DelayBoot=00:00:00 OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null) Command=/home/nfs/SomeNetID/jobscript.sbatch WorkDir=/home/nfs/SomeNetID StdErr=/home/nfs/SomeNetID/slurm_8580148.err StdIn=/dev/null StdOut=/home/nfs/SomeNetID/slurm_8580148.out Power= MailUser=SomeNetId@tudelft.nl MailType=END  See statistics of a running job  $ sstat 1 JobID AveRSS AveCPU NTasks AveDiskRead AveDiskWrite ------- ------- ------- ------- ------------ ------------ 1.0 426K 00:00.0 1 0.52M 0.01M  See accounting information of a finished job (also see –long option)  $ sacct -j 8580148 JobID JobName Partition Account AllocCPUS State ExitCode ------------ ---------- ---------- ---------- ---------- ---------- -------- 8580148 jobscript+ general ewi-insy 2 COMPLETED 0:0 8580148.bat+ batch ewi-insy 2 COMPLETED 0:0 See overall job efficiency of a finished job\n$ seff 8580148 Job ID: 8580148 Cluster: insy User/Group: SomeNetID/domain users State: COMPLETED (exit code 0) Nodes: 1 Cores per node: 2 CPU Utilized: 00:00:00 CPU Efficiency: 0.00% of 00:01:00 core-walltime Job Wall-clock time: 00:00:30 Memory Utilized: 340.00 KB Memory Efficiency: 0.03% of 1.00 GB  See partition definitions  $ scontrol show partition PartitionName=general AllowGroups=ALL AllowAccounts=ALL DenyQos=influence AllocNodes=login[1-3],oodtest Default=YES QoS=N/A DefaultTime=00:01:00 DisableRootJobs=NO ExclusiveUser=NO GraceTime=0 Hidden=NO MaxNodes=UNLIMITED MaxTime=UNLIMITED MinNodes=0 LLN=NO MaxCPUsPerNode=UNLIMITED Nodes=3dgi[1-2],100plus,awi[01-26],cor1,gpu[01-11],grs[1-4],influ[1-6],insy[11-16],tbm5,wis1 PriorityJobFactor=1 PriorityTier=1 RootOnly=NO ReqResv=NO OverSubscribe=NO OverTimeLimit=NONE PreemptMode=OFF State=UP TotalCPUs=4064 TotalNodes=59 SelectTypeParameters=NONE JobDefaults=(null) DefMemPerNode=1024 MaxMemPerNode=UNLIMITED TRESBillingWeights=CPU=0.5,Mem=0.083333333G,GRES/gpu=16.0  See Quality of Service definitions  $ sacctmgr list qos Name Priority GraceTime Preempt PreemptExemptTime PreemptMode Flags UsageThres UsageFactor GrpTRES GrpTRESMins GrpTRESRunMin GrpJobs GrpSubmit GrpWall MaxTRES MaxTRESPerNode MaxTRESMins MaxWall MaxTRESPU MaxJobsPU MaxSubmitPU MaxTRESPA MaxJobsPA MaxSubmitPA MinTRES ---------- ---------- ---------- ---------- ------------------- ----------- ---------------------------------------- ---------- ----------- ------------- ------------- ------------- ------- --------- ----------- ------------- -------------- ------------- ----------- ------------- --------- ----------- ------------- --------- ----------- ------------- normal 0 00:00:00 cluster DenyOnLimit 1.000000 cpu=1 short 50 00:00:00 cluster DenyOnLimit 1.000000 cpu=3562,gre+ 65536 04:00:00 cpu=2096,gre+ 10000 cpu=1,mem=1M long 25 00:00:00 cluster DenyOnLimit 1.000000 cpu=3144,gre+ 65536 7-00:00:00 cpu=838,gres+ 1000 cpu=1,mem=1M infinite 0 00:00:00 cluster DenyOnLimit 1.000000 cpu=32,gres/+ 65536 1 100 cpu=1,mem=1M interacti+ 100 00:00:00 cluster DenyOnLimit 2.000000 65536 01:00:00 cpu=2,gres/g+ 1 1 cpu=1,mem=1M student 10 00:00:00 cluster DenyOnLimit 1.000000 cpu=192,gres+ 65536 04:00:00 cpu=2,gres/g+ 1 100 cpu=1,mem=1M reservati+ 100 00:00:00 cluster DenyOnLimit,RequiresReservation 1.000000 65536 10000 cpu=1,mem=1M influence 100 00:00:00 cluster DenyOnLimit 1.000000 65536 10000 cpu=1,mem=1M guest-sho+ 10 00:00:00 cluster DenyOnLimit 1.000000 cpu=200,gres+ 65536 04:00:00 cpu=128,gres+ 100 cpu=1,mem=1M guest-long 0 00:00:00 cluster DenyOnLimit 1.000000 cpu=200,gres+ 65536 7-00:00:00 cpu=128,gres+ 1 10 cpu=1,mem=1M medium 35 00:00:00 cluster DenyOnLimit 1.000000 cpu=3352,gre+ 65536 1-12:00:00 cpu=1466,gre+ 2000 cpu=1,mem=1M Slurm’s job scheduling and waiting times When slurm is not configured for FIFO scheduling, jobs are prioritized in the following order:\n Jobs that can preempt: Not enabled in DAIC Jobs with an advanced reservation: See Slurm's Advanced Resource Reservation Guide    Partition PriorityTier: See Priority tiers Job priority: See Priority calculations and QoS priority Job ID  Priority tiers DAIC partitions are tiered: the general partition is in the lowest priority tier, department partitions (eg, insy, st) are in the middle priority tier, and partitions for specific groups (eg, visionlab, wis) are in the highest priority tier. Those partitions correspond to resources contributed by the respective groups or departments (see Brief history of DAIC).\nWhen resources become available, the scheduler will first look for jobs in the highest priority partition that those resources are in, and start the highest (user) priority jobs that fit within the resources (if any). When resources remain, the scheduler will check the next lower priority tier, and so on. Finally, the scheduler will try to backfill lower (user) priority jobs that fit (if any).\nThe partition priorities have no impact on resources that are in use, so jobs have to wait until the resources become available.\nWhere to submit jobs? The idea behind the tiering is that you submit to all partitions, e.g. --partition=wis,st,general, and let the scheduler figure out where the job can start the soonest. This should give the job the highest possible priority on the different partitions (resources) in the cluster, at no cost for yourself or others.\nResources of all partitions (eg, st) are also part of the general partition (see Fig 1). Thus:\n submitting to the general partition allows jobs to use all nodes submitting to group-specific partitions alone results in longer waiting times, since the general partition has much more resources than any of them (The bigger the resource pool, the more chances a job has to be scheduled or back-filled) The optimal way is to submit to both general and group-specific partitions when accessible. This is to skip over higher-priority jobs that would otherwise get started first on resources that are also in the specific partition.  Priority calculations Slurm continually calculates job priorities and schedules the execution of jobs based on its configurations. A few configuration parameters affect priority computations:\n SchedulerType: The type of scheduling used based on available resources, requested resources, and job priorities. On DAIC, slurm is used with backfill scheduling mechanism. This mechanism allows low priority jobs to backfill idle resources if doing so does not delay the expected start time of any high priority job (based on resource availability).  Tip With sched/backfil, jobs can only be started when the resources that they request fit within the available idle resources. Thus:\n The fewer resources a job request, the higher the chance that it will fit within the available idle resources. The more resources a job request, the long it will have to wait before enough resources become available to start. To check how the cluster is configured, you may run:  $ scontrol show config | grep SchedulerType SchedulerType = sched/backfil More details is available in Slurm’s SchedulerType\n  PriorityType: The way priority is computed. On DAIC, a multifactor computation is applied, where job priority at any given time is a weighted sum of the following factors:  Fairshare: a measure of the amount of resources that a group (ie account in slurm terminology) has contributed, and the historical usage of the group and the user. QOS: the quality of service associated with the job, which is specified with the slurm --qos directive (see QoS priority).    Info The whole idea behind the FairShare scheduling in DAIC is to share all the available resources fairly and efficiently with all users (instead of having strict limitations in the amount of resource use or in which hardware users can compute). The resources in the cluster are contributed in different amounts by different groups (see Brief history of DAIC), and the scheduler makes sure that each group can use a share of the resource relative to what the group contributed. To check how the cluster is configured you may run:\n$ scontrol show config | grep PriorityType PriorityType = priority/multifactor $ sprio --weights JOBID PARTITION PRIORITY SITE FAIRSHARE QOS Weights 1 20000000 40000000   The following commands are useful for checking prioritization of your own jobs:\n   Command Purpose     sprio -j \u003cYourJobID\u003e Determine the priority of your job   squeue -j \u003cYourJobID\u003e --start Request your job’s estimated start time   sshare -u \u003cYourNetID\u003e Determine your current fairshare value    Info To get more complete priority configurations of a cluster, run the command:\n$ scontrol show config | grep ^Priority PriorityParameters = (null) PrioritySiteFactorParameters = (null) PrioritySiteFactorPlugin = (null) PriorityDecayHalfLife = 2-00:00:00 PriorityCalcPeriod = 00:05:00 PriorityFavorSmall = No PriorityFlags = PriorityMaxAge = 7-00:00:00 PriorityUsageResetPeriod = NONE PriorityType = priority/multifactor PriorityWeightAge = 0 PriorityWeightAssoc = 0 PriorityWeightFairShare = 20000000 PriorityWeightJobSize = 0 PriorityWeightPartition = 0 PriorityWeightQOS = 40000000 PriorityWeightTRES = (null)   QoS priority The purpose of the (multiple) QoSs in DAIC is to optimize the throughput of the cluster and to reduce the waiting times for jobs:\n Long jobs block resources for a long time, thus leading to long waiting times and fragmentation of resources. Short jobs block resources only for short times, and can more easily fill in the gaps in the scheduling of resources (thus start sooner), and are therefore better for throughput and waiting times.  Thus, DAIC has the following policy:\n  To stimulate short jobs, the short QoS has a higher priority, and allows you to use a larger part of all resources, than the medium and long QoS.\n  To prevent long jobs from blocking all resources in the cluster for long times (thus causing long waiting times), only a certain part of all cluster resources is available to all running long QoS jobs (of all users) combined.\n  All running medium QoS jobs together can use a somewhat larger part of all resources in the cluster, and all running short QoS jobs combined are allowed to fill the biggest part of the cluster.\n These limits are called the QoS group limits. When this limit is reached, no new jobs with this QoS can be started, until some of the running jobs with this QoS finish and release some resources. The scheduler will indicate this with the reason QoS Group CPU/memory/GRES limit.    To prevent one user from single-handedly using all available resources in a certain QoS, there are also limits for the total resources that all running jobs of one user in a specific QoS can use.\n These are called the QoS per-user limits. When this limit is reached, no new jobs of this user with this QoS can be started, until some of the running jobs of this user and with this QoS finish and release some resources. The scheduler will indicate this with the reason QoS User CPU/memory/GRES limit.    These per-group and per-user limits (see Table 1) are set by the DAIC user board, and the scheduler strictly enforces these limits. Thus, no user can use more resources than the amount that was set by the user board. Any (perceived) imbalance in the use of resources by a certain QoS or user should not be held against a user or the scheduler, but should be discussed in the user board.\nResources reservations Slurm gives the possibility to reserve one or more compute nodes exclusively for a specific user or group of users. A reservation ensures that the designated node (or nodes) are dedicated solely to the reservation holder’s tasks and are not shared with other users during the reserved period. This feature allows users to plan the execution of future workloads, and accommodates cluster users with special needs beyond the batch system (eg latency measurement scenarios).\nNote Using reservations is in line with the General cluster usage clauses of DAIC users' agreement. However, please be mindful that reservations are intended to facilitate special needs that cannot be satisfied by the batch system, and should not be requested to guarantee fast throughput for production runs.  Requesting a Reservation To request a reservation for nodes, please use to the Request Reservation form. You can request a reservation for an entire compute node (or a group of nodes) if you have contributed this (or these) nodes to the cluster and you have special needs that needs to be accommodated.\nGeneral guidelines for reservations' requests:\n You can be granted a reservation only on nodes from a partition that is contributed by your group (See DAIC partitions and contributors page to check the name of the partition contributed by your group, and the DAIC hardware page for a listing of available nodes and their features). Please ask for the least amount of resources you need as to minimize impact on other users. Plan ahead and request your reservation as soon as possible: Reservations usually ignore running jobs, so any running job on the machine(s) you request will continue to run when the reservation starts. While jobs from other users will not start on the reserved node(s), the resources in use by an already running job at the start time of the reservation will not be available in the reservation until this running job ends. The earlier ahead you request resources, the easier it is to allocate the requested resources .  Using a reservation Once your reservation request is approved and a reservation is placed on the system, you can run your jobs in the reservation by specifying --qos=reservation along with the following directives to your slurm commands: --reservation=\u003cname\u003e and --partition=\u003cpartition\u003e. For example, to submit the job job.sbatch to a reservation named icra_iv on the cor1 node on the cor partition use:\n$ sbatch --qos=reservation --reservation=icra_iv --partition=cor job.sbatch Alternatively, it is possible to add the following lines to the job.sbatch file, and submitting this file as usual:\n#SBATCH --qos=reservation #SBATCH --reservation=icra_iv #SBATCH --partition=cor  Note It is possible to submit jobs to a reservation once it is created. Jobs will start immediately when the reservation is available, but already running jobs on resources will not be canceled for the reservation to start.  Note When a reservation is used to run your jobs, remember to also pass the reservation parameters to your srun steps:\n$ srun --qos=reservation --reservation=\u003creservation_name\u003e --partition=\u003cpartition_name\u003e \u003csome_script.sh\u003e   Viewing reservations To view all active and future reservations run the scontrol command as follows:\n$ scontrol show reservations ReservationName=icra_iv StartTime=2023-09-09T00:00:00 EndTime=2023-09-16T00:00:00 Duration=7-00:00:00 Nodes=cor1 NodeCnt=1 CoreCnt=32 Features=(null) PartitionName=cor Flags= TRES=cpu=64 Users=(null) Groups=(null) Accounts=3me-cor Licenses=(null) State=ACTIVE BurstBuffer=(null) Watts=n/a MaxStartDelay=(null) ReservationName=maintenance weekend 2023-10-14 StartTime=2023-10-13T20:00:00 EndTime=2023-10-16T09:00:00 Duration=2-13:00:00 Nodes=3dgi[1-2],100plus,awi[01-26],cor1,gpu[01-11],grs[1-4],influ[1-6],insy[11-12,14-16],tbm5,wis1 NodeCnt=58 CoreCnt=2000 Features=(null) PartitionName=(null) Flags=MAINT,IGNORE_JOBS,SPEC_NODES,ALL_NODES TRES=cpu=4000 Users=root Groups=(null) Accounts=(null) Licenses=(null) State=INACTIVE BurstBuffer=(null) Watts=n/a MaxStartDelay=(null)  Note  Jobs can run on a reservation only if explicitly requested as shown in the Requesting a reservation section. Only jobs from the Users or Accounts associated with the reservation (as shown in the scontrol show reservations output) will be run on the reservation STATE of a reservation will show as ACTIVE (instead of INACTIVE) during the reservation window.   Parallelizing jobs with Job Arrays There can be scenarios, eg in simulations or benchmarking, where a job script needs to run many times with only different parameter set each time. If done manually, keeping track of the parameter values and corresponding jobIds is cumbersome. Job Arrays are a convenient mechanism for submitting and managing such jobs.\nA job array is created by adding the --array=\u003cindexes\u003e directive to an sbatch script (or in the command line), where \u003cindexes\u003e can be either a comma separated list of integers, or a range with optional step size, eg, 1-10:2. The minimum index value is 0, and the maximum is a Slurm configuration parameter (MaxArraySize - 1).\nWithin a job array, all jobs have the same SLURM_ARRAY_JOB_ID, but each job will have its own environment variable SLURM_ARRAY_TASK_ID that corresponds to the array index value. Additionally, all jobs in the array inherit the same compute resources requirements. In the following examples, arrays of size 2 are created, but with different indexes:\n$ sbatch --array=1,4 jobscript.sbatch # Indexes specified as a list, and have values 1 and 4 Submitted batch job 8580151 $ $ squeue -u SomeNetID # Replace SomeNetId with your NetID  JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 8580151_1 general jobscrip SomeNetID R 0:01 1 grs4 8580151_4 general jobscrip SomeNetID R 0:01 1 awi18 $ sbatch --array=1-2 jobscript.sbatch # Range specified with default step size = 1. Index have values 1 and 2 Submitted batch job 8580149 $ $ squeue -u SomeNetID # Replace SomeNetId with your NetID  JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 8580149_1 general jobscrip SomeNetID R 0:21 1 grs4 8580149_2 general jobscrip SomeNetID R 0:21 1 awi18  Note To limit the maximum number of simultaneously running jobs in an array use the % separator, eg--array=1-15%3 to run only 3 tasks at a time.  JobId and environment variables As shown in the previous section, Parallelizing jobs with job arrays, jobs within an array are assigned special slurm variables. These variables can be exploited for various computational objectives. Among these, SLURM_ARRAY_TASK_ID is the index of an individual task within the array, and SLURM_ARRAY_JOB_ID is the slurm jobId of the entire array job.\nIn the simplest case, you can use the ${SLURM_ARRAY_TASK_ID} directly in a script to assign parameter values. For example, to run a workflow across a set of images image_1.png … image_5.png, you can simply create an array using the sbatch directive --array=1-5, and then, within your sbatch script, use image_${SLURM_ARRAY_TASK_ID}.png to indicate the corresponding image.\nIn more complex scenarios, eg, when the parameters of interest are not mappable to indexes (of a job array), you can use a config file to map the parameters to the job array indexes. For example, let’s assume the following parameters:\n$ cat jobarray.config i Flower Color Origin 1 Rose Red Worldwide 2 Jasmine White Asia 3 Tulip Various Persia\u0026Turkey 4 Orchid Various Worldwide 5 Lily Various Worldwide Now, you can use these parameters inside a job script as follows:\n$ cat jobarray.sbatch #!/bin/bash #SBATCH --job-name=JobArrayExample #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --array=1-5 # Arry with 5 tasks #SBATCH --output=slurm-%A_%a.out # Set name of output log. %A is SLURM_ARRAY_JOB_ID and %a is SLURM_ARRAY_TASK_ID #SBATCH --error=slurm-%A_%a.err # Set name of error log. %A is SLURM_ARRAY_JOB_ID and %a is SLURM_ARRAY_TASK_ID config=jobarray.config # Path to config file # Obtain parameters from config file: flower=$(awk -v ArrayTaskID=$SLURM_ARRAY_TASK_ID '$1==ArrayTaskID {print $2}' $config) color=$(awk -v ArrayTaskID=$SLURM_ARRAY_TASK_ID '$1==ArrayTaskID {print $3}' $config) origin=$(awk -v ArrayTaskID=$SLURM_ARRAY_TASK_ID '$1==ArrayTaskID {print $4}' $config) # Use the parameters, eg, print the index and parameter values to a file: echo \"Array task: ${SLURM_ARRAY_TASK_ID}, Flower: ${flower}, color: ${color}, origin: ${origin}\" \u003e\u003e output.txt $ $ sbatch jobArray.sbatch Submitted batch job 8580317 $ squeue -u SomeNetID # Replace SomeNetId with your NetID  JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 8580317_[1-5] general JobArray SomeNetID PD 0:00 1 (Priority) In this example, slurm created 5 jobs in a job array, each using the same settings (the name JobArrayExample, the general partition, short QoS, 00:01:00 time, 1 task with 1 CPU and 1G memory, and an output and error file with both array job Id and task id). Each task looks up certain parameter values from a config file leveraging its index via the awk command.\nNote The command:\nflower=$(awk -v ArrayTaskID=$SLURM_ARRAY_TASK_ID '$1==ArrayTaskID {print $2}' $config) assigns a value to the variable flower by reading a configuration file ($config), and printing the value in the second column ({print $2}) where the first column matches the value of the ArrayTaskID variable ($1==ArrayTaskID). The ArrayTaskID is an awk variable set to the value of the SLURM environment variable SLURM_ARRAY_TASK_ID. For more on the awk utility, see this awk tutorial.\n Jobs within a task array are run in parallel, and hence, there’s no guarantee about their order of execution. This is evident looking at the output file from this example:\n$ cat output.txt Array task: 2, Flower: Jasmine, color: White, origin: Asia Array task: 3, Flower: Tulip, color: Various, origin: Persia\u0026Turkey Array task: 1, Flower: Rose, color: Red, origin: Worldwide Array task: 5, Flower: Lily, color: Various, origin: Worldwide Array task: 4, Flower: Orchid, color: Various, origin: Worldwide Other slurm variables that are set inside a job array are shown in the following table, with values based on the preceding example:\n   Slurm Environment Variable Description Value in example     SLURM_ARRAY_JOB_ID The first job ID of the array. 8580317   SLURM_ARRAY_TASK_ID The job array index value. A value in range 1-5   SLURM_ARRAY_TASK_COUNT The number of tasks in the job array. 5   SLURM_ARRAY_TASK_MAX The highest job array index value. 5   SLURM_ARRAY_TASK_MIN The lowest job array index value 1    Slurm commands and job arrays The squeue command reports all submitted jobs. By default, squeue reports all of the tasks associated with a job array in one line and uses a regular expression to indicate the SLURM_ARRAY_TASK_ID values. To explicitly print one job array element per line, use the --array or -r flag. The following examples highlight the difference, using the same jobarray.sbatch file from the JobId and environment variables section:\n$ sbatch jobarray.sbatch Submitted batch job 8593299 $ $ squeue -u SomeNetID # Replace SomeNetId with your NetID  JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 8593299_[1-5] general JobArray SomeNetID PD 0:00 1 (Priority) $ $ squeue -r -u SomeNetID # Replace SomeNetId with your NetID  JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 8593299_1 general JobArray SomeNetID PD 0:00 1 (Priority) 8593299_2 general JobArray SomeNetID PD 0:00 1 (Priority) 8593299_3 general JobArray SomeNetID PD 0:00 1 (Priority) 8593299_4 general JobArray SomeNetID PD 0:00 1 (Priority) 8593299_5 general JobArray SomeNetID PD 0:00 1 (Priority) scancel, on the other hand, can be used to cancel an entire job array by specifying its SLURM_ARRAY_JOB_ID. Alternatively, to cancel a specific task (or tasks), both its SLURM_ARRAY_JOB_ID and SLURM_ARRAY_TASK_ID must be specified, possibly with a regular expression, as shown in the following examples:\n$ sbatch jobarray.sbatch $ squeue -u SomeNetID # Replace SomeNetId with your NetID  JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 8593321_[1-5] general JobArray SomeNetID PD 0:00 1 (Priority) $ $ scancel 8593321_4 # Cancel task with index 4 in the array $ squeue -u SomeNetID # Replace SomeNetId with your NetID  JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 8593321_[1-3,5] general JobArray SomeNetID PD 0:00 1 (Priority) $ $ scancel 8593321_[1-3] # Cancel tasks in index range 1-3 in the array $ squeue -u SomeNetID # Replace SomeNetId with your NetID  JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 8593321_5 general JobArray SomeNetID PD 0:00 1 (Priority) $ $ scancel 8593321 # Cancel all tasks in the array $ squeue -u SomeNetID # Replace SomeNetId with your NetID  JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) $  Note For more information on job arrays, refer to Slurm Job Array Support  Troubleshooting Common Issues Please see the Frequently asked questions on Scheduler problems  and Job resources\nKerberos Authentication Kerberos is an authentication protocol which uses tickets to authenticate users (and computers). You automatically get a ticket when you log in with your password on a TU Delft installed computer. You can use this ticket to authenticate yourself without password when connecting to other computers or accessing your files. To protect you from misuse, the ticket expires after 10 hours or less (even when you’re still logged in).\nFile access Your Linux and Windows home  directories and the Group and Bulk shares are located on network fileservers, which allows you to access your files from all TU Delft installed computers. Kerberos authentication is used to enable access to, or protect, your files. Without a valid Kerberos ticket (e.g. when the ticket has expired) you will not be able to access your files but instead you will receive a Permission denied error.\nLifetime of Kerberos Tickets Kerberos tickets have a limited valid lifetime (of up to 10 hours) to reduce the risk of abuse, even when you stay logged in. If your tickets expire, you will receive a Permission Denied error when you try to access your files and a password prompt when you try to connect to another computer. When you want your program to be able to access your files for longer than the valid ticket lifetime, you’ll have to renew your ticket (repeatedly) until your program is done. Kerberos tickets can be renewed up to a maximum renewable life period of 7 days (again to reduce the risk of abuse).\nThe command klist -5 lists your cached Kerberos tickets together with their expiration time and maximum renewal time:\n$ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_random Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/01/01 00:00:00 01/01/01 10:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/08/01 00:00:00 Where:\n  Ticket cache: The Kerberos tickets that have been issued to you are stored in a ticket cache file. You can have multiple ticket cache files on the same computer (from different connections, for example) with different tickets and ticket expiration times. Some ticket cache files are automatically removed when you logout. Tip Make sure that you renew the tickets in the right ticket cache file (see this screen example).    Default principal: Your identity.\n  Service principal: The identity of services that you have gotten tickets for. You always need a Kerberos ticket-granting ticket (krbtgt) in order to obtain other tickets for specific services like accessing files (nfs) or connecting to computers (host).\n  Valid starting, Expires: Your ticket is only valid between these times (this period is called the valid lifetime). After this time you will not be able to use the service nor automatically renew the ticket (without password).\n  Renew until: Your ticket can only be renewed without password up to this time. After this time you will have to obtain a new ticket using your password.\n  Renewing Kerberos tickets If you have a valid Kerberos krbtgt ticket, you can renew it at any time (until it expires) by running the command kinit -R:\n$ kinit -R $ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_random Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/01/01 01:00:00 01/01/01 11:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/08/01 00:00:00  Note Renewing the ticket will not change the duration of the valid lifetime, i.e. a krbtgt ticket with a valid lifetime of 1 hour will, after renewal, be valid for another hour.  When the krbtgt ticket has expired or reached it’s renew until time, you will have to obtain a new ticket by running kinit -r 7d (note the difference in case for the r) and authenticating with your password:\n$ kinit -r 7d Password for YourNetID@TUDELFT.NET: $ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_random Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/01/01 11:00:00 01/01/01 21:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/08/01 11:00:00 The new ticket will have a valid lifetime of 10 hours and a renewable life of 7 days.\nOn the TU Delft Linux desktops your Kerberos ticket is refreshed (i.e. replaced by a new ticket) automatically every time you enter your password for unlocking the screen saver.\nTip Do not disable the screen saver password lock.  On remote computers you have to manually renew your tickets before they expire.\nSlurm \u0026 Kerberos  Slurm caches your Kerberos ticket, and uses it to execute your job Regularly renew the ticket in Slurm’s cache while your jobs are queued or running:  $ auks -a Auks API request succeed  To automatically renew your ticket in Slurm’s cache until you change your NetID password, run the following on the login1 server:  $ install_keytab Password for somebody@TUDELFT.NET: Installed keytab. You need to rerun this command whenever you change your NetID password (at least every 6 months). Otherwise, the automatic renewal will not work and you will receive a warning e-mail.\nRenewal using screen On the compute servers, the screen program has been modified to allow jobs to run unattended for up to 7 days. It creates a private ticket cache (to prevent the cache from being destroyed at logout) and automatically renews your ticket up to the maximum renewable life. For example, start MATLAB in Screen with screen matlab (the order is important!).\n$ screen matlab Warning: No display specified. You will not be able to display graphics on the screen. \u003c M A T L A B (R) \u003e Copyright 1984-2010 The MathWorks, Inc. Version 7.11.0.584 (R2010b) 64-bit (glnxa64) August 16, 2010 To get started, type one of these: helpwin, helpdesk, or demo. For product information, visit www.mathworks.com. \u003e\u003e For longer jobs you have to manually obtain a new ticket at least every 7 days by running kinit -r 7d from within screen (so you use the specific ticket cache file that screen is using):\n connect to screen (screen -r), create a new window (Ctrl-a c), run kinit -r 7d, exit the window (exit) and detach from screen (Ctrl-a d).  $ kinit -r 7d Password for YourNetID@TUDELFT.NET: $ klist -5 Ticket cache: FILE:/tmp/krb5cc_uid_private Default principal: YourNetID@TUDELFT.NET Valid starting Expires Service principal 01/08/01 09:00:00 01/08/01 19:00:00 krbtgt/TUDELFT.NET@TUDELFT.NET renew until 01/15/01 09:00:00 $ exit  Tip Use a repeating reminder (twice a week) in your agenda so you don’t forget.  Important When the end of the renewable life is reached, your tickets expire and your program(s) will return Permission denied errors when trying to access your files. Your program(s) will not be terminated automatically; you still have to terminate the program(s) yourself.  Extra functionality can be provided by the k5start and krenew programs. On most computers these are not available by default but can be installed (ask Robbert).\n","categories":"","description":"How to submit jobs to slurm?\n","excerpt":"How to submit jobs to slurm?\n","ref":"/docs/job_submissions/","tags":"","title":"Job submission and management"},{"body":" Running a ray cluster is known to be problematic on Slurm-based systems like DAIC. The DelftBlue documentation on Ray cluster provides handy tips on such settings\n ","categories":"","description":"How to run a ray cluster?\n","excerpt":"How to run a ray cluster?\n","ref":"/tutorials/ray_cluster/","tags":"","title":"Ray Cluster"},{"body":" At present DAIC and DelftBlue have different software stacks. This pertains to the operating system (CentOS 7 vs Red Hat Enterprise Linux 8, respectively) and, consequently, the available software. Please refer to the respective DelftBlue modules and DAIC available software documentation before commencing your experiments.\n Operating System DAIC runs the CentOS   7 Linux distribution, which provides the general Linux software. Most common software, including programming languages, libraries and development files for compiling your own software, is installed on the servers (see Available software and libraries). However, a not-so-common program that you need might not be installed. Similarly, if your research requires a state-of-the-art program that is not (yet) available as a package for CentOS   7, then it is not available. See Installing software for more information.\nJob Scheduling Software (Slurm) DAIC uses the Slurm scheduler   to efficiently manage workloads. All jobs for the cluster have to be submitted as batch jobs into a queue. The scheduler then manages and prioritizes the jobs in the queue, allocates resources (CPUs, memory) for the jobs, executes the jobs and enforces the resource allocations. See the scheduler pages for more information.\n","categories":"","description":"How to set up your tools and/or run certain libraries\n","excerpt":"How to set up your tools and/or run certain libraries\n","ref":"/docs/software_environment/","tags":"","title":"Software Environment"},{"body":" Coming soon!\n   ","categories":"","description":"How to profile your compute usage\n","excerpt":"How to profile your compute usage\n","ref":"/docs/performance/","tags":"","title":"Performance profiling and Optimization"},{"body":"TU Delft clusters DAIC is one of several clusters accessible to TU Delft CS researchers (and their collaborators). The table below gives a comparison between these in terms of use case, eligible users, and other characteristics.\n    DAIC DelftBlue DAS     Primary use cases Research, especially in AI Research \u0026 Education Distributed systems research, streaming applications, edge and fog computing, in-network processing, and complex security and trust policies, Machine learning research, ...   Contributors Certain groups within TU Delft (see Brief history of DAIC) All TU Delft faculties Multiple universities \u0026 SURF   Eligible users   Faculty, PhD students, and researchers from contributing departments  MSc and BSc students (if recommended by a professor) are provided limited access    All TU Delft affiliates    Faculty and PhD students who are either members of the ASCI research school or the ASCI partner universities   ASTRON employees   NLeSC employees   Master students (if recommended by a professor) are provided limited access      Website DAIC documentation DelftBlue Documentation DAS Documentation   Contact info DAIC community DHPC team DAS admin   Request account Access and Accounts  Get an account Email DAS admin with details like user's affiliation and the planned purpose of the account.   Getting started Quick start Crash course    Hardware Hardware infrastructure DHPC hardware  Head node +   16 x FAT nodes (Lenovo SR665, dual socket, 2x16 core, 128 GB memory, 1xA4000)   4 x GPU nodes (Lenovo SR665, dual socket, 2x16 core, 128 GB memory, 1xA5000)      Software stack Software environment DHPC modules Base OS: Rocky Linux, OpenHPC, Slurm Workload Manager   Data storage Filesystem \u0026 storage Storage Storage: 128 TB (RAID6)    Access to TU Delft Network storage ✓ Only in login nodes  Not supported    Sharing data in collaboration ✓ ✗    Has GPUs? ✓ ✓ ✓   Cost of use Contribution towards hardware purchase  -    SURF clusters SURF, the collaborative organization for IT in Dutch education and research, has installed and is currently operating the Dutch National supercomputer, Snellius, which houses 144 40GB A100 GPUs as of Q3 2021 (36 gcn nodes x 4 A100 GPUs/node = 144 A100 GPUs total) with other specs detailed in the Snellius hardware and file systems wiki.\nSURF also operates other clusters like Spider for processing large structured data sets, and ODISSEI Secure Supercomputer (OSSC) for large-scale analyses of highly-sensitive data. For an overview of SURF clusters, see the SURF wiki.\nTU Delft researchers in TBM and CITG already have direct and easy access to the compute power and data services of SURF, while members of other faculties need to apply for access as detailed in SURF’s guide to Apply for access to compute services.\nTU Delft cloud resources For both education and research activities, TU Delft has established the Cloud4Research program. Cloud4Research aims to facilite the use of public cloud resources, primarily Amazon AWS. At the administrative level, Cloud4Research provides AWS accounts with an initial budget. Subsequent billing can be incurred via a project code, instead of a personal credit card. At the technical level, the ICT innovation teams provides intake meetings to facilitate getting started. Please refer to the Policies and FAQ pages for more details.\n","categories":"","description":"","excerpt":"TU Delft clusters DAIC is one of several clusters accessible to TU …","ref":"/tud_clusters/","tags":"","title":"TU Delft clusters comparison"},{"body":"These pages contain basic concepts and details to make optimal use of TU Delft’s DAIC. Alternatively, you might with to jump to the QuickStart guide or Tutorials for more thematic content.\n","categories":"","description":"","excerpt":"These pages contain basic concepts and details to make optimal use of …","ref":"/docs/","tags":"","title":"DAIC Documentation"},{"body":"Pre-requisits  Access credentials (see Access and Accounts). Basic familiarity with the command line (see The software carpentry's Unix shell materials   )  Cluster workflow When working with HPCs in general, or DAIC in particular, the workflow of Fig 1 needs to be followed, where:\n code is developed locally (eg, in a laptop or PC), then ported to the cluster (see Connecting to DAIC and Data transfer methods).  Possibly, software and dependencies are set up (see Software environment).   Typically, code is tested in the cluster, eg in an interactive session (see Interactive jobs on compute nodes), following Best practices, and consulting with Support resources. If testing is successful, jobs scripts are submitted to the scheduler (see Job submission), and progress is monitored (see Checking slurm jobs). Finally, once all is done, intermediate files are deleted (see How do I clean up tmp?), and final results are downloaded for subsequent downstream analysis.    Cluster workflow, with key Unix* based commands for each step. Text within angle brackets \u003c, \u003e denote names that are chosen by the user\n  Handy commands    Command Purpose Example     sinteractive For requesting an interactive node, typically, during testing phases. Needed compute resources are specified as part of the command, including mem, time, and gres, analogously to sbatch directives. Request a GPU node for 10 minutes: sinteractive --time=00:10:00 --gres=gpu   sbatch For submitting a script to slurm for queuing (in batch mode). Requested resources are specified as directives on top of the script Submit a job in script.sh file sbatch script.sh   squeue Check the status of jobs in the queue. Check the user’s jobs: squeue -u $USER   scancel Cancel a job or all jobs of a user. scancel -u $USER or scancel \u003cjobid\u003e   slurmtop DAIC-specific command to view top jobs in the queues and their resource use    scontrol show job \u003cjobid\u003e Show the details and resources allocated to the job with slurm id \u003cjobid\u003e, eg, the UserId, QOS, TimeLimit, Partition, Command, WorkDir, StdErr and StdOut scontrol show job 1    ","categories":"","description":"","excerpt":"Pre-requisits  Access credentials (see Access and Accounts). Basic …","ref":"/quickstart/","tags":"","title":"QuickStart"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tutorials/","tags":"","title":"Tutorials"},{"body":" give it a try- find the names on\n Acknowledge different contributors:\n Sys admins? Funders? Content Writers? Reviewers and Editors? Input and Suggestions?  ","categories":"","description":"Who contributed to this documentation?\n","excerpt":"Who contributed to this documentation?\n","ref":"/docs/contributors/","tags":"","title":"Contributors"},{"body":" This is a placeholder page that shows you how to use this template site.\n ","categories":"","description":"New page\n","excerpt":"New page\n","ref":"/tutorials/new_page/","tags":"","title":"New page"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/","tags":"","title":"Categories"},{"body":"  #td-cover-block-0 { background-image: url(/dike-denny-muller-ERCLVWKrlGM-unsplash-background_hu3d03a01dcc18bc5be0e67db3d8d209a6_1983423_960x540_fill_q75_catmullrom_smart1.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/dike-denny-muller-ERCLVWKrlGM-unsplash-background_hu3d03a01dcc18bc5be0e67db3d8d209a6_1983423_1920x1080_fill_q75_catmullrom_smart1.jpg); } }  Welcome to DAIC: The Delft AI Cluster Get Started   Learn more    DAIC (formerly known as INSY-HPC or just plainly HPC) is a TU Delft High Performance Computing (HPC) cluster consisting of Linux compute servers with a lot of processing power and memory for running large, long or GPU-enabled jobs. DAIC was initiated within the INSY department in 2015, and later, resources were joined with ST (both from EWI), and with other departments across TU Delft faculties (TNW, TPM, 3mE and CiTG). The cluster is available (only) to users from the participating departments, and access can be arranged through the department's contact persons.           Campus-wide disruptions and maintenance\n      Frequently Asked Questions For handy tips and current known issues, head over to the Support area\n Read more …\n   Contributions welcome! We do a Merge Request contributions workflow on GitLab. Feedback is always welcome!\n Read more …\n   Community support For specific questions, you can consult with us and other cluster users on MatterMost, or even submit a ticket in the TU Delft Self-Service Portal\n Read more …\n       Download from AppStore Get the Goldydocs app!\n    Contributions welcome! We do a Pull Request contributions workflow on GitHub. New users are always welcome!\n Read more …\n   Follow us on Twitter! For announcement of latest features etc.\n Read more …\n    --  This is another Section     -- ","categories":"","description":"","excerpt":"  #td-cover-block-0 { background-image: …","ref":"/","tags":"","title":"HomePage"},{"body":"   slurmtop (login required)    slurmtop is available as both a cluster command, and as a webpage. Both the command and webpage display the following tables:  Summary on resources allocations in the general partition in: Allocated/Idle/Other/Total (in the command line version) or Total/allocation (in the webpage version) format Per-node details on status and resources allocations in the general partition  Normalized and Effective per-account resource usage information  Resource usage and fairshare information for the top 10 cluster users (in terms of Normalized usage) Details of jobs in the cluster, sorted by priority and jobID      SlurmEff (login required)    A summary of efficiency statistics of your own jobs. Statistics are calculated on the basis of requested vs consumed resources.   Cluster Monitoring Graphs      DAIC status check (Access from TUD network)    A brief overview of:  Login nodes status Compute nodes status  Summary graphs     ","categories":"","description":"","excerpt":"   slurmtop (login required)    slurmtop is available as both a …","ref":"/monitoring/","tags":"","title":"Monitoring"},{"body":"","categories":"","description":"","excerpt":"","ref":"/search/","tags":"","title":"Search Results"},{"body":"Group-specific resources In line with the steps in What to do in case of problems, the following links are group-specific resources that you may find relevant:\n Main DAIC landing page    Central wiki    PRB wiki    DBL wiki    Imphys docs    Influence github resources    WIS Google doc     Support \u0026 contact: If the answer to your question is not found in any of the Group-specific resources, consult with:\n DAIC community on: Mattermost HPC support via the Self Service Portal via the I have a different question form under the Research support tile. Please give a relevant Brief Description, and in your question include the line This question is for operator group ICT-SYSTEMS-HPC.  Note  The linux bastions and the central storage are general IT facilities, not dedicated parts of the DAIC cluster. Direct your questions about those to their respective support teams.   Stop! Do not send email directly to the personal email address of a support person, as that person may not always be able to respond immediately (i.e. outside office hours or when away or ill) and the other HPC support team members will not be able to see and respond to your question.  External resources:   Introduction to High-Performance Computing     Introduction to Using the Shell in a High-Performance Computing Context     The Unix Shell     HPC carpentry lessons     Other software carpentry lessons     Data carpentry lessons     ","categories":"","description":"","excerpt":"Group-specific resources In line with the steps in What to do in case …","ref":"/support/","tags":"","title":"Support"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/","tags":"","title":"Tags"}]