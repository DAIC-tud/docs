[
  {
    "QuestionTitle": "<h3>SSH: <code>The authenticity of host 'login<em>X</em>' can't be established.</code></h3>",
    "QuestionBody": "",
    "Tags": [
      "General questions"
    ],
    "Answer": "<p>When connecting to a login node for the first time, you must <strong>not continue</strong> when the key fingerprint reported by your ssh connection does <strong>not match</strong> one of the fingerprints shown here:\n  * For <code>login1.daic.tudelft.nl</code> and <code>login2.daic.tudelft.nl</code>:\n      * <code>2iPjH/j/Tf5JZU4OJyLpASA/GZ40eCqvcQnSSa++3nQ (ECDSA)</code>\n      * <code>MURg8IQL8oG5o2KsUwx1nXXgCJmDwHbttCJ9ljC9bFM (ED25519)</code>\n      * <code>mKgxUQvmOVM74XvFNhWt0ODsRvfnmwIgZWcw8uPJ68o (RSA)</code>\n      * <code>05:24:a0:b4:83:27:05:32:4b:83:78:2a:20:99:f8:5c (ECDSA)</code>\n      * <code>C5:21:46:cb:73:cd:72:e6:18:04:d6:67:2a:67:90:75 (ED25519)</code>\n      * <code>05:17:84:7f:9f:18:e3:71:b4:df:5e:c0:12:db:e8:fc (RSA)</code>\n  * For <code>login3.daic.tudelft.nl</code>:\n      * <code>IaBwyYiZi1Etj7yBDtdv7sByHzH+hedW69QA8UxGUqk (ECDSA)</code>\n      * <code>O3AjQQjCfcrwJQ4Ix4dyGaUoYiIv/U+isMT5+sfeA5Q (ED25519)</code>\n      * <code>fslv0RnC9zkVBf34i3g1BPKaYBcsTgKqu8+PMKLTEvw (RSA)</code>\n      * <code>5e:9a:69:30:75:d3:b5:75:29:b3:32:fc:48:ab:b2:f9 (ECDSA)</code>\n      * <code>31:eb:cd:95:8f:d1:78:29:e1:70:f9:8b:b0:cd:56:5c (ED25519)</code>\n      * <code>ba:b9:92:4b:1a:00:8c:f1:aa:49:09:53:fa:b6:79:5f (RSA)</code>\nWhen the key fingerprint matches, you can safely continue.\nWhen in the future your ssh connection tells you that the key has changed, and it doesn’t match one of the fingerprints above, contact the <a href=\"../../#support--contact\">DAIC support team</a>.</p>"
  },
  {
    "QuestionTitle": "<h3>SSH: <code>Permission denied, please try again.</code></h3>",
    "QuestionBody": "",
    "Tags": [
      "General questions"
    ],
    "Answer": "<ul>\n<li>The DAIC cluster is not freely accessible. It is facilitated by several departments and groups within the university for their research/education.</li>\n<li>If you have access, this message indicates that either your access expired (in case an end date was set), or your account was (temporarily) disabled due to problems with your use of the login nodes, or there is a problem with your NetID account/password.</li>\n</ul>"
  },
  {
    "QuestionTitle": "<h3><code>-bash: cd: /home/nfs/&lt;NetID&gt;: Key has expired</code></h3>",
    "QuestionBody": "",
    "Tags": [
      "General questions"
    ],
    "Answer": "<ul>\n<li>This means your Kerberos ticket has expired. Your need to renew it, either by running <code>kinit</code>, or by logging out then logging in again (using your password!). Also see <a href=\"/docs/manual/job-submission/kerberos\">Kerberos authentication</a>.</li>\n<li>Please log out when you’re not using the cluster (so you don’t hit this problem, and so you don’t block resources on the login node).</li>\n</ul>"
  },
  {
    "QuestionTitle": "<h3><code>Disk quota exceeded</code></h3>",
    "QuestionBody": "",
    "Tags": [
      "General questions"
    ],
    "Answer": "<ul>\n<li>The size of the data in this storage has reached the maximum allowed size (also known as <em>quota limit</em>). For <code>$HOME</code> folders (see <a href=\"/docs/system#personal-storage-aka-home-folder\">Personal storage</a>) the maximum allowed amount of data is 8 GB, for project storage (see <a href=\"/docs/system#project-storage\">Project storage</a>) the quota limit can be up to 5TB of data.</li>\n<li>To see how much space your <code>$HOME</code> files are using, run <code>du -h ~ | sort -h</code>. (When you have many files or folders, this can take a long time!)</li>\n<li>To make space available, you'll either need to clean up some files (like installation archives and caches), or move some of your files somewhere else. </li>\n</ul>\n<p>{{% alert title=\"Note\" color=\"info\" %}}</p>\n<p>Your home folder is for storing settings and installing small software packages, not for storing data or large software installations. You need to store those in project storage. Your project leader/supervisor can request project storage for you via the <a href=\"https://tudelft.topdesk.net/tas/public/ssp/\">Self Service Portal (TOPdesk)</a>.</p>\n<p>{{% /alert %}}</p>"
  },
  {
    "QuestionTitle": "<h3><code>The system load on login_X_ is too high! Please use another node if you can.</code></h3>",
    "QuestionBody": "",
    "Tags": [
      "General questions"
    ],
    "Answer": "<ul>\n<li>This message is mainly a warning for the person that is causing the high load. If that is you, you should either do the work as a cluster job, or limit the number of threads or memory that you use. If you're not the one running heavy tasks, you can choose to ignore it.</li>\n</ul>"
  },
  {
    "QuestionTitle": "<h3>staff-umbrella: <code>Operation not permitted</code></h3>",
    "QuestionBody": "",
    "Tags": [
      "General questions"
    ],
    "Answer": "<ul>\n<li>The network filesystem for the bulk, groups and project storage (staff-bulk, staff-groups, staff-umbrella) does not support <code>chmod</code> (changing permissions) or <code>chown</code> (changing owner or group) operations:</li>\n<li>When you run these operations, you will receive an <code>Operation not permitted</code> error. This has nothing to do with your personal rights, it’s just not supported.</li>\n<li>It's also not necessary to change these, since the <em>default permissions are correct for normal use</em>. So, you can <strong>safely</strong> skip these operations or <strong>ignore</strong> these errors in many situations.</li>\n<li>For <code>rsync</code> operations , use <code>rsync -a --no-perms</code>.</li>\n<li>If all else fails, a workaround is to (temporarily!) use the <code>/tmp</code> folder: move your folder that gives the error to <code>/tmp</code>, create a symbolic link from the folder in <code>/tmp</code> to the original location, rerun the commands that gave the error as before, then move your folder back from <code>/tmp</code> to the original location. For example, when you get an error in folder <code>&lt;foldername&gt;</code>, do:\n    <code>mkdir /tmp/${USER}\n    mv &lt;foldername&gt; /tmp/${USER}/&lt;foldername&gt;\n    ln -s /tmp/${USER}/&lt;foldername&gt; &lt;foldername&gt;\n    :\n    : #&lt;rerun command(s) that gave the error&gt;\n    :\n    rm &lt;foldername&gt;\n    mv /tmp/${USER}/&lt;foldername&gt; &lt;foldername&gt;\n    rmdir /tmp/${USER}</code></li>\n</ul>"
  },
  {
    "QuestionTitle": "<h3>My program requires a newer version of CMake</h3>",
    "QuestionBody": "",
    "Tags": [
      "Software questions"
    ],
    "Answer": "<p>Use <code>cmake3</code>.</p>"
  },
  {
    "QuestionTitle": "<h3>How can I run a Docker container?</h3>",
    "QuestionBody": "",
    "Tags": [
      "Software questions"
    ],
    "Answer": "<p>Using <code>apptainer</code>. See the <a href=\"/tutorials/apptainer\">Apptainer tutorial</a> for more information.</p>"
  },
  {
    "QuestionTitle": "<h3>My program requires a newer version of GCC</h3>",
    "QuestionBody": "",
    "Tags": [
      "Software questions"
    ],
    "Answer": "<p>Newer versions of GCC are available through the {{&lt; external-link \"https://developers.redhat.com/products/developertoolset/overview\" \"devtoolset\" &gt;}} modules. See the <a href=\"/docs/manual/software/modules#environment-modules\">Modules</a> for information on using modules.</p>"
  },
  {
    "QuestionTitle": "<h3>I want to use R</h3>",
    "QuestionBody": "",
    "Tags": [
      "Software questions"
    ],
    "Answer": "<p>There are a few options:\n* You can use the pre-installed R\n* You can {{&lt; external-link \"https://docs.anaconda.com/anaconda/user-guide/tasks/using-r-language/#creating-a-new-environment-with-r\" \"install R using Conda\" &gt;}}. Conda is available via the <code>miniconda</code> module.\n* You can {{&lt; external-link \"https://rviews.rstudio.com/2017/03/29/r-and-singularity/\" \"use R from a container\" &gt;}}. Containers can be run using <code>Apptainer</code> on DAIC, as explained in this <a href=\"/tutorials/apptainer\">Apptainer tutorial</a>.</p>"
  },
  {
    "QuestionTitle": "<h3>How to use TensorBoard on the DAIC cluster?</h3>",
    "QuestionBody": "",
    "Tags": [
      "Software questions"
    ],
    "Answer": "<ul>\n<li>TensorBoard is very insecure: anybody can connect to it, without authentication (i.e. when you run TensorBoard on the DAIC cluster, any TU Delft user can connect to it). And this is actually {{&lt; external-link \"https://github.com/tensorflow/tensorboard/issues/267\"  \"on purpose\"&gt;}}, because making it secure and being able to guarantee that would require too much effort. <strong>So you can't run TensorBoard directly on the DAIC cluster!</strong></li>\n<li>The most secure way to run TensorBoard is to run it on your personal computer (with a proper firewall). When you put your TensorFlow log files on a network folder, you can access them directly on your personal computer so you can use TensorBoard in the same way as you do in the DAIC cluster. (You can also download the log files if you find that easier.)</li>\n</ul>"
  },
  {
    "QuestionTitle": "<h3>What are the limits?</h3>",
    "QuestionBody": "",
    "Tags": [
      "Job resource questions"
    ],
    "Answer": "<ul>\n<li>The <a href=\"/docs/manual/job-submission/qos\">hard limits</a> are in place only to protect the cluster from extreme overloads. The guiding principles of the cluster are fair-share and fair-use: all users should be able to use the cluster at the same time, and nobody should cause problems for the cluster or other users (see <a href=\"/docs/manual/job-submission/priorities#slurms-job-scheduling-and-waiting-times\">Slurm's job scheduling and waiting times</a>). So you need to make sure that your jobs do not unfairly hinder other jobs.</li>\n</ul>"
  },
  {
    "QuestionTitle": "<h3>What is the minimum runtime for a job?</h3>",
    "QuestionBody": "",
    "Tags": [
      "Job resource questions"
    ],
    "Answer": "<ul>\n<li>Submitting, scheduling and starting jobs bring along a certain overhead. To reduce the effects of the overhead, jobs need to run for at least 1 minute, but preferably 5 minutes to 1 hour. So limit the number of CPUs to at most 2 to 4 (to make the jobs easier to schedule and not finish <em>too quickly</em>) and where possible combine multiple jobs into one job (to reduce the number of jobs to at most 50 and add together the runtime of short jobs).</li>\n</ul>"
  },
  {
    "QuestionTitle": "<h3>How to determine the CPU load, memory use and number of active threads of your program?</h3>",
    "QuestionBody": "",
    "Tags": [
      "Job resource questions"
    ],
    "Answer": "<ul>\n<li>Log in on a login node (<code>login1</code> or <code>login3</code>) and run your program (<code>python …</code>).</li>\n<li>Log in a second time on the same login node and run <code>top -H -o TIME -u $USER</code> to see all your threads and their <code>%CPU</code> and <code>%MEM</code> use.</li>\n<li>A <code>%CPU</code> of 100 corresponds to 1 CPU, 200 to 2 CPUs, and less than 75 is an indication that your program is not able to fully (optimally) use a CPU. </li>\n<li><code>%MEM</code> is a percentage of 16 GB (<code>login1</code>) or 512 GB (<code>login3</code>); you’ll need to convert the <code>%MEM</code> to MB or GB for your job script (round up to the next GB for a little extra headroom). To determine the active threads of the program, count the threads (belonging to the program) with <code>%CPU</code> &gt; 5 and steadily increasing <code>TIME+</code>.  (Quit top by pressing <code>q</code>.)</li>\n<li>For running jobs, the <code>CPU Load</code> statistics on the website is one indicator of problems (when the CPU load exceeds 100%).</li>\n<li><em>Background:</em> the system restricts a job so that it can only use the allocated CPU cores, and a CPU core can only run one thread at a time. So the actual CPU use cannot exceed the allocated number of CPU cores. When your program uses more active threads than the number of allocated CPU cores, only some threads can run, and the rest of the threads have to wait. So the CPU load, i.e. the number of active (running and waiting) threads per core, goes above 100%. Having threads waiting reduces the performance (and adds overhead).</li>\n</ul>\n<p><strong><span style=\"text-decoration:underline;\">Example</span></strong></p>\n<p>Let’s assume you want to know the sum of 50 numbers. The simplest way to calculate this is to add the first two numbers, then add the third number to that, and so on. You will need to perform 49 additions, one at a time (one after the other), in one sequence, thus <strong>1 thread</strong>. To run this single thread, <strong>a single CPU</strong> is needed (to perform one addition at a time). The “time” needed to compute the sum is <strong>49 CPU cycles</strong> (1 addition per cycle). The <strong>CPU load is 100%</strong> (1 active thread per CPU) and the <strong>CPU efficiency is 100%</strong> (the one CPU will be active the whole time).</p>\n<p>A simple way to speed this up would be to divide the 50 numbers in two groups of 25 numbers, and use <strong>2 separate threads</strong> to calculate the sums of the two groups at the same time (in parallel). Then add the sums of the two groups to get the total sum. Each thread needs its own CPU to perform the addition, so you need <strong>2 CPUs</strong> to be able to run the <strong>2 threads</strong> in parallel. Afterwards you will need to add the two sums together to get the total sum. So the time needed when using 2 threads is <strong>25 cycles</strong> (24 additions to sum each group, and 1 addition for the total). The average CPU load will be <strong>98%</strong> (1 active thread per CPU for 24 cycles, plus one final addition on only 1 CPU; the other CPU will be idle). The average CPU efficiency is also <strong>98%</strong> (49 additions in 50 available CPU cycles).</p>\n<p>The <strong>2 threads</strong> could also have shared <strong>1 CPU</strong> (by first doing an addition in the first thread, temporarily storing that result, then doing an addition in the second thread, temporarily storing that result, and so on). However, because of the additional overhead, it would take <em>longer</em> than by using <em>one</em> thread. The average CPU load would be <strong><em>196%</em></strong> (24 additions with 2 active threads on 1 CPU, plus the final addition). The CPU efficiency would be <strong>100%</strong> (the one CPU will be active the whole time), but for a longer time. <em>In general, more than 1 active thread per CPU is less efficient.</em></p>\n<p>You might have jumped to the conclusion that more threads would be even better, right? What happens when you divide the numbers in 8 groups (6 groups of 6 numbers and 2 groups of 7 numbers), and use <strong>8 threads</strong> (on <strong>8 CPUs</strong>) in parallel to sum those groups? The 6 threads that sum 6 numbers will be finished in <strong>5</strong> cycles, but the 2 threads that sum 7 numbers need <strong>6</strong> cycles. So those 6 CPUs will have to wait <strong>idle</strong> for one CPU cycle for the other <strong>2 threads</strong> to finish. Then you still have to add the sums of the 8 groups to get the total sum. If you use <strong>1 thread</strong> for this, you will need 7 CPU cycles. During this time, the other 7 CPUs will all be waiting <strong>idle</strong>!</p>\n<p>The time needed would be <strong>13 cycles</strong> (<em>excluding any additional overhead</em>): 6 cycles to sum the groups and 7 cycles to calculate the total. The first 5 cycles 8 active threads run (on the 8 CPUs), the next cycle only 2 active threads remain and the final 7 cycles only 1 active thread runs. So the average CPU load is only <strong>47%</strong> (= (5 * (8 / 8) + (2 / 8) + 7 * (1 / 8)) / 13). In total 104 CPU cycles  (13 cycles * 8 CPUs) were available to perform the 49 additions, so the CPU efficiency is only <strong>47%</strong> (= 49 / 104). <em>In general, the more threads you use, the lower the average CPU efficiency!</em></p>\n<p>Conclusion: in this example, you can run <strong>4</strong> jobs with <strong>2</strong> threads each <strong><em>in less time</em></strong> than it takes to run <strong>2</strong> jobs with <strong>8</strong> threads each (especially considering that larger jobs often have longer waiting times). <em>So choose your number of threads optimally.</em></p>"
  },
  {
    "QuestionTitle": "<h3>How to determine the GPU use of your program?</h3>",
    "QuestionBody": "",
    "Tags": [
      "Job resource questions"
    ],
    "Answer": "<ul>\n<li>Log in on a login node with a GPU (<code>login1</code> or <code>login3</code>).</li>\n<li>Run <code>nvidia-smi -l</code>. Make sure no processes are using the GPU.</li>\n<li>Log in a second time <em>on the same login node</em> and run your program (<code>python …</code>). </li>\n<li>The current GPU utilization of your program is reported by <code>nvidia-smi</code> under <code>GPU-Util</code>.</li>\n<li>Quit your program and nvidia-smi by pressing <code>Ctrl+c</code>, and log out from the login node.</li>\n</ul>"
  },
  {
    "QuestionTitle": "<h3>How do I request CPUs for a multithreaded program?</h3>",
    "QuestionBody": "",
    "Tags": [
      "Job resource questions"
    ],
    "Answer": "<ul>\n<li>\n<p>If you can specify the number of threads your program will use:</p>\n<ul>\n<li>determine a “smart“ number of threads ( for example the number of currently available CPUs, or the number of threads needed to finish within 4 hours), <strong>always a multiple of 2</strong>, but <strong>preferably no more than 8</strong>,</li>\n<li>request a cpu for each thread (<code>--cpus-per-task=&lt;#threads&gt;</code>), and</li>\n<li>tell your program to use  the <code>$SLURM_CPUS_PER_TASK</code> variable for  threads.\n    <code>bash\n    #/bin/sh\n    #SBATCH --ntasks=1 --cpus-per-task=2\n    srun ggsearch36 -t \"$SLURM_CPUS_PER_TASK\"</code></li>\n</ul>\n</li>\n<li>\n<p>If your program uses a fixed number (2, 4,..) of threads:</p>\n<ul>\n<li>request a cpu for each thread (<code>#SBATCH --cpus-per-task=&lt;#threads&gt;</code>).\n    <code>bash\n    #/bin/sh\n    #SBATCH --ntasks=1 --cpus-per-task=2\n    srun my_program</code></li>\n</ul>\n</li>\n<li>\n<p>If your program automatically uses as many threads as there are CPUs on a node (for example <strong>java</strong> programs):</p>\n<ul>\n<li>do <strong>not</strong> use functions that detect the total number of CPUs of a computer (for example,  <code>os.cpu_count()</code> for Python), instead, use functions that detect the <strong>CPU affinity</strong> (for example, <code>len(os.sched_getaffinity(0))</code> for Python).</li>\n<li>\n<p>for some code you can explicitly specify the correct number of threads using an environment variable (<code>export &lt;VARIABLE&gt;=&lt;value&gt;</code>):\n    <code>bash\n    #/bin/sh\n    #SBATCH --ntasks=1 --cpus-per-task=2\n    export NUMBA_NUM_THREADS=\"$SLURM_CPUS_PER_TASK\"\n    srun python script.py</code></p>\n</li>\n<li>\n<p>If that is not possible, request a complete node for one task (<code>#SBATCH --ntasks=1 --exclusive</code>), and tell srun to use <code>\"$SLURM_CPUS_ON_NODE\"</code> threads.</p>\n<p>```bash</p>\n<h1>/bin/sh</h1>\n<h1>SBATCH --ntasks=1 --exclusive</h1>\n<p>srun --cpus-per-task=\"$SLURM_CPUS_ON_NODE\" java_program\n```</p>\n</li>\n</ul>\n</li>\n</ul>"
  },
  {
    "QuestionTitle": "<h3>How can I use a GPU?</h3>",
    "QuestionBody": "",
    "Tags": [
      "Job resource questions"
    ],
    "Answer": "<ul>\n<li>See <a href=\"/docs/manual/job-submission/job-gpu#jobs-on-gpu-resources\">Jobs on GPU resources</a></li>\n</ul>"
  },
  {
    "QuestionTitle": "<h3>How can I let multiple programs use the same GPU?</h3>",
    "QuestionBody": "",
    "Tags": [
      "Job resource questions"
    ],
    "Answer": "<ul>\n<li>\n<p>If you want to let multiple instances of your program share a GPU, you'll have to start them in parallel from the same job using <code>srun</code>.\n    <code>#!/bin/sh\n    #SBATCH --gres=gpu:1\n    #SBATCH --ntasks=2\n    srun program</code></p>\n</li>\n<li>\n<p>If you want to use different programs, use the <code>--multi-prog</code> option:</p>\n<p>job.conf:\n<code>bash\n0 &lt;program 1&gt;\n1 &lt;program 2&gt;</code></p>\n<p>job.sh:\n```bash</p>\n<h1>!/bin/sh</h1>\n<h1>SBATCH --gres=gpu</h1>\n<h1>SBATCH --ntasks=2</h1>\n<p>srun --multi-prog job.conf\n```</p>\n</li>\n</ul>"
  },
  {
    "QuestionTitle": "<h3>How much memory can I use?</h3>",
    "QuestionBody": "",
    "Tags": [
      "Job resource questions"
    ],
    "Answer": "<ul>\n<li>A task can run on a single node only. Since most jobs consist of a single task, those jobs are therefore limited to the total amount of memory in a node. However, since that would leave no memory for other jobs on the node, do not request more memory than your jobs need! (Also see <a href=\"#how-can-i-see-the-cpu-or-ram-usage-of-a-job\">How can I see the CPU or RAM usage of a job?</a>).</li>\n<li>There is also a per-user and per-QoS memory limit for the combined requested memory of all running jobs (of a user) in a certain QoS (see  <a href=\"/docs/manual/job-submission/qos#partitions-and-quality-of-service\">Quality of Service</a>). So, to run the most jobs at the same time, don’t request more memory than your jobs need.</li>\n<li>The average amount of memory that you can request when you want to run a lot of jobs is <strong>less than 8Gb</strong> per job (<strong>less than 4Gb</strong> per CPU). This will give you the most running jobs on the cluster.</li>\n</ul>"
  },
  {
    "QuestionTitle": "<h3>How can I see the CPU or RAM usage of a job?</h3>",
    "QuestionBody": "",
    "Tags": [
      "Job resource questions"
    ],
    "Answer": "<ul>\n<li><code>sstat</code> shows specific usage information for (running) job <em>steps</em> (i.e. when you start your program using <code>srun <em>program</em></code>), using something like this (all on one line):\n    <code>sstat --allsteps --format=JobID,NTasks,MinCPU,MaxRSS,MaxDiskRead,MaxDiskWrite &lt;jobid&gt;</code>\n    The <code>MaxRSS</code> field, for example, shows the maximum amount of memory used until now.</li>\n<li><code>seff &lt;jobid&gt;</code> shows basic CPU and memory usage statistics of the whole job, including easy to understand percentages, but only for <em>finished</em> jobs.</li>\n</ul>"
  },
  {
    "QuestionTitle": "<h3>How can I see the GPU usage of a job?</h3>",
    "QuestionBody": "",
    "Tags": [
      "Job resource questions"
    ],
    "Answer": "<ul>\n<li>See <a href=\"/docs/manual/job-submission/job-gpu#jobs-on-gpu-resources\">Jobs on GPU resources</a></li>\n</ul>"
  },
  {
    "QuestionTitle": "<h3>How can I limit the number of jobs per node?</h3>",
    "QuestionBody": "",
    "Tags": [
      "Job resource questions"
    ],
    "Answer": "<ul>\n<li>When a job risks overloading a node (for example because it creates a large load on the network storage, or because it uses a lot of space in <code>/tmp</code>), you need to explicitly limit the number of jobs that can be run simultaneously on a node.</li>\n<li>One way to do this is to use the GRES (Generic consumable RESource) <code>jobspernode</code>. This allows you to limit the number of jobs per node to one, two, three or four. You do this by requesting <strong>one</strong> of the following GRES in your jobs (pick the one you need):\n    <code>bash\n    #SBATCH --gres=jobspernode:one:1\n    #SBATCH --gres=jobspernode:two:1\n    #SBATCH --gres=jobspernode:three:1\n    #SBATCH --gres=jobspernode:four:1</code>\n    When you specify one of these in your job, for example the one for four jobs per node, the scheduler will start (up to) four jobs on one node, and will then wait until a job finishes before starting another job on that node.</li>\n</ul>"
  },
  {
    "QuestionTitle": "<h3>Should I feel guilty about running a lot of jobs with GPU/CPU usage?</h3>",
    "QuestionBody": "",
    "Tags": [
      "Job resource questions"
    ],
    "Answer": "<ul>\n<li>Actually, the more jobs running and waiting in the queue, the more efficient and fair the scheduler can plan and divide the resources over the waiting jobs, so the higher the throughput and the sooner all work is finished. The scheduler will make sure that the available resources are divided fairly between the jobs of all users. This is done based on previous usage: the amount of allocated CPUs, GPUs and memory multiplied by a job’s real runtime. The higher a user’s previous usage, the lower the priority of that user’s jobs waiting in the queue. So when one user has a high previous usage, the waiting jobs of other users will be started before the jobs of that user. (The previous usage continuously decays, so when the usage stops, that user’s priority automatically goes back up again in a few days.)</li>\n</ul>"
  },
  {
    "QuestionTitle": "<h3>How do I clean up <code>/tmp</code> (when a job fails)</h3>",
    "QuestionBody": "",
    "Tags": [
      "Job resource questions"
    ],
    "Answer": "<ul>\n<li>When your job stores temporary data locally on a node, your job needs to clean up this data before it exits. So include an <code>rm</code> command at the end of your job script. (The system does not clean up this data automatically.)</li>\n<li>\n<p>When the job fails (or is canceled or hits a timeout), the clean up requires some special code in the job script:\n    ```bash\n    #!/bin/sh\n    #SBATCH ... # Your usual resources' specifications</p>\n<p>tmpdir=\"/tmp/${USER}/${SLURM_JOBID}\" # Create local temporary folder\nmkdir --parents \"$tmpdir\"</p>\n<h1>You may want to uncomment this to know what to clean up when the clean up fails:</h1>\n<h1>echo \"Temporary folder: $(hostname --short):${tmpdir}\"</h1>\n<p>function clean_up { # Cleanup temporary folder\n  rm --recursive --force \"$tmpdir\" &amp;&amp; echo \"Clean up of $tmpdir completed successfully.\"\n  exit\n}\ntrap 'clean_up' EXIT # Setup clean_up to run on exit</p>\n<h1>Your code- Make sure your program uses this \"$tmpdir\" location</h1>\n<p>srun …\n<code>* If all else fails, login interactively to the node and manually clean up the files:</code>bash\nsinteractive --nodelist=<node>\n```</p>\n</li>\n<li>\n<p>Request a node with enough space in <code>/tmp</code> (at least twice what your script needs, because other jobs need to use <code>/tmp</code> too):</p>\n<p>```bash</p>\n<h1>SBATCH --tmp=<size>G</h1>\n<p>```</p>\n</li>\n</ul>"
  },
  {
    "QuestionTitle": "<h3>Interactive sessions hang when left for some time without input</h3>",
    "QuestionBody": "",
    "Tags": [
      "Slurm questions"
    ],
    "Answer": "<ul>\n<li>This seems to be a bug. For now, use <code>sattach</code> or one of the login nodes.</li>\n<li>Of course, if a session is really idle (i.e. nothing running), just close it so another job can run.</li>\n</ul>"
  },
  {
    "QuestionTitle": "<h3>Job pending with reason <code>QOSGrpCpuLimit</code></h3>",
    "QuestionBody": "",
    "Tags": [
      "Slurm questions"
    ],
    "Answer": "<ul>\n<li>Each QoS (Quality of Service) sets limits on the total number of CPUs in use for that QoS. If the total number of CPUs in use by running jobs (of the whole group of users combined) in a QoS hits the set limit, no new jobs in that QoS can be started, and jobs will stay pending with reason <code>QOSGrpCpuLimit</code>. This is not an error; when running jobs finish, the highest priority pending job will be started automatically (see <a href=\"/docs/manual/job-submission/priorities\">Priorities</a>).</li>\n</ul>"
  },
  {
    "QuestionTitle": "<h3>Job pending with reason <code>ReqNodeNotAvail</code></h3>",
    "QuestionBody": "",
    "Tags": [
      "Slurm questions"
    ],
    "Answer": "<ul>\n<li>Sometimes nodes are not available to start new jobs (for example when they are reserved for maintenance). When jobs can only run on those nodes, they will remain pending with the reason <code>ReqNodeNotAvail</code> until the node become available again.</li>\n<li>The requested runtime of a job is an important factor here. When a reservation starts in 1 day, a job with a requested runtime of 7 days will not be able to start (since it would not be finished before the start of the reservation), but a job with a requested runtime of 4 hours can still be run normally. So, when possible, reduce the requested runtime.</li>\n<li>The requested resources (number of CPUs, amount of memory, number or type of GPUs or specific constraints) limit the number of nodes that are suitable to run a job. So, when possible, reduce the requested resources and constraints, and do not request specific GPU types.</li>\n</ul>"
  },
  {
    "QuestionTitle": "<h3>Why does my job run on some nodes but fail on other nodes?</h3>",
    "QuestionBody": "",
    "Tags": [
      "Slurm questions"
    ],
    "Answer": "<ul>\n<li>The nodes have different configurations (processor types, number of cores, memory size, GPU support, and so on- see <a href=\"/docs/system\">System specifications</a>). If you need a specific feature, make sure to request it in your sbatch script. Examples:\n    <code>bash\n    #SBATCH --constraint=avx\n    #SBATCH --constraint=avx2\n    #SBATCH --gres=gpu</code></li>\n<li>If your program uses specific processor instruction extensions (AVX/SSE/AES/…), it will crash (with an <code>Illegal instruction</code> error) on processors that do not support those extensions. </li>\n<li>Either compile your program to use only standard instructions (be generic), or request nodes that have support. </li>\n<li>The login node <code>login3</code> has the least advanced CPUs so if you compile your programs there they should run on all other nodes.</li>\n</ul>"
  },
  {
    "QuestionTitle": "<h3>Why does my job fail and is there no slurm-XXXXX.out output (or error)?</h3>",
    "QuestionBody": "",
    "Tags": [
      "Slurm questions"
    ],
    "Answer": "<ul>\n<li>When your job doesn't have a valid Kerberos ticket (see <a href=\"/docs/manual/job-submission/#kerberos\">Kerberos authentication</a>) it can't read or write files (such as the <code>slurm-XXXXX.out</code> output).</li>\n<li>It's best to do a fresh login to a login node when you want to submit a new job. This way you're sure your job's Kerberos ticket is valid and will remain valid for the next 7 days. </li>\n<li>If needed, you can update the Kerberos ticket for a running job by executing <code>auks -a</code> (also from a fresh login).</li>\n</ul>"
  },
  {
    "QuestionTitle": "<h3><code>sbatch: error: Batch job submission failed: Access/permission denied</code></h3>",
    "QuestionBody": "",
    "Tags": [
      "Slurm questions"
    ],
    "Answer": "<ul>\n<li>You can only submit jobs (using <code>sbatch</code>) from the login nodes (<code>login1, login2, login3</code>). When you try to submit a job from within another job (including an interactive job) you will receive this error.</li>\n<li>To submit multiple jobs from a script, create a file with the submit commands:\n    <code>sbatch job1.sh\n    sbatch job2.sh\n    sbatch job3.sh\n    …</code>\n    Then login to one of the login nodes and source the file:\n    <code>source script</code></li>\n</ul>"
  },
  {
    "QuestionTitle": "<h3><code>sbatch: error: Batch job submission failed: Invalid account or account/partition combination specified</code></h3>",
    "QuestionBody": "",
    "Tags": [
      "Slurm questions"
    ],
    "Answer": "<ul>\n<li>You are trying to use a (special) partition that you don’t have access to, Or</li>\n<li>Your account has been (temporarily) disabled because of problems with your jobs. The usual problems are (a combination of) of these:<ol>\n<li>Your jobs are not using all of the requested resources (CPUs, GPUs, memory, runtime).</li>\n<li>Your jobs try to use more resources than requested (eg, more active threads than the requested number of CPUs, out of memory failures, timeout failures ... etc).</li>\n<li>Too many jobs are failing or being cancelled.</li>\n<li>Generally, failing to follow the <a href=\"/tutorials/quickstart\">Cluster workflow</a>.</li>\n</ol>\n</li>\n</ul>\n<p>You need to figure out the problem(s), fix them, and then send an email to the <a href=\"mailto:beheer-o-linux-ictfm@tudelft.nl\">cluster administrators</a> to explain the problem(s) and the way you fixed them.</p>"
  },
  {
    "QuestionTitle": "<h3><code>srun: error: Unable to allocate resources: Invalid qos specification</code></h3>",
    "QuestionBody": "",
    "Tags": [
      "Slurm questions"
    ],
    "Answer": "<ul>\n<li>You can’t directly execute a jobscript, you need to submit the jobscript using <code>sbatch</code>:\n    <code>sbatch jobscript.sh</code></li>\n</ul>"
  },
  {
    "QuestionTitle": "<h3><code>slurmstepd: error: Exceeded step memory limit at some point.</code></h3>",
    "QuestionBody": "",
    "Tags": [
      "Slurm questions"
    ],
    "Answer": "<ul>\n<li>Your program wants to use more memory than you requested. You’ll either need to limit the memory use of your program or request more memory. Also see <a href=\"/support/faqs/job-resources#how-much-memory-can-i-use\">How much memory can I use?</a> and <a href=\"/support/faqs/job-resources#how-can-i-see-the-cpu-or-ram-usage-of-a-job\">How can I see the CPU or RAM usage of a job?</a>.</li>\n</ul>"
  },
  {
    "QuestionTitle": "<h3><code>Auks API request failed : auks cred : credential lifetime is too short</code> / <code>Auks API request failed : krb5 cred : unable to renew credential</code></h3>",
    "QuestionBody": "",
    "Tags": [
      "Slurm questions"
    ],
    "Answer": "<ul>\n<li>Your authentication (Kerberos ticket) expired (see <a href=\"/docs/manual/job-submission/kerberos\">Kerberos authentication</a>). You get a Kerberos ticket (using your password) when you log in to the bastion server or a login node. Jobs that run on a compute node also require authentication. Therefore, when you submit a job, your Kerberos ticket is stored in the Auks system so that your job can use it. However, the maximum lifetime of a Kerberos ticket is 7 days. So 7 days after you last logged in and submitted a job, the Kerberos ticket in the system expires and the job fails.</li>\n<li>Therefore, for infinite jobs or long jobs that have had to wait in the queue for a couple of days, the Kerberos ticket needs to be renewed before it expires. The simple way to do this is to log in to a cluster login node and run <code>auks -a</code>.</li>\n<li>When you frequently need to do this, you can (on a cluster login node) run <code>install_keytab</code> to install an encrypted version of your password (called Kerberos keytab) for automatic Kerberos ticket renewal. </li>\n</ul>\n<p>{{% alert title=\"Important\" color=\"warning\" %}}\nWhen you change your NetID password, your Kerberos keytab becomes invalid, so you will need to rerun the <code>install_keytab</code> command.\n{{% /alert %}}</p>"
  },
  {
    "QuestionTitle": "<h3>What can be done about some jobs using all CPUs or memory (making it impossible for me to use other unused resources like GPUs)?</h3>",
    "QuestionBody": "",
    "Tags": [
      "Slurm questions"
    ],
    "Answer": "<ul>\n<li>Resources can be used by one job at a time only, so when some resources are completely used, other jobs will have to wait until their required resources become available. The waiting is unavoidable.</li>\n<li>See the answer to <a href=\"/support/faqs/job-resources#should-i-feel-guilty-about-running-a-lot-of-jobs-with-gpucpu-usage\">Should I feel guilty about running a lot of jobs with GPU/CPU usage?</a> (The scheduler is already dividing the available resources fairly over all jobs based on the policies, using priorities based on previous usage. As soon as a running job finishes and it’s resources become available again, the highest priority waiting job is automatically started.)</li>\n<li>The policies for the scheduler are set by the cluster users, in the cluster board meeting (see <a href=\"/docs/policies#general-cluster-usage\">General cluster usage</a>). To change the policies, all users must agree that the changes are necessary and fair to all users. (So <em>the cluster administrators aren’t able to change the policies on request!</em>)</li>\n<li>You can always contact a user (nicely!😉) about the possibility to (for example) exclude a certain node. That user can decide for him-/herself if he/she wants to cooperate or not (for example in case of deadlines).\n    It’s usually possible to determine a person’s initial (or first name) and last name from his/her username; if not, run: <code>finger &lt;username&gt;</code></li>\n<li>If you experience a real problem because of this (not being able to efficiently develop code because of the waiting, or not going to make an upcoming deadline), you should contact <a href=\"/support/#support--contact\">cluster support</a> to see if they can provide support for that specific problem.</li>\n<li>It’s not desirable to reserve resources for certain types of jobs only. Since other types of jobs wouldn’t be able to use those resources even when they would be idle, this would reduce the overall cluster throughput (and recreate the exact same problem that you experience for those other jobs).</li>\n<li>No type of research can make special claims regarding resources. All research that uses the cluster is equally dependent on the cluster resources: 50+ CPU jobs or jobs requiring 50GB memory can’t be run on a laptop any more than a GPU job requiring 5GB of GPU memory. When one type of resource is completely used, that is because it is required to do the same kind of research that you need the cluster for.</li>\n</ul>"
  }
]